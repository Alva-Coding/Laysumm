Variation in textile production processes from archaeological contexts can distinguish communities of weavers and signal distinct group identities. In this paper, we present an analysis of 141 textiles recovered from a single grave located in the mid-Chincha Valley, Peru that dates from the Late Horizon (1400–1532 CE) to the Colonial Period (1532–1825 CE). This sample represents one of the largest and best-preserved textile assemblages from a clearly defined and radiocarbon-dated archaeological context in the Chincha Valley. For this study, we document techniques used in two distinct phases of textile production: yarn production (spinning and plying) and weaving. We 1) develop a manual hierarchical classification method for identifying groups of textiles featuring consistent associations among techniques used for each production phase and 2) assess how these groups vary in terms of thread count, size, garment type, and design. Our results reveal six groupings of textile production techniques that account for 71% of the assemblage by count. We compare these results to that of an independent cluster analysis that examines the joint co-occurrence of yarn production and weaving techniques and find that they are largely in accordance with each other. We suggest that these multiple textile groups corresponded to distinct communities or households of weavers associated with this grave. Our study provides a methodology for analyzing the variation and consistency of textile production to learn about communities of weavers within and outside the Andes. Craft production broadly entails the transformation of raw materials into finished products. Several anthropological studies on technology describe production as a social process mediated by communities and individuals (Dobres, 2000; Lemonnier, 1986, 1992, 2013). Through this process and the identities of the individuals who made them, products acquire value, meaning, and power (Clark and Parry, 1990; Costin, 1998a, b, 2001, p. 274; Lechtman, 1993). Scholars draw attention to the body as a medium through which techniques are learned, transmitted, and employed, and assert that these activities occur in social environments that integrate people, relationships, and material culture (Budden and Sofaer, 2009; Dobres, 2000, pp. 131, 137; Mauss, 2006). Embodied techniques and motor skills can leave physical traces on finished products, and they can reveal social norms because they are frequently learned through imitation and repetition in social contexts (Minar, 2001; Tiballi, 2010, p. 146). Methodologies derived from the concept of chaîne opératoire, translated as “chain of operations,” consider traces left by embodied techniques to model sequences of actions and decisions that characterize the life history of an artifact (Lemonnier, 1986, 1992, 2013; Leroi-Gourhan, 1943; Mauss, 2006). Different sequences of production can foster variations in the material record, variations that may connect to style (Carr, 1995; Conkey and Hastorf, 1990; Sackett, 1977, 1982, 1990; Wiessner, 1983, 1984; Wobst, 1977), group identity (Brumfiel, 1998; Costin, 1998a, 1998b), individual experimentation and personal preference, and cultural and community-based norms and preferences (Dobres, 2000; Peters, 2014; Tiballi, 2010). 
While rare, post-ictal thoracolumbar burst fractures are commonly missed due to confounding factors, resulting in delayed treatment and the potential for serious neurological deficits. This paper serves as a call for a high-degree of clinical suspicion when treating post-ictal patients to ensure they undergo a focused neurological examination of the lower extremities. If unresponsive/uncooperative, spinal precautions should be maintained until the spine can be cleared clinically or radiographically. In all events, if the patient is complaining of musculoskeletal pain possibly originating from the spine, radiographic evaluations are warranted to prevent possible deficits caused by a missed thoracolumbar fracture. Fractures are a well-known and common result of a violent and traumatic generalized tonic–clonic seizures (GTC seizure) [1]. It is also common that the identification of fractures is often delayed due to the absence of physical signs and other pressing symptoms, the result of which is a delay in treatment [2]. The literature has shown that when a GTC seizure causes a spine fracture, they are most commonly asymptomatic compression fractures. [3] However, there are rare reports of post-ictal thoracolumbar burst fractures and when these fractures go undiagnosed and treatment is delayed, serious neurological deficits can result [4–10]. We add to this literature, reporting on two recent cases of lumbar burst fracture following a GTC seizure, with a delay to diagnosis resulting in deficits, which strongly supports the call for high clinical suspicion post-seizure to rule out a potentially devastating fracture type. SECTION Background SECTION Case 1 
Infants in a skeletal population are important proxies of an ancient society's adaptation and well-being. This study uses microscopic dental enamel defects (Accentuated Retzius Lines, ALs) to provide a close-to-longitudinal and detailed estimate of the non-fatal stress prevalence in the first years of life in the community of Portus Romae (necropolis of Isola Sacra, 2nd to 4th century CE, Italy). Eighty-four teeth, 17 deciduous and 67 permanent, from 18 individuals were selected for histological thin sectioning. We scored and assessed the individual chronology of ALs across dental sets, by matching homologous intervals between ALs on several teeth in the same individual. After a steep increase following the 3rd month, AL prevalence distribution shows a maximum between the 9th and 11th month of life. Following the prevalence maximum, the distribution declines steadily until the 25th month, after which it remains almost stable. The ALs frequency by tooth type shows that the bulk of affected enamel is in the center of the tooth crown, with the apical and cervical portions less susceptible to recording stress. Our results illustrate how to derive a longitudinal profile of health status in the childhood segment of an ancient population through histomorphometry. Comparison of the ALs profile with the previously published δ15N and δ13C of Portus Romae and the application of a Bayesian statistical model allowed us to relate the prevalence maximum to the beginning of the weaning process. A multifactorial approach to the palaeobiology of a skeletal series is therefore rewarding and allows correlation with the biological life history of children in this ancient Roman Imperial community. The mortality profile and the health status of the children of any human population represent a measure of the biocultural adaptability of the community (Goodman and Armelagos, 1989; Lewis, 2018). As highlighted by several epidemiological studies on modern populations (for a review see Humphrey and King, 2000), the conditions during the first years of life - from intrauterine life until childhood - are often reflected in ill-health and earlier mortality in adult age (Armelagos et al., 2009; Barker, 2004). However, the reconstruction of life conditions and health in a human skeletal population is an extremely difficult and complex task. The Osteological Paradox (DeWitte and Stojanowski, 2015; Wood et al., 1992) proposes that skeletal series are biased proxies of the corresponding populations' life conditions. These authors contend that heterogeneous frailty and selective mortality are disruptive factors, often impossible to quantify in a skeletal series. However, the individual biological history of the first years of life can be read in the mineralized tissues that form during this period and, particularly, in teeth. Indeed, dental enamel during its formation is a biological archive, recording at a microscopic level important rhythmical growth markers as well as physiological stress episodes (Antoine et al., 2009; Dean, 2006; Hillson, 2014; Smith and Tafforeau, 2008). The non-specificity of the ALs as stress markers allows us to use their prevalence as generalized indicator of adaptive responsiveness of the Portus Romae population. Stress prevalence was established longitudinally from birth to about four years of age. This gives not only one single frame at the moment of death, but a “movie” of the stresses in the first years of life. Prevalence of ALs during early childhood for the Isola Sacra skeletal assemblage is high, but no correlation exists between ALs prevalence and age at death. The majority of stresses seem to accumulate in the first 25 months of life so that the prevalence profile tends to reach a stable level afterwards. We propose that the presence of the observed levels of ALs is not indicative of a poor health status of the Isola Sacra infants. Rather, they represent a picture of short-term non-fatal stresses during infancy buffered by a reasonable quality of life, as inferable both from the diet (Prowse et al., 2004, 2005, 2008) and from the social conditions (Garnsey, 1999b).
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. The research and consequent development of sensory support and navigation technologies for visually impaired people (VIP) is increasing (Chan et al., 2017; Mekhalfi et al., 2016; Tsirmpas et al., 2015; Mascetti et al., 2016; Bourbakis et al., 2013; Aladren et al., 2014; Xiao et al., 2015). Although it is a specific area of research, solving the problems and difficulties faced by these people requires comprehensive analysis, and should include several different difficulties that a VIP has to deal with where they move. 
Different hypotheses exist to explain population development and replacement on the East Coast of the Yucatan Peninsula after the so-called Maya collapse, a period of dynamic population movement responding to changing socio-economic and political spheres of influence in the region. Here we investigate this dynamic by combining the evidence from dental morphology, and 87Sr/86Sr and δ18O isotopic analyses of the human skeletal remains from the Late Postclassic coastal trader settlements of El Meco, El Rey, and Tulum (1200–1550 CE). Isotopic results show different scenarios by locality. The sample from El Meco does not show foreign individuals, while El Rey presents 20% of the sample as likely non-local. Tulum has the most varied isotopic values of the three, though only one individual could be non-local. The series from the core settlement of Tulum displays lower dental morphological heterogeneity, but higher diversity in artificial head shapes and a small proportion of non-locals, probably due to the small sample size available. El Rey shows homogeneity in head shape and high morphological variability. El Meco is overall the most homogeneous sample from the East Coast of the Peninsula of Yucatán. In the broader contexts of social organization and regional population dynamics, our results indicate broad trends in population movement and cultural cohesion along the Yucatecan coast. Human mobility and biological affinity among Maya groups have been studied under different bioarchaeological lenses, including head shape (Sierra Sosa et al., 2014; Tiesler, 2014; Tiesler and Cucina, 2012, 2010; Tiesler and Ortega Muñoz, 2013), aDNA (González-Oliver et al., 2001; Matheson et al., 2003; Merriwether et al., 1997), dental morphology (Austin, 1978; Cucina, 2013, 2015, 2016; Cucina and Ortega-Muñoz, 2014; Cucina et al., 2008, 2009, 2018; Tiesler and Cucina, 2012; Wrobler and Graham, 2015), morphometric criteria (Cucina and Tiesler Blos, 2004; Scherer, 2007; Scherer and Wright, 2013, 2015; Serafin et al., 2013, 2014; Wrobler and Graham, 2015), as well as with trace elements (Cucina et al., 2011) and strontium and oxygen isotope ratios (Buikstra et al., 2003; Freiwald, 2011; Freiwald et al., 2014; Olsen et al., 2014; Price et al., 2006a, b, c, 2007, 2008, 2010, 2014, 2015, 2018; Scherer and Wright, 2013, 2015; Sierra Sosa et al., 2014; Wright, 2005, 2012; Wright et al., 2010; Wrobler et al., 2014). 
Hemispherotomy is a surgical treatment indicated in patients with drug-resistant epilepsy due to unilateral hemispheric pathology. Hemispherotomy is less invasive compared with hemispherectomy. We reviewed our experience performing 24 hemispherotomy and report the results of 16 patients with prolonged follow-up of this relatively uncommon procedure in two centers in Indonesia. This is a retrospective observational study conducted from 1999 to July 2019 in two epilepsy neurosurgical centers in Semarang, Indonesia. Surgical techniques included vertical parasagittal hemispherotomy (VPH), peri-insular hemispherotomy (PIH), and modified PIH called the Shimizu approach (SA). The postoperative assessment was carried out using the Engel classification system of seizure outcome. Seizure freedom (Engel class I) outcome was achieved in 10 patients (62.5%), class II in 3 patients (18.7%), class III in 2 patients (12.5%), and class IV in 1 patient (6.3%) with follow-up duration spanning from 24 to 160 months. To the best of our knowledge, this series is the most extensive documentation of hemispherotomy in an Indonesian population. Hemispherotomy is a potential surgical treatment indicated for patients with drug-resistant epilepsy due to unilateral hemispheric pathology [1,2]. The underlying etiology for unilateral hemispheric pathology may include conditions such as Rasmussen syndrome, Sturge–Weber syndrome, porencephaly, perinatal stroke and disturbances in neuronal migration (e.g., hemimegalencephaly, cortical dysplasia, and hemiconvulsion–hemiplegia–epilepsy syndrome) [3,4]. 
Coastal shell middens are known for their generally excellent preservation and abundant identifiable faunal remains, including delicate fish and bird bones that are often rare or poorly preserved at non-shell midden sites. Thus, when we began our human ecodynamics research project focused on the fauna from Čḯxwicən (45CA523, pronounced ch-WHEET-son), a large ancestral village of the Lower Elwha Klallam Tribe, located on the shore of the Strait of Juan de Fuca, Port Angeles, Washington (USA), we anticipated generally high levels of bone identifiability. We quickly realized that the mammal bones were more fragmented and less identifiable than we had expected, though this was not the case with the bird and fish bone or invertebrate remains. To better understand why this fragmentation occurred at Čḯxwicən, we evaluate numerous hypotheses, including both post-depositional and behavioral explanations. We conclude that multiple factors intersected (to varying degrees) to produce the extreme bone fragmentation and low identifiability of mammal bones at the site, including bone fuel use, marrow extraction, grease rendering, tool production, and post-depositional breakdown. Using a human ecodynamics framework, we further consider how both social factors and external environmental forces may mediate human choices, such as the economic decision to use bone for fuel or render bone grease. We place our findings from Čḯxwicən in a regional context and discuss the potential of the approach for other coastal archaeological sites worldwide. Taphonomy is the study of the myriad behavioral, physical, and chemical factors that collectively influence deposition, survival, and recovery of bones and other animal parts from archaeological sites. Since the 1970s, archaeologists have recognized the importance of taphonomic analysis in isolating the factors that affect faunal remains across their ‘life history’ before drawing meaningful interpretations about past human activities (e.g., Behrensmeyer and Hill, 1980; Blumenschine, 1988; Brain, 1981; Ferraro et al., 2013; Gifford, 1981; Lyman, 1994; Orton, 2012). Taphonomic scholarship has two main goals: “to strip away the taphonomic overprint” (Lawrence, 1979:903; cited in Gifford, 1981) to obtain an accurate understanding of the original biological community or the systemic context (e.g., sensu Schiffer, 1987); and to understand the so-called overprint itself—with the idea that the overprint holds behavioral meaning in its own right (Gifford, 1981; Lyman, 1994). 
The analysis of human behavior is a popular topic of research since it allows obtaining specific information about individuals, their motivations, and the problems and difficulties they can encounter. Human behavior can be grouped to elaborate profiles that would enable the classification of individuals. Nevertheless, the elaboration of profiles related to human behaviors presents some difficulties associated with the volume of data and the number of parameters typically considered. Thus, the development of software able to automatize the manipulation of data through graphical assistants to produce understandable visualizations of the human behaviors is crucial. In this paper, the VISUVER framework is presented. It uses finite state machines to represent and visualize the dynamic human behavior automatically. This behavior could be provided by real data collected by specific sensors or simulated data. The state machines are built in sequential steps in order to illustrate the dynamic evolution of the behavior over time. VISUVER also includes similarity metrics based on text mining techniques to establish possible profiles among the analyzed behaviors. The Intelligent Transportation Systems (ITS) domain has been considered in order to validate the proposal. Human behavior has traditionally been one of the most interesting fields of study for the scientific community (Skinner, 1953) be addressed by several domains of application. The main idea behind these approaches is to obtain knowledge from different tasks achieved by individuals. For instance, this knowledge can indicate which are the human needs (i.e., motivations) (Deci and Ryan, 2000), how they establish relationships with the surrounding environment (e.g., other individuals) (Zastrow and Kirst-Ashman, 2006), which are the most common problems detected (Bonanno, 2004), or how to influence over them in order to modify undesirable behaviors (i.e., persuade individuals) (Cialdini and Cialdini, 2007). This paper presents a novel framework called VISUVER which is able to analyze and represent human behavior over time using finite state machines. In order to achieve these goals, it accepts a set of observable variables as input data where each one of the observations is an instant of time.
Two patients at our center experienced florid visual hallucinations following hemispherectomy. The first patient had drug-resistant left hemispheric focal seizures at 20 months of age from a previous stroke. Following functional hemispherectomy at age 3, he experienced frightening hallucinations 1 month post-operatively lasting 3.5 months. Our second patient underwent subtotal hemispherectomy at age 6 for drug-resistant focal seizures from right hemispheric cortical dysplasia. Eighteen months later he developed scary visual hallucinations during which he would shout and throw things. Hallucinations recurred for 6 months. In our experience in these patients, even though symptoms were florid, they were transient and subsided 3–6 months later. To our knowledge visual hallucinations following hemispherectomy have not been reported [1,2]. Visual hallucinations refer to visual images that are not in fact present, and can be either elemental (occipital origin) or formed (temporal origin) representing visual memories. There have been reports of hallucinations in adults following intracranial surgeries, such as musical hallucinations and visual hallucinations associated with Charles-Bonnet syndrome [3,4]. Here we present two pediatric patients out of 24 consecutive patients operated at our center, with visual hallucinations after hemispherectomy. Hemispherectomy was functional in one person and subtotal in another. The hallucinations were disturbing to patients and families, but that eventually proved to be transient. SECTION Case report SECTION Patient 1 
This work deals with the application of archaeological methods and modern methodologies of point clouds survey to structural analysis, with the purpose of creating a series of products, such as Elevation Maps, Orthophotos, 3D Models, in order to highlight the building and mechanical past of the examined buildings and to further the knowledge of the territory's seismic history. These products are to be used as a base for the study of the cognitive process of the material structure, the constructive techniques and the restoration of a specific context, of importance for future vulnerability and restoration analyses. The present paper will focus on the trinomial “technology-archaeoseismology-earthquakes” in order to bring to the attention of the scientific community the advantages and critical issues of an innovative point of view. The historical center of Florence and, specifically, the church of San Remigio, has been chosen as a case study to illustrate the methodology. In a vast, heterogenous and averagely seismic landscape like the Italian one, characterized by a significant presence of historical edifices that require safeguarding, the recent issuing by the MiBAC of the “Linee Guida per la valutazione e riduzione del rischio sismico del patrimonio culturale allineate alle nuove Norme tecniche per le costruzioni (d.m. 14 gennaio 2008) (Mibac, 2011)” has attempted to regulate interventions in the field of architecture (Pugi and Galassi, 2013; Paradiso et al., 2014a). This paper specifically outlines the need of a multidisciplinary approach towards the study of past monuments, achieved from the interaction of analyses conducted by different disciplines related to the fields of the sciences and the humanities. In this panorama, the systematic use of archaeoseismic research applied to historical buildings in broad territorial districts (Arrighetti, 2015, 2016) has produced new groundbreaking data obtained through archaeological methods and tools that are in themselves perfectly integrable with data provided by other disciplines (e.g. historical seismology, structural engineering, earth sciences, restoration, etc.). In a preliminary stage of investigation, the operational praxis developed for the analysis of such contexts has considered that specific attention should be given to the study of seismologic databases and seismic maps. This is followed, in the operational phase, by the integration of archaeological data and macro-features reading (Doglioni et al., 1994; Doglioni, 2003; Doglioni and Mazzotti, 2007) along with the documented evidence and analysis of disruptions and restorations, consequently leading to a stratigraphic “breakdown” and interpretation of the constructive history and mechanics of the buildings. SECTION Research aim 
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. Fractional calculus, formulated 300 years ago, is basically a generalization of the well-known traditional calculus that deals with non-integer order derivatives. Although studies of the fractional calculus topic were initiated in the 17th century, it was not until the last few years when it was used for modeling of many physical phenomena with engineering applications. Fractional order derivative operator in fractional calculus acts as an effective tool for describing memory and hereditary properties of the system. In fact, fractional order-based models, in comparison with the integer order-based models, have the potential to model the behavior of complicated systems more accurately. This property is mainly due to more degrees of freedom and infinite memory which is intrinsic in fractional order calculus. Because of the mentioned properties, the fractional order models are highly successful in describing many phenomena in the various branches of science such as, health monitoring system (Leyden and Goodwine, 2018), aging material (Beltempo et al., 2018), a hydro-turbine system (Xu et al., 2017), biological tissues (Magin, 2010), diffusion phenomena (Pintelon et al., 2005), Viscoelasticity (Soczkiewicz, 2002) and numerous of other examples in various fields. 
Perampanel is the first-in-class selective and noncompetitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid receptor antagonist. It is authorized in the U.S. and Europe as an add-on antiepileptic drug for partial-onset seizures, and for primary generalized tonic–clonic seizures. Single reports have also indicated a potential efficacy for myoclonic jerks. Here, we report a patient whose drug-resistant epilepsia partialis continua completely resolved after adding perampanel. She has remained seizure-free in an eighteen-month follow-up period. Epilepsia partialis continua reemerged transiently after perampanel was temporarily discontinued, with no recurrence after its reintroduction. Therefore, this effect was reproducible, and suggests that it might be worth trying perampanel in similar settings. Perampanel (PER) is a third-generation antiepileptic drug (AED), first-in-class selective and noncompetitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor antagonist. It has been approved and marketed as an add-on AED in partial-onset seizures with or without secondary generalization [1], and in primary generalized tonic–clonic seizures in patients from 12 years of age [2]. Large series in real life have proven the efficacy of PER in focal epilepsy [3,4]. Furthermore, single reports and short series have suggested a potential efficacy of PER in other specific conditions, such as myoclonic jerks in progressive myoclonic epilepsy like Lafora disease [5–7], and in Lance–Adams syndrome [8]. Although its efficacy and safety in the elderly have been previously highlighted [3,9], no clinical experience has been reported among this special population in epilepsia partialis continua (EPC). SECTION Case report 
Ancient mummies are very valuable human remains especially for the study of the evolution of disease. Non-invasive imaging methods such as computed tomography and X-ray are the gold standard to study such precious remains. We report the case of an ancient Egyptian child mummy from the Musée d'art et d'histoire in Geneva, Switzerland with multifocal sclerotic bone lesions affecting the spine and the left hip. The mummy is of unknown provenance, dating to the Roman period with an estimated age of 4–5 years. An infectious origin of the lesions such as tuberculosis seems most likely. Also regarding the time period an infectious etiology is plausible, since there is evidence that tuberculosis was wide spread in ancient Egypt. However, multiple differential diagnoses are discussed, since the evaluation of disease in ancient remains is different to the clinical standards. Medical history and additional invasive investigations are lacking. Also the desiccation and mummification processes lead to alteration of the tissue resulting in anatomico-morphological distortions. Thus our hypothesis can not be proven and multiple differential diagnoses need to be taken into consideration in this rare case. The value of historic human remains for the study of the evolution of human morphology and disease patterns is widely recognized (Bosch, 2000). Preserved tissues are the most direct source, in comparison to secondary sources such as visual or written records. 
This pilot study attempts to document the potential of Prehistoric human bone and teeth collagen from Cyprus (9th-2nd mill. BC), for isotopic analysis and palaeodietary reconstruction. We sampled archaeological human skeletons and some faunal remains coming from six sites located in different locations, with different burial modes. The analysis of carbon and nitrogen elemental compositions and stable isotope ratios (δ13C, δ15N), indicate an extremely poor preservation of collagen, probably in relationship with burying conditions. Although very few individuals were successfully analysed, stable isotope data from this study allow a discussion on different protein food resources intake by humans in comparison with some other published data in the Near East (Greece, Cyprus, Turkey) from the Neolithic to the Bronze Age. These diachronic data provide documentations for future studies, including palaeodietary and environmental field research. In the Eastern Mediterranean and the Near East, most of the C and N stable isotope studies have been conducted on the Neolithic period (Lange-Badré and Le Mort, 1998; Lösch et al., 2006; Papathanasiou, 2003; Richards et al., 2003). Some Bronze Age materials (e.g. Triantaphyllou et al., 2008; Vika, 2009, 2015) and materials from more recent periods (e.g. Byzantine; Bourbou and Richards, 2007) has also been analysed, mainly in Greece. These local studies mainly indicate the lack of marine resources in human diet even close to the sea, and give some information regarding herding practices and plant consumption. In contrast, studies have rarely been conducted on Cypriot materials whatever the chronological period. The heterogeneity of collagen preservation is also highlighted, and collagen is often difficult to extract, especially in the arid areas of the Near East (Lange-Badré and Le Mort, 1998; Papathanasiou, 2003). In the case of Cyprus there are some data from Neolithic sites but a lack of isotopic and palaeodietary data for Chalcolithic and Bronze Age populations is in contrast to the greater range of archaeological data available. This initial bioarchaeological study aims to contribute towards understanding ancient diet from Prehistoric Cyprus by using bone and teeth collagen extraction and elemental and stable isotope (carbon and nitrogen) analyses. This research focuses on (1) the evaluation of the organic preservation (collagen reliability for stable isotope analysis) in human bone and teeth, and (2) if the previous step is successful, the evaluation of human protein intake in comparison with contextual archaeological data, in order to discuss some hypotheses in regard to human palaeodiets on Cyprus. SECTION Sites and contexts This pilot study presents as attempt to examine some collagen preservation criteria from Chalcolithic and Bronze Age human and animal bone and teeth, as well as the palaeodietary signals for different sites in Cyprus using stable isotope analysis which up to now have only been scarcely documented in Cyprus. A very poor preservation of collagen, especially in bones, is highlighted, together with a degree of preservation which could be impacted by the sample's age and location. The analysis of the few stable isotope data successfully obtained (3/28) within this study is consistent with a diet probably mainly based on terrestrial resources, as observed in other prehistoric populations from the region (Richards et al., 2003; Papathanasiou, 2003; Lösch et al., 2006; Triantaphyllou et al., 2008; Vika, 2009, 2015), and potentially including a significant part of legumes. However, palaeodietary information should be considered with caution due to the very small sample size and the absence of local archaeozoological and archaeobotanical isotopic data. This work provide new elements to better understand the potential of Chalcolithic and Bronze Age bone and dental remains for isotopic analysis and the feasibility of palaeodietary studies in Cyprus. Considering these results, it seems that samples should be preferentially taken from (1) teeth, from (2) human and animal remains buried directly in sediment (excluding re-opened spaces) and (3) coming from sites located on sedimentary superficial formations derived from a calcareous substratum. Further analyses should include thorough studies of environmental data, and additional faunal remains need to be analysed in order to discuss our results with more relevance.
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. Planning defines a specific type of state-transition problem where the goal is to find an admissible sequence of actions to bring the system from a given initial state to a target final state. Some approaches in the literature aim to improve the performance of intelligent automated planners by trying to optimize search algorithms for a general solution (Edelkamp and Jabbar, 2006). In addition, most existing work on AI planning use a domain independent approach where specific knowledge and restrictions of the target problem are not modeled and analyzed in the planning domain. However, even domain independent approaches lead to very smart solution frameworks – normally based on STRIPS – and in practice, can be adapted to solve real problems. In this work, a domain-independent general approach is still used as inspiration for planners algorithms, but before planners start the search for a sequence of actions that lead to the final state the whole planning domain is modeled and analyzed based on requirements (Vaquero et al., 2013b). 
During the Last Glacial Maximum (LGM), very specific but rare osseous decorated artifacts were produced using the “pseudo-excise” technique. These artifacts present a large geographical distribution, extending at least from the Aquitaine basin to Asturias. While in France a Badegoulian age is traditionally accepted for the “pseudo-excise” technique, this is mostly based on arguable data from old excavations and/or problematic archaeostratigraphic contexts. Since it is a key-site for this matter we have focused our attention on Pégourié Cave (Lot, France) in order to establish the chronocultural attribution of pseudo-excised pieces in southwest France. The interdisciplinary reassessment of the lithic and osseous industries from Layer 8 and 9, including inter-layer refittings, has shown (1) the irrelevance of previous stratigraphic subdivisions and (2) the strong cultural heterogeneity of this assemblage, which combines Azilian, Magdalenian, Badegoulian, Solutrean and Gravettian components. At the same time, a broad 14C program based on the direct dating of specific bone and antler technical wastes and tools—including a pseudo-excised point—was implemented after 3D recording using photogrammetry. The results obtained have allowed us to establish a new, firm confirmation of the Badegoulian age of pseudo-excised decoration and, in doing so, to more precisely define the time-range of this specific feature's trans-regional dissemination. Finally, by comparing the results with recent data notably obtained at Llonín cave (Asturias, Spain), new light has been shed on the cultural geography of southwestern Europe during the LGM, allowing us to discuss and fuel the still-controversial “Iberian Badegoulian” hypothesis. At the start of the last Glacial Maximum (LGM; 23–19 cal. ka BP: MARGO Project members, 2009; Mix et al., 2001), the Upper Palaeolithic of southwestern Europe experienced several changes whose breadth and chronology highly depends on geography and/or researchers' interpretations. The Solutrean-to-Magdalenian transition (circa 24–19 cal. ka BP; see Table 1 for a synthetic chronocultural framework) is indeed part of a lively current debate where the issue of the existence and geographical extent of Badegoulian technical traditions is a central focus. In short, two contradictory models have been proposed by researchers to interpret the evolving cultural trajectories of hunter-gatherer groups in this area and time-span. From a “regionalized” Solutrean substratum primarily expressed through a coherent geographical distribution of different types of lithic points (e.g., de la Rasilla and Santamaría Álvarez, 2005; Straus, 1977), the first model defends the idea of a general, abrupt and more or less contemporaneous change throughout this geographical area, corresponding to the development of the quite different Badegoulian technical traditions around 23 cal. ka BP. This scenario, which has been broadly documented and long accepted in France, where Badegoulian industries were first defined (e. g. Allain, 1968; Allain and Fritsch, 1967; Cheynier, 1939; Vignard, 1965), was first supported in the Iberian Peninsula in the 1950s with Cheynier's identification of specific Badegoulian tools—such as the so-called “raclettes”—at Parpalló Cave (Gandía, Valencia) (e.g., Aura, 1995, 2007; Breuil, 1954; Cheynier, 1951). This then extended to northwest Spain following the work of Utrilla (1989, 1981) and later, the critical reassessment by Bosselin and Djindjian of published data (Bosselin, 2000; Bosselin and Djindjian, 1999). This scenario is currently supported by research conducted between Asturias and Levantine Spain notably as part of the SOBAMA project (J.-E. Aura dir.; Aura et al., 2012; de la Rasilla et al., 2014). In contrast, other researchers argue that the data available in the Iberian Peninsula does not indicate abrupt changes but a process of progressive replacement of Solutrean hunting technology through the so-called “desolutreanization” process (de la Rasilla, 1989; Straus, 2000), directly leading to the Magdalenian technical traditions observed from around 20.5 cal. ka BP (Corchón Rodríguez et al., 2015; Rios Garaizar et al., 2013; Straus et al., 2014). This second model, which excludes the Badegoulian “stage” in Spain, considers that the technical similarities are non-significant typo-technological convergences also taking place within different kinds of environments, and assumes that, between 23 and 20.5 cal. ka BP, southwestern Europe was marked by a clear cultural geography (i.e., Badegoulian in present-day France versus Upper/Final Solutrean in the Iberian Peninsula; Banks et al., 2009, 2011). Crystallized in the early 2000s (Bosselin and Djindjian, 2000; Straus and Clark, 2000) and much more complex than a simple logomachie issue (Sauvet et al., 2008), the opposition between these two models still generates a lively debate (Álvarez Alonso and Arrizabalaga, 2012). The results presented in this paper have allowed us to conclusively establish the link between “pseudo-excised” artifacts and raclette-yielding French Badegoulian industries and, in doing so, to reconsider the cultural link between French and Spanish “pseudo-excise”-yielding industries. Within the framework of the long-lasting debate of the existence of an Iberian Badegoulian (Bosselin and Djindjian, 1999, 2000; Straus and Clark, 2000), fueled by conflicting information (see above), we propose an archaeostratigraphic and radiometric correlation favoring the hypothesis of comparable and sub-contemporaneous cultural changes from Asturias to southwest France (Fig. 11; regardless to the geographical origin and modalities of cultural dissemination). Of course, this model is open to discussion and further studies need to be carried out to (1) confirm and complement the data obtained at Les Harpons D regarding the Pyrenean Upper Solutrean and Badegoulian chronology and characteristics and, using the example of Pégourié, (2) discuss in greater depth the coherence of the French corpus of “pseudo-excised” artifacts, mostly known and defined from a bibliographic perspective. Parallel to the reassessment of each archaeological context, typo-technological analysis and direct dating, a specific focus should be made on the technological characteristics of each presumed “pseudo-excised” decoration. This work would provide greater credibility to a very specific and original Badegoulian feature, which is currently “blurred” because of the application of too broad and imprecise attribution criteria, which have rendered it a catch-all category (Duarte et al., 2014).
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. Web images are exploding at an exponential rate due to the rapid development of image acquisition technology, which makes the analysis and understanding of image becoming more difficult. How to effectively express, classify and manage images from the vast web image database has become a research hotspot in the field of computer vision. Traditional automatic image annotation technology extracts the semantic information through analyzing the low-level features of images by machine learning methods, which could not solve the problem of semantic gap effectively. With the development of the Web2.0, users can share their comments conveniently on online socializing platform, which causes that web images always being around with abundant contextual information (Lu et al., 2016). And these text context information of web images can be effectively used to alleviate the problem of semantic gap in image annotation and understanding. 
This study aimed to determine the rate, cause and management of seizures in the context of potential ART–ASD interactions in a cohort of HIV + individuals. Records of 604 HIV + patients were reviewed and those reporting epilepsy/seizure diagnosis were further evaluated. This cohort exhibited a seizure rate of 2.4%. HIV + patients treated for epilepsy displayed low serum ASD levels and failed to achieve seizure control. They were more likely to disengage from Neurology follow-up. For HIV + patients presenting with seizures/epilepsy the ASD prescription and the provision of supplementary support services needs to be carefully considered. In Ireland newly diagnosed cases of HIV have been reported at an annual rate that ranges from 7.0 to 7.5 per 100,000 [1]. Despite the introduction of highly active anti-retroviral therapy (HAART), 40–60% of HIV-infected individuals develop neurological complications [2–4]. The frequency of new seizures in the HIV positive (+) population is estimated to be between 4 and 11% in the populations studied [5]. To date the literature on the epidemiology of seizures and epilepsy in HIV has not generated reliable per patient year incidence estimates. Also no prevalence rates have been determined that can easily separate recurrent provoked seizures from recurrent unprovoked attacks (epilepsy). The data we have so far suggest a prevalence of all seizures of about 6% in a reasonably large HIV + cohort with approximately half of these identified as being unprovoked attacks [3]. 
We report a case of auditory disturbance in an adult female that developed after starting lacosamide treatment for epilepsy. While carbamazepine is known to change auditory pitch perception in some patients, that has not been previously reported as a side effect of lacosamide administration. In our description of pitch perception deficit associated with lacosamide, we outline features seen in our patient and compare our findings with those of previous reports describing carbamazepine-associated auditory disturbance. Lacosamide, a third-generation anti-seizure drug administered for focal epilepsy [1], blocks sodium channels, as also seen with the traditional anti-seizure drug carbamazepine. However, in contrast to that drug, lacosamide blocks the slow inactivation state of sodium channels, though has no effect on fast inactivation. In a large-scale double-blind trial, lacosamide was shown to be non-inferior to carbamazepine-CR and also well tolerated when used as first-line monotherapy in patients aged 16 years or older with newly diagnosed epilepsy [1]. Moreover, lacosamide is a non-enzyme-inducing anti-seizure drug with a predictable pharmacokinetic profile and low potential for interactions with other drugs, indicating its suitability for first-line monotherapy in patients with focal epilepsy. 
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. In spite of the recent advances in renewable energies, oil and gas remain the most important energy sources in the world. As oil reservoirs are drained with time, more sophisticated technologies are required for extraction, to a great extent due to the loss of reservoir pressure and the accumulation of impurities, such as wax, gas hydrates, and asphaltene to name a few. These hurdles led to the concept of “flow assurance”, initially coined by Petrobras in the 90’s, which is concerned with the development and applications of technologies that ensure a stable and economically desirable production, under a wide range of operating conditions (Jahanshahi, 2013). 
The Bronze-Iron Age transition in Lika, Croatia is characterized by a seemingly rapid and significant transformation in sociopolitical organization. New hillfort centers were presumably supported by the intensification and specialization of economic activities to a larger degree than in previous periods, though Lika's challenging environment and topography likely made large-scale agriculture and livestock keeping difficult. We present new stable carbon and nitrogen isotope values for domesticated and wild fauna from hillforts and caves dating from the Middle Bronze to Early Iron Ages to examine changing sociopolitical and economic organization during this time. Our results suggest animal husbandry was carried out across multiple spatial and organizational scales to take advantage of finite resources, from the centralized movement of cattle and ovicaprid herds across greater swaths of the landscape to the continued management of pigs by individual households. The European Late Bronze Age was characterized by the rapid reorganization of social, political, and economic lifeways, resulting in larger and denser populations, new regional centers, and expanded trade networks that demanded increased economic specialization to sustain them. The intensification and diversification of agricultural and husbandry practices throughout Europe was one result of these new systems, and was carried out through the production of more durable tools from technological advancements in metal-working; development of intensive practices such as plowing, manuring, and irrigation; and the introduction of either new domesticates or management techniques that increased the productivity of old ones (Harding, 2002; Kristiansen and Larsson, 2005; Stevens and Fuller, 2012; Reed, 2013). Groups did not uniformly adopt these new changes, however, but instead engaged in complex decision-making processes that evaluated the potential benefits of new technologies and systems against perceived hazards and local environmental constraints. In more marginal environments with limited resources, these calculations were especially important. People, plants, and animals all require land for settlement, cultivation, and pasture, and expanding communities faced balancing these competing interests in ways that maximized yields without sacrificing efficiency or value (Allaby et al., 2015; Banks et al., 2013; Zavodny et al., 2017). 
In this paper, a diagnostic evaluation of the state of the art of archaeological waterlogged foundation piles in Riga Cathedral (1211 CE) was carried out. Microscopic, chemical and instrumental methods were applied to study the impact of deterioration of piles leading to the deformations of the Cathedral building. Severe biodeterioration by microorganisms in the majority of pile samples was determined. Chemical analyses showed an extensive depletion of hemicellulose and cellulose sugars while lignin seemed to be unaffected. Restricted degradation of hemicellulose sugars –arabinose and galactose – was characteristic of bacterial degradation of wood. FTIR spectroscopy proved itself as a quick and efficient method for determination of changes in wood components in foundation piles in comparison with chemical method. The increased ash content (up to 70%) in waterlogged wood consisted of deposited salts, oxides and other inorganic compounds. The X-ray diffraction method determined the main inorganic impurities, including calcite, quartz, sodium magnesium silicate and muscovite. The historic centre of Riga is included in the UNESCO World Heritage list. Riga Cathedral (1211) is one of the oldest sacred buildings of the medieval period in Latvia and also the Baltics. In recent years, deformations (cracks) have been observed in a number of historic buildings, including the Cathedral. Those buildings are supported by wooden foundation piles. Extensive research work with many expert institutions has been carried out to understand the reasons for the deterioration of Cathedral constructions. It is found that the mortar foundation is stable but the ground complex underneath, containing wooden piles and ground material, could be the main reason for super-normative deformations. The piles are situated under permanent groundwater; however, because of water level fluctuations, the upper parts of piles could be exposed above the water for longer periods. The main geotechnical problem is the declined load bearing of damaged piles which is unpredictable and can lead to an increased construction's settlement in future. The Riga Cathedral wooden piles underneath the mortar foundation were mainly deteriorated by erosion bacteria and in some cases by soft rot fungi. The degree of degradation was classified from moderate to total.
We report a teenager with childhood onset focal seizures associated with the chapeau de gendarme sign or ictal pouting of anterior insular lobe origin. The chapeau de gendarme sign has been associated with frontal lobe seizures in patients with focal epilepsy. However, in this case, stereo-electroencephalography (SEEG) localized seizures to the anterior insular cortex prior to her typical clinical manifestations. Surgical resection of the insular and frontal-lobe network resulted in seizure freedom. We propose that the anterior insular cortex should be a site of investigation during pre-surgical phase 2 evaluation in patients exhibiting the chapeau de gendarme sign during focal seizures. Frontal lobe seizures constitute nearly 30% of focal neocortical seizures [1] but remain difficult to characterize due to their complex semiology, large surface area and electrophysiological patterns [2]. One localizing sign that has been purported for frontal lobe epilepsy (FLE) is a characteristic down turning of the mouth into a deep frown, known as ictal pouting or ‘chapeau de gendarme’ (CdG) sign. While the CdG sign is not frequently seen, researchers and clinicians have used it to localize the seizure onset zone (SOZ) to the frontal lobe [3–5]. Due to the complexity of connections with the frontal lobe, however, other regions may initiate epileptogenic networks with complex propagations to the frontal lobe thereby resulting in false localizations for this unique semiological finding. 
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. Machine learning algorithms have been widely used in various fields such as data mining and pattern recognition, where support vector machine (SVM) (Vapnik, 1998; Frenay and Verleysen, 2010; Shawe-Taylor and Sun, 2011; Liu et al., 2012; Yang and Dong, 2018), neural network algorithms (Li et al., 2015; Kolbaek et al., 2017), ensemble learning (Rojarath et al., 2017), deep learning (Deng and Yu, 2014) and logistic regression (Yang and Qian, 2016) have been successfully used in classification, regression, function approximation, feature selection, feature extraction and so on. 
Depressive disorders in epilepsy often present characteristic clinical manifestations atypical in primary, endogenous depression. Here, we report a case of a 64-year-old woman with right mesial temporal lobe epilepsy, who complained of bizarre, antipsychotic-refractory cenesthetic hallucinations in her interictal phase, and was hospitalized after a suicide attempt. Detailed clinical observations revealed mood symptoms, which led to the diagnosis of interictal dysphoric disorder comorbid with interictal psychosis. Sertraline with low-dose aripiprazole markedly alleviated both depressive and psychotic symptoms. This case suggested that the two diagnostic entities may overlap and that depressive symptoms tend to be concurrent when concurring with psychosis, which hampers the appropriate choice of a treatment option. Patients with epilepsy have a higher incidence of psychiatric comorbidities than does the general population [1]. Psychiatric disorders associated with epilepsy are classified into those with a temporal relationship to seizures (periictal) and those without (interictal). Periictal psychiatric disorders can further be classified into those occurring during (ictal), preceding (preictal), and following (postictal) seizures. A large-scale study involving the general population revealed that depression was comorbid in 18% of adults with epilepsy, and psychosis in 9% [1]. Additionally, a meta-analysis that included studies of four populations found the prevalence of lifetime depression in patients with epilepsy to be 13.0% (95% CI: 5.1–33.1) [2]. A community-based study comprising patients with epilepsy identified a higher incidence of depression in patients with recurrent seizures than in those in remission [3]. 
To our knowledge, there are no reports of status epilepticus (SE) associated with mitochondrial diseases and treated with perampanel (PER). We present three cases of patients with refractory SE associated with MELAS syndrome who responded favorably to PER. All cases were diagnosed as non-convulsive SE (focal without impairment of level of consciousness). After an initial treatment with other anti-seizure drugs, PER was added in all cases (8, 16 and 12 mg) and cessation of SE was observed within the next 4-8 hours. All the cases involved a stroke-like lesion present on brain MRI. In our patients, PER was an effective option in SE associated with MELAS syndrome. Epilepsy is commonly described in mitochondrial diseases. The types of epilepsy reported in both adults and children include myoclonic seizures, focal to bilateral tonic–clonic seizures, epilepsia partialis continua, and generalized epilepsy syndromes [1–3]. The prevalence of status epilepticus (SE) in these patients is unknown and it is reported less often than other types of seizures, although it may go unrecognized. 
This work examines the carbon and nitrogen composition of human and animal collagen from the Roman necropolis of Lucus Feroniae (Rome, 1st–3rd century AD) and the Longobard cemetery of La Selvicciola in northern Latium (Viterbo, 7th century AD), with a special focus on possible dietary variations at the transition between classical and post-classical times. A substantial isotopic difference between the two series reveals distinct dietary practices at the two sites, especially the consumption of cereals and contribution of other foodstuffs to a mainly grain-based diet. We argue that such differences are explained through the social and cultural background of the two populations examined, where the isotopic variance of the Roman data is in line with that of a group of heterogeneous origin and varied dietary practices, while the tight clustering of isotopic signatures for the Longobard people reflects the foodways of a homogeneous group. Intra-site variation shows no significant difference according to the sex of the deceased. Outlying individuals might be explained through cultural practices that call for further insight. Isotopic data have shown to successfully reflect social and cultural phenomena of human groups in a changing world, in a way that other archaeological proxies have sometimes failed to achieve. There are no isotopic works in Europe focusing on the transition from classical to post-classical times. While most isotopic studies concentrate either on the first period (Chenery et al. 2010; Fuller et al. 2006; Prowse et al. 2005; Prowse et al. 2004) or the latter (Bourbou et al. 2011; Hakenbeck et al. 2010; Müldner and Richards 2005; Reitsema et al. 2010; Richards et al. 2006) a diachronic perspective is still lacking. However, the transition to the post-classical times is one of important changes, which involved a shift in social and economic aspects of everyday life for most populations in the western world. Health status and life conditions changed dramatically and human groups faced periods of instability that, among other aspects, are also testified in population replacement (Amorim et al. 2018). In Italy, in particular, with the fall of the Western Roman Empire (476 CE), the Gothic war and the following Byzantine domination, the contribution of Longobard migration to the Peninsula had a profound impact on the cultural as well as genetic background of the Italian people (Melucco Vaccaro, 1988; Rotili 2010). 
This publication aims to shed light on the influence that prior heating (burning) of mollusc shells during human activity may have on the results of radiocarbon dating. We compare the geochemical and mineralogical composition of heated and unheated shells of Anadara uropigimelana and Terebralia palustris recovered from Neolithic and Bronze Age archaeological contexts at Kalba, Sharjah Emirate, in the United Arab Emirates (UAE). Our research examined whether the heating of shells impacts on the determination of reservoir effects, or whether in spite of heating, this material remains a viable material for precise 14C measurements. Our results show that both heated and non-heated shells of A. uropigimelana and T. palustris provide consistent results, although the mineral composition of the shells changes from aragonite to calcite. Our results are important, since some of our selection of shells did not initially appear to have been heated. A heating process will then usually be detected as a greyish, marble like structure when cutting the shells. As a result of this work, we have also developed insights into prehistoric cooking practices of shells collected in Arabia. Our results provide archaeologists and associated researchers with confidence when assessing the results of radiocarbon dating during their studies of shells that might have been heated. Shells provide valuable sources of information within archaeology, whether for establishing chronologies for archaeological sites by means of radiocarbon dating and AAR (Kosnik et al., 2017) or through palaeoclimate and palaeoenvironmental studies (Biagi, 1994; Lindauer et al., 2017; Zazzo et al., 2016). This is especially so in arid environments where other organic matter such as peat or humic silt and clay are poorly preserved. Given the abundance of burnt shells in archaeological deposits (Barker et al., 2010; Stringer et al., 2008; Taylor et al., 2011), it is fundamental to understand the effect of heating on the geochemical composition of the shelly material, which in turn, may have consequences for the determination of reservoir effects during radiocarbon dating. An important result of our study has been to explore the link between modern ethnological studies of shell foraging and processing (heating) in Australia (Meehan, 1982) and ancient food processing methods in Arabia. Previously, the details of shellfish consumption in this latter region in later prehistory were largely unknown. Our data suggest that the shells were mainly used as food and they were likely being processed below short-lived, but hot fires. This conclusion sheds light on the dietary and cooking habits of the local populations, thus contributing to a better understanding of their lifestyle during the late Neolithic.
In this article we draw on suites of new information to reinterpret the date of the Java Sea Shipwreck. The ship was a Southeast Asian trading vessel carrying a large cargo of Chinese ceramics and iron as well as luxury items from outside of China, such as elephant tusks and resin. Initially the wreck, which was recovered in Indonesia, was placed temporally in the mid- to late 13th century based on a single radiocarbon sample and ceramic styles. We employ new data, including multiple radiocarbon dates and inscriptions found on some of the ceramics, to suggest that an earlier chronological placement be considered. The Java Sea Shipwreck was recovered in Indonesia in 1996 (Fig. 1). The ship was a trading vessel thought to have been sailing from Quanzhou in southern China to Indonesia, possibly Tuban on the island of Java (Flecker, 2003). The timing of this voyage has been debated; an issue that we address here. In this article, we use several new lines of evidence to argue that the Java Sea Shipwreck vessel may have sailed almost a century before previously thought (Flecker, 2003), possibly as early as 1162 CE. The diverse suite of data we consider includes comparative ceramic finds from land and underwater wreck sites, new radiocarbon dates, and inscriptions on ceramic pieces. Based on updated visual assessments of ceramic materials, the Jianning Fu inscription, and the suite of radiocarbon dates, it is very possible that the Java Sea Shipwreck vessel sailed almost 100 years earlier than initially thought, dating to the earlier part of the Southern Song dynasty (1127–1279 CE)—the heyday of export ceramic production in China—instead of during the transition from the Southern Song to Yuan dynasty. This was a time when the Chinese court encouraged private traders from China to travel abroad instead of relying on trade obtained through the tributary system. An earlier date range also would mean that the ship sailed closer to the fall of Srivijaya, one of Southeast Asia's greatest maritime empires, when other coastal polities, including the Kediri kingdom on Java, were competing for economic and political power.
We describe an adolescent girl with non-paraneoplastic anti-NMDA-receptor encephalitis (ANMDARE), who despite persistence of the extreme delta brush (EDB) pattern for nearly 2 years in her serial EEGs, she exhibited a speedy and sustained response to immunotherapy. To the best of our knowledge, our patient had the longest persistence of the EDB pattern on EEG reported to date. Our patient illustrates that, although presence of EDB supports the diagnosis of ANMDARE, its presence and persistence may not be a reliable predictor of response to immunotherapy and overall clinical prognosis. The red circle depicts the functional deficit zone over the left parietal lobe at or close to angular gyrus. Serial EEGs show left hemispheric polymorphic slow activity, maximally over the centro-parietal region, along with EDB. The montages for all EEGs are as magnified in the box.Unlabelled Image Anti-N-methyl-d-aspartate receptor encephalitis (ANMDARE) is a disorder presenting with subacute onset of seizures, psychosis, memory and language deficits, abnormal movements, and breathing and autonomic disturbances [1,2]. Although initially identified in association with ovarian teratomas, in a majority of children and young adults, ANMDARE occurs without tumors [2]. Three-fourths of patients with ANMDARE make substantial recovery with early diagnosis, tumor resection and immunotherapy, but relapses may occur in a quarter of them [1]. 
Bone modifications are associated with a broad range of agents, including carnivores, stone tools, sediments, etc., and can be categorized as one of two types: conspicuous or inconspicuous. Contrary to the larger, more easily identifiable conspicuous modifications, inconspicuous modifications are small, shallow and almost unnoticeable without the aid of a hand lens and strong light, making them harder and more time consuming to identify. This has led to arguments for their omission from tooth mark counts, even though their presence on the bones of archaeological and paleontological faunal assemblages have been recognized and mentioned in literature as having interpretive potential. This study employs visible light microscopy and high resolution scanning electron microscopy to present evidence that positively identifies inconspicuous carnivore modifications as abrasions that also have diagnostic morphology which differentiates them from abrasion created by other taphonomic agents. These inconspicuous carnivore marks may also be associated with the presence of soft tissue and so may help reconstruct the amount of flesh present at specific stages of carcass consumption. This study shows the interpretive potential of ICA may be substantial, requiring their inclusion in tooth mark counts for more accurate reconstructions of past carnivore behavior. Furthermore, understanding the etiology and implications of inconspicuous carnivore marks may provide a new way to interpret faunal assemblages that exhibit such marks, such as those associated with FLK Zinjanthropus, Tanzania. Such interpretations may be able to help develop stronger inferences regarding methods of hominin carcass acquisition. Conspicuous and inconspicuous modifications 
Fermentation is seldom considered in paleodietary analyses of Arctic and Subarctic peoples despite its ubiquitous traditional use as a cooking technique in high latitudes. Further, chemists have yet to assess the potential isotopic effects of fermentation on animal tissues, though isotopic research has documented measurable isotopic effects associated with other cooking techniques such as stewing and boiling. To measure the effects of fermentation on stable carbon and nitrogen isotopes, muscle tissues from 11 central Alaskan Chinook salmon (Oncorhynchus tshawytscha) were fermented following ethnographic methods. The results of this experiment demonstrate statistically significant isotopic differences in both carbon (−1.5‰) and nitrogen (+1.3‰) values taken from fermented muscle tissues compared to raw muscle tissues. Additionally, this study found a significant physiological fractionation effect between bone collagen and raw muscle (+3.3‰) consistent with previous research on Pacific salmon. In contrast to previous research, however, these tissues' nitrogen values show no significant fractionation. These findings have implications for central Alaskan dietary reconstructions, residue analyses, and our understanding of past subsistence practices in the Arctic and Subarctic. Dietary reconstructions are fundamental to anthropological assessments of past human behavior. During the last half-century, archaeologists have increasingly based dietary reconstructions on isotopic data. In contrast to faunal or paleobotanical analyses, an isotopic analysis can be directly applied to human remains to evaluate the types of food that individuals consumed in the past. However, accurate isotopic dietary reconstruction requires precise isotopic baseline values for food items in the trophic web (Schwarcz, 1991). Further, recent isotopic research has shown that certain food processing or cooking techniques have a significant isotopic effect, and therefore, dietary reconstructions on past human populations must also account for the effects of food processing techniques (Warinner and Tuross, 2009; Brettell et al., 2012). Fermentation is a culturally and historically significant fish processing technique, and it may yield a recognizable isotopic effect that anthropologists have yet to incorporate into isotopic dietary mixing models (Speth, 2017; Tanasupawat and Visessanguan, 2014). 
Photogrammetry is quickly becoming an important, cost effective technique for recording cultural heritage. Beyond the micro-scale of site evaluation, however, there are also effective landscape applications, with drone-based image collection allowing for large-scale survey. This combination of highly portable technology, which is not fully automated, can be used to create accurate and dense three-dimensional models at a fraction of the cost of LiDAR, and often at a much high spatial resolution. Yet, despite this, few studies have assessed the viability of this technique in regard to landscape studies. Those that have, such as Muñoz-Nieto et al. (2014), highlight the effectiveness of this technique and its ease of use. This paper assesses the viability of this technology for mapping large archaeological sites such as hillforts, providing a case study for its application to landscape archaeology. Photogrammetry is quickly becoming an important, cost effective technique for recording cultural heritage, with particularly impressive and innovative applications in excavation and artefact recording (De Reu et al., 2014; Dellepiane et al., 2013; Roosevelt et al., 2015; Lerma and Muir, 2014). Beyond the micro-scale of site evaluation, however, there are also effective landscape applications, with drone-based image collection allowing for large-scale survey. This combination of technology can be used to capture overlapping geo-referenced vertical and oblique photographs in order to create accurate and dense three-dimensional models at a fraction of the cost of LiDAR, and often at a much high spatial resolution. Furthermore, the hardware is highly portable, allowing the user to access a wider range of sites. The surveys have become fully automated and modern structure from motion software allows the user to create ortho-rectified aerial photographs and DSM's (Digital Surface Model) at the click of a button. Yet, despite this, few studies have assessed the viability of this technique in regard to landscape studies. Those that have, such as Muñoz-Nieto et al. (2014), highlight the effectiveness of this technique and its ease of use. This paper assesses the viability of this technology for mapping large archaeological sites such as hillforts, providing a case study for its application to landscape archaeology. SECTION Drone survey and photogrammetry There are obvious disadvantages to photogrammetry, the most notable being its inability to penetrate vegetation and tree canopy. However, without the proper conditions and sensors, LiDAR can also struggle in these environments. There are also the stated issues with regard to the absolute accuracy of photogrammetry data, thought there are ways to alleviate these issues (see above).
Excavations at the site of Kul Tepe in the Jolfa region in north-western Iran have unearthed various archaeological materials from Late Neolithic/Early Chalcolithic to Achaemenid periods (end of 6th millennium to 3rd century BC). During the Chalcolithic and the Bronze Age most lithic tools used in Kul Tepe were made of obsidian. From the first and second excavation seasons, 53 and 32 obsidian samples were selected and analyzed by pXRF. According to the results, the main source of obsidian for the workshops in Kul Tepe was Syunik, but other sources in the Lake Sevan Basin like Ghegam, Bazenk, Choraphor and Gutansar and the Lake Van region (Nemrut Dağ and Meydan Dağ) were utilized also. The site of Kul Tepe (E 45° 39′ 43″ - N 38° 50′ 19″, 967 m asl), as shown in Fig. 1, is located near the city of Jolfa (Hadishahr). It is a multi-period Tepe and covers an area of c. 6 ha with a preserved height up to c. 19 m. It is one of the larger prehistoric sites of the region, lying in a strategic position in the middle Araxes valley. It is located next to a broad valley at the crossroads of major routes linking the Iranian plateau to Anatolia, the Southern Caucasus and northern Mesopotamia. Based on cultural materials recovered during the first and second excavation seasons and according to radiocarbon dating, the following cultural phases at Kul Tepe are present: the Late Neolithic/Early Chalcolithic (Dalma), Late Chalcolithic (Pisdeli = LC1; LC 2 and 3 = Chaff-faced Ware), Proto-Kura-Araxes and Kura-Araxes I, Early, Middle, Late Bronze Age, Iron III, Urartian and Achaemenid periods (Abedi et al., 2014) (Table 1). 
Multiple-criteria decision-making (MCDM) typically assumes that crowds make completely rational decisions. In MCDM, a crowd as a whole, or its individual members, generally make decisions free from any influence of valence, arousal, emotional state or environment. In contrast, various theories dealing with crowd psychology (Gustave Le Bon, Freudian, Deindividuation, Convergence, Emergent norm, Social identity) analyze, in one form or another, the emotions of the crowd. According to above theories, crowd is influenced by a range of behavioral factors, such as physical, social, psychological, culture, norms, and emotions. It can be argued that the emotional state, valence and arousal of crowds affect their decision making to a considerable degree and multiple criteria crowd behavior modeling must, therefore, consider this impact as well. In this light, the integration of crowd simulation and biometric methods, behavioral operations research and emotions in decision making has taken a prominent place as it leads to a better understanding of crowd emotions and crowd decision making. In this context, the authors developed the Affective Analytics of Demonstration Sites (ANDES) that added to this body of research in four ways. The crowd analysis and simulations conducted with ANDES used a neuro decision matrix. The matrix contains a detailed description of demonstration sites (public spaces) in question and the emotions, valence, arousal and physiological parameters of people present there. With ANDES’s Remote Sensor Network, emotional (emotions, valence, arousal) and physiological (average crowd facial temperature, crowd composition by gender and age group, etc.) parameters of people present at demonstration sites can be mapped. ANDES can assist experts in more effective implementations of public spaces planning and a participation process by attendees by collecting and examining various layers of data on the emotional and physiological parameters of visitors based on a visitors-centric public spaces planning approach. ANDES can determine the public space and real estate values. Both scholars and practitioners have been attempting over the past century to evaluate the emotions of residents quantitatively. Such data was meant for practical applications when executing important events and exhibitions and when planning public spaces and cities. For example, Saroff and Levitan (1969) use opinion surveys and sampling methods in the urban planning process. They have analyzed various quantitative and qualitative data, while paying special attention to human emotions. The techniques, technologies and systems for planners during these times were mostly limited to questionnaires, interviews and the like, but there were no tools for analyzing emotions in an urban setting and public spaces (Zeile et al., 2015, 2016). Throughout the development of the Fourth Industrial Revolution, technologies emerged for applying remote biometrics and physiological measures of emotions at events, exhibitions and public spaces in real time. Nonetheless, to date, the use of remote biometrics and physiological systems is rare. These emerged biometrics technologies could support studies on the human emotions and people’s physiological states found in the public spaces along with human circadian rhythm, weather conditions, pollution in a combined way. Then applications of such data along with information and mining outcomes could serve as additional knowledge for the planning objectives of events, exhibitions and public spaces. Such is the purpose of this research involved in developing the Affective Analytics of Demonstration Sites (ANDES). 
Within the evolutionary dynamics of post-Gravettian techno-complexes, one can observe an intense regionalization phenomenon, both on a European scale, with the creation of two main provinces, and within the Italian peninsula. To date, typological studies have led to the recognition of several Italian Epigravettian facies, identifying trends, similarities, and differences in the lithic complexes. An important contribution was made by the technological method which in recent years has allowed us to identify the evolutionary processes of the lithic industries in numerous deposits of northern Italy. It is the intent of this reporting to add information which contributes to the debate on the latest Pleistocene complexes, expanding the area to southern Italy. The Grotta della Serratura, by virtue of a large and well-detailed stratigraphy, the optimum state of conservation of the findings, and for previous interdisciplinary studies that have been undertaken, represents an excellent reference site for the lower Tyrrhenian Italian coast. Thus it begins to bring new technological data to already known collections in an area rich in archaeological evidence that can make important contributions to the discussion of the lithic complexes in the Mediterranean area from the late Upper Paleolithic. The paleogeographic changes which followed the last glacial maximum raised natural barriers and boundaries that probably form the basis of the division into two large post-Gravettian techno-complexes: that of central-western Europe, where the chrono-cultural unity of the Solutrean, Badegoulian, Magdalenian, Epimaddalenian/Azilian are developed, and the eastern and Mediterranean Europe, defined on a techno-typological and chrono-cultural scale as “Epigravettian province”. 
We report a case of a 52-year-old man with drug-resistant temporal lobe epilepsy, with post-ictal violent aggressive behaviors. Postictal violent outbursts would occur 3–4 times per year following clusters of seizures or generalized tonic–clonic convulsions. The violent outbursts were traumatizing for his family, and lead to multiple emergency department presentations as well as conflicts with police over the course of nine years. After initiation of pindolol the patient has had no episodes of violent behavior in two years despite experiencing the same frequency and severity of seizures as before pindolol. The abrupt cessation of postictal violent outbursts after introduction of pindolol in this case provides a novel management option for the treatment of postictal violence in patients with drug-resistant epilepsy and supports the importance of the beta adrenergic and potentially serotonergic systems in postictal violent behavior.  
Modelling of archaeological remains in the landscape is a common tool in multidisciplinary research projects that focus on the movement of material and people. The interactions of human, material, and environmental spheres, however, depend on the scale, permeability and availability of the environment. This article deals with the construction and range of activity spheres in Early Medieval eastern France. A transformation period necropolis at Niedernai that dates to the 5th and early 6th century shows long-term settlement continuity with regional communication networks and local land-use strategies among other locally scattered communities. The consistently populated site-catchment in the Upper Rhine Valley was chosen to estimate the theory of large migration processes in the 5th century. Spatial analyses of GIS-based environmental and remotely sensed geomorphological data derived from satellite imagery were conducted. Social structure and social organisation changed at the transition from the Imperial Period to the Early Medieval Ages, affecting land use strategies and economic developments. Larger infrastructural networks were replaced by small-scale dwelling assemblages indicating local spheres of influence in a rather fragmented landscape. Human-environmental networks are based not only on physical but also mental concepts of the surrounding landscape on different spatio-temporal levels (Butzer, 1982; Brather, 2017). Landscapes not only provide resources but also include a system of layered cultural, social, religious and cultic meanings (Brather, 2011; Zimmermann et al., 2005; Fleming, 2006; Bintliff, 2008; Lüning, 1997; Meier, 2009, 2017; Doneus, 2013; Ermischer, 2004). Cognitive maps (Ingold, 2000; Butzer, 1982) reflect socio-cultural as well as pragmatic and economic interests (Gramsch, 1996), that result in modified land-use methods and local human-environmental interaction. In Early Medieval archaeology, however, fluent transformation of landscape concepts due to changes in physical conditions, technical enhancements and administrative circumstances in the late 5th and early 6th century AD are traditionally linked to large-scale migration processes after the so-called ‘Fall of Rome’ (e.g. Von Rummel, 2013; Fehr, 2010). Depending on the scale, permeability and availability of the environment, ‘barbarian invasions’ can rather be replaced by intra-site mobility, fragmentation and social remodelling of small groups displaying various identities in a joint environment (e.g. Halsall, 1999). In this case, the study site represents a part of the periphery of the former Roman Empire that has been selected to conduct GIS-based spatial analysis of the Early Medieval archaeological record, the surrounding geological and pedological units, as well as the modern landcover deriving from satellite imagery. Within the broad research area of the Upper Rhine Valley, a 5th and 6th century graveyard with 31 graves and 32 human individuals at Niedernai, France (Brather-Walter, 2015) permitted the assessment of land-use strategies of a small and continuous population at the transition to the Merovingian Period. The interconnections between graveyards and settlements were evaluated in terms of distance analysis to further estimate a possible modern bias caused by present-day residential coverage. SECTION Material and methods SECTION Regional settings 
Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%. Internet of Things (IoT) is a concept where physical, digital, and virtual objects (i.e., things) are connected through a network structure and are part of the Internet activities in order to exchange information about themselves and about objects and things around them (Singh and Kapoor, 2017). IoT enables devices to interact not only with each other but also with services and people on a global scale (Akeju et al., 2018). The development of this paradigm is in constant growth due to the continuous efforts of the research community and due to its usefulness to a wide range of domains, such as airports, military, and healthcare (Singh and Kapoor, 2017; Sarhan, 2018). 
A multi-proxy palaeoenvironmental study (pollen, non-pollen palynomorphs, charcoal particles, mollusk macrofauna) of coastal marshland in Doñana National Park (southwestern Iberian Peninsula) was undertaken to trace environmental change, human activities related to woodland clearance, and past land-use during the mid-late Holocene (~5000–2800 cal BP). The results of this study are combined with archaeological data from the Copper and Bronze Ages and are subsequently compared with those of similar research carried out at the south-westernmost part of Europe with the aim of discerning regional differences or similarities. Our research has allowed us to recognize climate changes and four extreme wave events in the Guadalquivir paleoestuary, which might have contributed to both the cultural change that is observed in the archaeological record at the end of the Chalcolithic and the subsequent population decline during much of the Bronze Age. During the Upper Pleistocene and the Holocene, the south-western extreme of Europe is a territory of major archaeological and anthropological interest due to its border position at the crossroads between Europe and Africa and between the Atlantic Ocean and the Mediterranean Sea (López-García and López-Sáez, 1994a, 1994b; Pérez-Díaz et al., 2017). In the southwest of the Iberian Peninsula it has been documented the earliest known use of marine resources was by Neanderthals ~150 kyr ago (Cortés-Sánchez et al., 2011) and was a crucial reservoir of biodiversity during the Upper Pleistocene and early Holocene (Carrión et al., 2008; Cortés-Sánchez et al., 2008). Southwest Iberia is also key to understanding the neolithization process in the Iberian Peninsula; evidence of Neolithic settlement is present from at least 7500 cal BP, when an agricultural and food producing economy quickly replaced the coastal economies of the Mesolithic populations after the 8200 cal BP abrupt climatic event (López-Sáez et al., 2011; Cortés-Sánchez et al., 2012). Southwest Iberia is a unique area for the study of the funerary record and demography from the Mesolithic to the Copper Age (Díaz-Zorita et al., 2012), when some highly-elaborate tombs, such as Montelirio (Seville), feature ceremonial and rich burial goods unparalleled in Chalcolithic Europe (Fernández-Flores et al., 2016). Importantly, this region exhibits sharp cultural disruptions at the time of the 4200 cal BP climatic event and is, therefore, crucial for tracking plausible population movements between the SW and the SE of the Iberian Peninsula (Lillios et al., 2016; Blanco-González et al., 2018). Finally, the timing and forms of interaction between local Late Bronze Age and Early Iron Age societies and the expansion of Mycenaean, Phoenician, and Greek influence, as evidenced by artefacts, are the subject of intense debate (López-Sáez et al., 2002b; Celestino-Pérez et al., 2008). A multi-proxy approach has proved a powerful tool to identify environmental changes ~5000–2800 cal BP in the present-day marshes of the Doñana National Park. Major changes in vegetation composition and structure of the marshland ecosystems couple with paleoestuary configuration, climate variability, and human dynamics. The low marsh and both freshwater-submerged and floating aquatic macrophyte communities dominated during the humid periods, when the Guadalquivir paleoestuary harboured a brackish lagoon. In contrast, in the arid periods (e.g. beginning from 4.2 kyr cal BP) the high marsh was the dominant vegetation and more salt – as a consequence of an EWE – concentrated in the ground. The amplitude of the vegetation response shows the elasticity and resilience of marshland communities to changes in the morpho-sedimentary environment. In any case, the aquatic habitats of the Doñana marshland are enormously heterogeneous and fluctuating, depending on the marine influence, the tides, and the Holocene evolution of the paleoestuary. For all these reasons, it must be understood that the marshland ecosystem is a mosaic of plant communities in which species with very different ecologies can coexist.
We report a child with Lennox–Gastaut syndrome with an increase in seizure frequency and loss of psychomotor skills due to a disintegrated cervical VNS lead, not detected during standard device monitoring. The lead was completely removed and replaced by a new 303 lead on the same nerve segment. After reinitiating VNS, side effects forced us to switch it off, resulting in immediate seizure recurrence. EEG recording demonstrated a non-convulsive status epilepticus that was halted by reinitiating VNS therapy. Thereafter, he remained seizure free for eight months, and regained psychomotor development. Lennox–Gastaut syndrome (LGS) is one of the most challenging epilepsies to manage, due to a range of different seizure types which are frequently refractory to anti-seizure drug treatment. In patients with drug-resistant epilepsy, treatment options other than anti-seizure drugs are often considered; including epilepsy surgery, vagus nerve stimulation (VNS) therapy and a ketogenic diet. 
Identifying the range of plants and/or animals processed by pounding and/or grinding stones has been a rapidly developing research area in world prehistory. In Australia, grinding and pounding stones are ubiquitous across the semi-arid and arid zones and the associated tasks have been mostly informed by ethnographic case studies. More recently, plant microfossil studies have provided important insights to the breadth of plants being exploited in a range of contexts and over long time periods. The preservation of starch and/or phytoliths on the used surfaces of these artefacts is well documented, though the factors determining the survival or destruction of use-related starch residues are still largely unknown. Some of these artefacts have also been used for grinding up small animals and these tasks can be identified by specific staining methods for organic remains such as collagen. In this study, 25 grinding and pounding stones identified during an archaeological project in arid South Australia, were examined for starch and collagen residues. The artefacts were from 3 locations in central South Australia, all located in exposed settings. Of these localities, Site 11 in the Western Valley near Woomera is an important Aboriginal landscape specifically associated with male ceremonial practice in the recent past. The remaining two sites, one in the adjacent Nurrungar Valley and the other near Andamooka 100 km distant, have unrestricted access and potentially a different suite of residues. The Kokatha Mula Nations, the Traditional Owners of Woomera, requested that this study be undertaken to explore the range of plants that may have been processed here. It provided an opportunity to investigate the preservation potential of starch and collagen on grinding stones; explore the range of taphonomic factors involved in the persistence of residues in extreme environmental conditions; and test the methodological developments in identifying specific plant origin of starch residues. Of the 25 grinding/pounding stones tested, 7 yielded starch grains. Geometric morphometric analysis identified 3 economic grass species, Crinum flaccidum (Andamooka Lily) and Typha domingensis (Bulrush/Cumbungi). Folded collagen was identified on one artefact. Oral histories recount the movement between Andamooka and Nurrungar/Western Valley for men's ceremonies, and documented in the movement of stone resources, e.g. oolytic chert. The survival of residues in this environment and the identification of economic plant taxa complement the current knowledge of ceremonial activities and the movement of people and resources across significant distances in arid South Australia. Grinding technology emerged in the Late Pleistocene across the globe, and the specific tasks associated with these implements varied considerably through time and space (e.g. Piperno et al., 2000; Fullagar, 2006; Fullagar et al., 2008, 2015; Liu et al., 2014; Field et al., 2016; Louderback and Pavlik, 2017; Barton et al., 2018). The distribution varies with environment, and in Australia, while grinding stones can be found in most environmental zones, they are ubiquitous across the arid and semi-arid regions. Numerous shapes and forms have been described (e.g. Smith, 1999; Smith et al., 2015), and the uses and tasks performed (involving both plants and animals) have been described in the ethnographic literature (e.g. Hayden, 1979; Gould, 1980), or identified in functional studies (e.g. Fullagar et al., 2008). Evaluating the survival of use-related residues on the surfaces of stone artefacts varies considerably with environmental conditions and is rarely predictable. In this pilot study, evaluating the persistence of ancient starch and other residues has resulted in the identification of a number of economic plant taxa as well as collagen in one instance. Crinum flaccidum and Typha domingensis are well known as important economic plants across the eastern half of Australia, and in this study, have been recovered from grinding stones in a marginal environmental setting of the Australian arid zone.
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. Barley, Hordeum vulgare, is an important cereal crop in the temperate world, used as both food and feed (reviewed in Newton et al., 2011). It is a diploid and self-fertilizing species, first domesticated around 10,000 years ago (Zohary et al., 2012). Studies of geographical distribution of genetic diversity suggest multiple domestication events (Dai et al., 2012; Morrell and Clegg, 2007; Ren et al., 2013; Wang et al., 2016) or an origin from a mosaic ancestry from wild progenitors from a wide geographic region (Poets et al., 2015). 
The decision-making trial and evaluation laboratory (DEMATEL) method is a hot issue in industrial engineering field for it can help determine critical factors in complex system. Although lots of efforts have been spent on improving the DEMATEL, they are just the extensions from the subjective perspective but lack of the objective perspective. This study focuses on providing a new improved DEMATEL method based on both subjective experience and objective data. In order to reasonably determine the initial direct-relation (IDR) matrix, the basic probability assignment (BPA) function is employed to extract expert experience and the Dempster’s rule with Shafer’s discounting is employed to make combination to derive the subjective IDR matrix. Then the path analysis is suggested to test each possible influence relation included in the subjective IDR matrix, and the objective IDR matrix consisting of path coefficients of any two factors is derived by training the sample data of factors. Following the principle of one-vote negation, the Dempster’s rule is once again used to make combination for two kinds of IDR matrices, based on which an algorithm for the new improved DEMATEL is summarized to find the major factors in a complex system. Finally, numerical comparison and discussion are proposed to demonstrate the applicability and superiority of the prosed method. The decision-making trial and evaluation laboratory (DEMATEL) method was originally developed by the Science and Human Affairs Program of the Battelle Memorial Institute of Geneva between 1972 and 1976. The DEMATEL as a common multi-criteria decision making (MCDM) method is aimed at describing the basic concept of contextual relations and identifying cause–effect chain components/factors for a complex decision problem (Gabus, 1973; Fontela, 1974). In the present study, we proposed a new improved DEMATEL method based on both subjective experience and objective data, where the two perspectives of subjectivity and objectivity of a complex problem could be well considered. The main contributions of the present study can be summarized into four aspects.
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. Ceramics are one of the main types of evidence that archaeologists work with on a daily basis. Due to their ubiquity, durability, and chronological sensitivity, even in the age of scientific dating methods ceramics still form the basis of many chronological frameworks, especially in Chinese archaeology (Hein, 2016). This tendency to define archaeological cultures based on ceramic types is highly problematic, for the relationship between ceramics and past identity groups is by no means straightforward. Ceramics form a complex entity shaped by the nature of the raw material, the available technology, the abilities and preferences of the potter as well as their ideas of what a specific vessel should look like. These ideas, in turn, are shaped by the users and observers who engage with these wares, all of them acting within various overlapping communities of practice (Wenger, 1998). Research on early ceramics aims to understand these communities and their interplay – both locally and across space and time. Such insights are made possible by the nature of ceramics; they preserve traces of the various production stages, thus allowing for the reconstruction of the chaînes opératoires that created them. Furthermore, individual vessels can be compared to each other and fitted within larger assemblages to reconstruct whole craft traditions (Stilborg, 2017: 658–9). 
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. The term Popoloca became confused because of the connotation given by the Aztecs to many groups that were not Nahuas (Jäcklein, 1991). At present this denomination (ib.) applies to indigenous groups of Puebla (Mexico) that linguistically belong to the Otomangue family. The term “Historical Popoloca” used by Jäcklein refers to the group that existed during the Early Classic period (ca. 200 CE), and he considers (ib.) to the proto–Otomangues to be the predecessors, who in ca. 7000 BCE began the domestication of several plants. The Popoloca people lived in the southern and central regions of the State of Puebla, the northern zone of Oaxaca, and perhaps in the eastern zone of Guerrero and the south of Tlaxcala. However, so far, insufficient studies have been conducted regarding about this ethnic group. According to the typology, ceramics of the Groups 1, 2, and 5, and the ungrouped samples cover an extensive chronology for the sites of Metzontla Valley to the Early Preclassic–Late Postclassic periods. The exemplars of Group 6 correspond to the Late Classic–Late Postclassic periods, and those of Group 4 are later (Middle–Late Postclassic period). Finally, those of Group 3 (Teteles de Santo Nombre) correspond to the Late Preclassic–Midlle Classic periods.
This provenance study of yellow-firing clays in north central New Mexico examines whether clays recovered in the vicinity of Tunque Pueblo (LA 240) may have been used as slip clays at contemporaneous San Marcos Pueblo (LA 98). A sample of 72 ceramic sherds, bricks, and clays were analyzed through chemical characterization using laser-ablation inductively coupled plasma mass spectrometry (LA-ICP-MS). We argue that Tunque potters were using a subset of clays available at their village to produce pottery. Although San Marcos potters appear to have possibly been using clay from Tunque Pueblo to slip their vessels, these clays were not the same as those used by Tunque potters. Given San Marcos potters' apparent reliance on this slip clay over time, we argue our findings demonstrate that extremely stable social networks were developed and sustained among Rio Grande Pueblo households and communities across north central New Mexico during the late prehispanic and early colonial periods (1400–1680 CE). Among ethnohistoric potters, the raw materials used to make pottery often come from a variety of locations. Some resources such as clay or temper are widely available and are typically obtained close to the area of production (Arnold, 1985). Other resources, such as slip clays or pigments, are more limited in their availability and potters may travel longer distances to obtain these materials (Dillingham, 1992; Najohai and Phelps, 1998; Parsons, 1932). Acquisition of these more specialized materials are embedded within social and economic networks of varying scales; this may have also been the case in the past (Herhahn, 2006; Huntley et al., 2012; Nelson and Habicht-Mauche, 2006). In this study, we are specifically examining the provenance of yellow-firing clays used in the production of Ancestral Pueblo glaze-painted pottery at two villages in north central New Mexico. The results of the current study, as well as previous studies of the circulation of lead for glaze-paint production in this same region (Habicht-Mauche et al., 2000, 2002; Huntley et al., 2007; Huntley, 2008; Nelson and Habicht-Mauche, 2006), suggest that inter-community ties as reflected in resource procurement were amazingly stable through time. But this project is only one of a suite of projects attempting to understand the movement of different types of resources up and down the Rio Grande valley. Once completed, these studies will need to be synthesized to create a broader understanding of the nature of different scales of social networks at play in this period of ethnogenesis. In developing such a synthesis, we suggest that Whiteley's concept of ‘relationality’ may be of great value (Whiteley, 2004). Physical access to material resources may have been of far less significance than perceptions of exchange dependencies. Social fictions about the adequacy of specific resources might have played a role in the genesis of identity on the one hand, and the development of exchange networks on the other. For example, making yellow-slipped pottery may have been an important ethnic marker for San Marcos potters, but the yellow slip clay may also have had to have originated from Tunque Pueblo. As Whiteley explains (2004: 149), such social fictions preserve exchange dependencies because difference must be “established or recognized as a means for exchange, and thereby alliance, producing distinctions at one level simultaneous with identity at another.”
We address the problem of jellyfish polyp counting in underwater images. Modern methods utilize convolutional neural networks for feature extraction and work in two stages. First, hypothetical regions are proposed at potential locations, the features of the regions are extracted and classified according to the contained object. Such methods typically require a dense grid for region proposals, explicitly test various scales and are prone to failure in densely populated regions. We propose a segmentation-based polyp counter — SegCo. A convolutional neural network is trained to produce locally-circular segmentation masks on the polyps, which are then detected by localizing circularly symmetric areas in the segmented image. Detection stage is efficient and avoids a greedy search over position and scales. SegCo outperforms the current state-of-the-art object detector RetinaNet (Lin et al., 2017) and the recent specialized polyp detection method PoCo (Vodopivec et al., 2018) by 2% and 24% in F-score, respectively, and sets a new state-of-the-art in polyp detection. Jellyfish (Scyphozoa) blooms have attracted significant interest of researchers over the recent years (Kogovšek et al., 2018; Brotz et al., 2012; Condon et al., 2013). During blooming, the jellyfish population rapidly increases in a relatively small geographical area, which importantly affects the local ecosystem. Bloom and jellyfish population dynamics prediction can be established by analyzing the population dynamics of polyps, which are one of the many forms in the jellyfish complex life cycle. 
Around 2900–2300 cal BCE, mobile stockbreeders introduced the Neolithic Corded Ware culture (CWC) into the Eastern Baltic. Here, a Central or Northern European Neolithic economy and ideology took hold despite differences in burial practices. Although around 90 CWC graves are known in the region their contents have not been intensively studied. Here, we present new AMS radiocarbon (14C) measurements and carbon and nitrogen stable isotope data obtained on human bone collagen, molecular and isotopic data obtained from ceramic beakers, and user-wear data of flint and bone tools from several CWC graves, Benaičiai, Biržai, Krasnasieĺski, Dakudava 5, and Drazdy 12, in Lithuania and Western Belarus. The bone collagen δ13C and δ15N stable isotope data are rather homogenous and demonstrate that the majority of consumed protein was derived from terrestrial resources. Organic residue analysis of two CWC beakers yielded lipids consistent with ruminant carcass and dairy fats, whilst use-wear analyses indicates that bone pins, flint blades and axes were used as grinders, functional tools or had been carefully renewed before deposition respectively. The peoples of the Corded Ware culture (hereafter CWC) are traditionally regarded as mobile stockbreeders who brought animal husbandry into the Eastern Baltic between ca. 2900–2300 cal BCE. The presence of domesticated faunal remains within CWC contexts as well as stable isotope data obtained on human bone collagen has demonstrated that terrestrial derived protein was preferentially consumed when compared with the preceding Subneolithic hunter-gatherers who relied heavily on aquatic resources (Lõugas et al., 2007; Piličiauskas et al., 2017b, 2017d). Throughout Europe the CWC appears abruptly and differs to all preceding cultures in terms of material culture. Thus, nearly 100 years ago it was postulated that the CWC was brought into Europe by a mass migration of pastoralists from the Pontic steppe (Childe, 1926; Gimbutas, 1979); a hypothesis that has been critiqued numerous times (e.g. Lang, 1998; Furholt, 2014; Beckerman, 2015) until recently proved by genetic analyses (Allentoft et al., 2015; Haak et al., 2015; Saag et al., 2017; Mittnik et al., 2018). Overall, the specific material culture and burial practices of the CWC in the Eastern Baltic do not have their roots in any local hunters-gatherers' cultures. Flexed burials including an array of grave goods, for example stone and flint axes, ceramic vessels, flint knives, bone pins and other tools, link the Eastern Baltic CWC with Central Europe despite the absence of burial mounds.
Constructing effective models for detecting, reducing, and/or preventing adverse events is very important in domains such as aviation safety, healthcare, drug administration, and war theaters. This study presents batch and data streaming models to detecting adverse events using data from a war theater context. In all the previous studies, regression models and several machine learning techniques were used for predicting continuous values in an active theater of war, and the error values reported on the test sets were large. In order to overcome the shortcoming, this study investigates the effectiveness of batch and data streaming classification algorithms in detecting or classifying adverse events given infrastructure development spending data and other variables in an active theater of war in Afghanistan. By the feature selection, the valid input variables are obtained and their indexes show that the input variables are mainly the adverse events (t-1) at the previous month, the population densities and related project investments. From the country level, fewer of the 14 project investments affect the adverse events. From the region level, some projects with higher index values, such as Security in the South Western region, Energy and Emergency Assistance in the North Eastern region, and Education in the Eastern region are mainly affecting factors. Three batch classification methods and three data streaming classification methods were assessed for their ability to detect adverse events given infrastructure development data. The study uses cost-sensitive measures to address the very unbalanced nature of the data and it applies variable reduction techniques to identify significant variables. The three batch classification algorithms are C4.5, k-nearest Neighbor, and Support Vector Machine. The three data streaming algorithms are Naïve Bayes, Hoeffding Tree, and Single Classifier Drift. In general, the performance of the cost-sensitive methods in the batch setting is comparable to those in the data stream setting. However, in the batch setting the cost matrix needs to be adjusted manually. In contrast the data stream setting allows one to adjust the models based on the analysis of the classifiers’ performance over time and changing data distribution. The Kappa values using Naïve Bayes are the highest in the three data stream algorithms in the whole country and its regions. The Naïve Bayes classifier has the best global performance. By the Kappa statistic curve, we can observe the concept drifts. In a region level, many models have a better performance including more investments related to project compared with those in a country level. In addition as data distribution becomes more balanced, the classifiers in the data stream setting outperform in terms of the overall classification rates in comparison to the classifiers in the batch setting. The results thus demonstrate the potential of data streaming algorithms to significantly outperform when the data become less unbalanced, and can be used for detecting adverse events in similar areas. Building reliable models for predicting, detecting, mitigating, and/or preventing adverse events is crucial in the aviation industry (Reveley et al., 2011), healthcare institutions (Rafter et al., 2015; Rochefort et al., 2015), drug administration (Armitage and Knapman, 2003; Casillas et al., 2016), and an active war theater (Çakıt and Karwowski, 2018) to name a few. In the last case, adverse events are caused by terrorist activities that primarily target the civilian population in countries such as Afghanistan. The U.S. Department of Defense (DoD) developed the Human Social Culture Behavior (HSCB) modeling program (Bhattacharjee, 2007; HSCB Modeling Program, 2009) to help the military better understand different cultures while conducting a war in overseas countries, to supervise the human terrain, to shelter the civilians from terrorist activities (Drapeau and Mignone, 2007), and finally to undertake infrastructure development efforts to stabilize the country of Afghanistan, and consequently to counter or reduce terrorist events. However, assessing the effect of these efforts presents challenges as the data used to build models exhibit nonlinear and fuzzy behavior and are often ill defined with respect to their socio-economic-cultural factors. 
The Late Bronze Age (ca. 1700/1600–1050 BCE) in the Aegean started with strong connections between societies in the region and beyond, and was accompanied by the collapse of palatial polities around 1200 BCE. The collapse led to unrest and migration in the East Mediterranean. In the present study, we focus on settlement contexts dating to the transition between the Mycenaean palatial and post-palatial periods (ca. 1250–1050 BCE) in Greece, which saw the destruction of the Mycenaean palaces (ca. 1200 BCE). We aim to shed light on trade connections and mobility in the region during this substantial period through ancient DNA of livestock. We sequenced pig and cattle mitochondrial DNA (mtDNA) from Tiryns, Greece - a key Bronze Age site in the Aegean region. We discovered an Italian pig haplotype in palatial Tiryns. This is the first time that this particular haplotype is found outside Italy. By contrast, a genetic haplotype of Near Eastern descent (Y1) that was present in the Mycenaean palatial period cannot be ascertained in the post-palatial period. Whether comparable changes in the composition of livestock are also to be found in cattle, we are not able to say, because only the palatial period samples yielded ancient DNA. The results of this study corroborate the published data on mtDNA of pigs from the Mediterranean Basin from the Bronze and Iron Ages. They suggest that in the Mediterranean, pigs were translocated through various patterns of mobility; by Italian migrants to Mycenaean Greece as well as by other mobile groups (“Sea Peoples”) to the Levant. Ancient DNA is a powerful tool to reveal ancient translocations of species, and pigs serve a good proxy for tracing patterns of human mobility and interconnections. The Late Bronze Age (ca. 1700/1600 BCE–1050 BCE), was a period of an upsurge in interconnections between societies around the Mediterranean basin. These patterns of connectivity were driven by trade in precious raw materials and finished objects of metal, ivory, glass/faience, wood, pottery, etc. (Burns, 2010; Cline, 1994, 2007). Human mobility during this period ranging from travels of individuals to migration of entire groups of people also enhanced the dissemination of knowledge, materials and objects (Burns, 2010; Cline, 2007). The Bronze Age in the Aegean Basin was a period of great changes. Patterns of mobility in this period are well captured in the mtDNA of pigs. During this time, pigs with Near Eastern signature were brought to Greece, Sardinia and Sicily, while pigs with European signature were translocated east to Anatolia and the southern Levant. We also detected, for the first time, connections between Greece and Italy using ancient DNA of pigs.
Hypotheses on why the 1845 Franklin expedition to the Arctic ended in tragedy include suggestions of lead (Pb) poisoning. Hair keratin was sequentially analysed for isotopic ratios and lead concentrations from remains of a Franklin expedition member, tentatively identified as HDS Goodsir, buried on King William Island in the Canadian arctic. We approximated lead concentrations in Goodsir's blood to elucidate a pattern of lead burden in the three months prior to death. Lead isotope ratios in Goodsir's hair were almost identical to that of the bodies discovered on King William and Beechey Islands. The lead concentrations (73.3–84.4 ppm) reflect more immediate exposure and are high by today's standards. Estimated blood lead concentrations (~53.6–61.3 μg/dL), suggest that lead exposure, while high, may not have been sufficient to cause worsening physical and mental symptoms. Lead ingestion likely occurred during the expedition; however, it is probable that multiple factors are responsible for the loss of the expedition, of which lead exposure may have been one. In 1845, British explorer Sir John Franklin departed on a voyage to find the North-West Passage, a sea route that links the Atlantic Ocean to the Pacific. The expedition was to complete its mission within three years and return home, but the two ships, HMS Erebus and HMS Terror, and the 129 men aboard vanished in the Arctic. The last Europeans to see them alive were the crews of two whaling ships on Baffin Bay in July 1845, just before they entered the Arctic Archipelago (Beattie and Geiger, 2014). Several rescue expeditions were unsuccessful in locating Franklin and his men, and in 1859, a note from the expedition was discovered on King William Island. It was dated May 1847 and indicated that all was well, but an addendum, written in April 1848, stated that 24 members, including Sir John Franklin, had died by that date and the remaining 105 left their icebound ships and aimed to reach safety via the Back River on the Canadian mainland. 
This study represents the first starch grain analysis undertaken in Palau, performed on a sediment core extracted from a sinkhole on Ulong Island. Radiocarbon dating indicates the core spans the likely period of human occupation on Ulong (ca. 3000 years) as established by prior archaeological evidence. Samples were analysed for macrocharcoal, starch content, and geochemical composition. The results of the analyses indicate an initial period of intensive clearance and gardening from ca. 3000–2000 BP, during which banana (Musa spp.), yams (Dioscorea spp.), Polynesian arrowroot (Tacca leontopetaloides), Tahitian chestnut (Inocarpus fagifer), and breadfruit (Artocarpus sp.) were being utilised and/or cultivated. This initial phase was then followed by a period of reduced and stabilized gardening activity until ca. 1000 BP, during which banana (Musa spp.) disappears from the starch record. The period after 1000 BP represents the transition between the first permanent settlements on Ulong, abandoned between 500 and 300 BP, and the arrival of Europeans in 1783. This period is marked by a dearth of charcoal indicating the absence of significant burning, as well as a decrease in the variety of starch grains from cultigens. Starch grain analysis in the Pacific Islands 
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Dynamic economic emission dispatch (DEED) is a hot issue in power systems, and it is a multi-objective constrained optimization problem. DEED has two objectives including the fuel cost and pollutant emission minimizations. The former serves to realize considerable cost savings, and the latter serves to reduce the effect of pollutants from the point of view of environmental protection. Both objectives are subjected to several constraints, such as the generator capacity constraints, power balance constraints and generating unit ramp-rate constraints. Furthermore, the goal of DEED is to simultaneously minimize its two objective functions while satisfying all the equality and inequality constraints, which can be realized by scheduling the power outputs of generating units according to the predicted customer load demands over a certain number of time intervals. Mathematically, to appropriately characterize the DEED problem, it is necessary to establish a comprehensive and accurate evaluation model from the two aspects of multi-objective decision making and constraints handling. In this paper, a new enhanced harmony search (NEHS) algorithm has been proposed for dispatching the power outputs of different generating units over twenty-four time intervals, and its main goal is to find compromise solutions for the fuel cost and emission minimizations of the DEED problems. Moreover, a normalized objective function considering both objectives is employed to evaluate the solutions obtained by NEHS. In NEHS, the harmony memory considering and pitch adjusting operations are modified to improve the entire quality of HM. On the other hand, a repair technique is developed to help the newly generated harmony vector to get into the feasible solution space. Empowered by this repair technique, NEHS is capable of obtaining the solutions with the characteristics of strict feasibility and high quality. To demonstrate the superiority of NEHS on solving the DEED problems, another four algorithms including IHS, DHS, HS-NPSA and MHS are employed to compare with it from different points of view such as objective function value and average running time. These HSs are tested on five different instances consist of the 5-unit, 6-unit, 10-unit, 14-unit and 30-unit systems. The first three instances consider the transmission losses in the power balance constraints, and the last two instances neglect them. Experimental results show that NEHS is more efficient than the other four HSs for the DED, DEmD and DEED problems and has the potential to find desirable solutions in less running time.
Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices. Digital advancements in Internet of Things (IoT) arena make the lives of humans better than ever before. Sensing and tracking of human activities have become an inevitable part in various fields like surveillance, entertainment, healthcare, etc. Thus, human gesture or activity recognition gains a lot of research interest, especially in areas that require human–machine interaction in some form. Several IoT protocols are implemented for various applications like sensing soil moisture (Boada et al., 2018), monitoring and controlling smart building (Vo et al., 2018), detecting human (Shukri et al., 2016) and stuffs (Nickels et al., 2013), human activity (Razzaq et al., 2018; Bhat et al., 2018; Hossain et al., 2018; Wang et al., 2015) and gesture (Abdelnasser et al., 2015) recognition, locating objects (Nezhadasl and Howard, 2019), finger printing localization (Janssen et al., 2018), crowd sensing (Alvear et al., 2018), smoke alarm (Wu et al., 2018), healthcare (Malik et al., 2018) and location tracking (Hong et al., 2018). IoT protocols like ZigBee, Z-wave, Bluetooth, Long Range (LoRa), and Wi-Fi are the widely used protocols for human activity and gesture recognition applications. Device-free gesture recognition adopting model-based and learning-based approaches are broadly discussed in this paper. Robust recognition prerequisite appropriate data acquisition methods along with signal processing or pre-conditioning techniques, as it attributes to performance. Approaches utilizing CSI traces could achieve more accurate recognition accuracy by precisely capturing the action granularity. Predominantly, Wi-Fi CSI sensing considered being more convenient than the conventional methods, due to its privacy preserving and non-intrusive characteristics.
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. Although it is impossible to know their precise meaning to those who made and wore them, archaeological beads were likely signals of status, prestige and beauty, as they are in contemporary societies. Beads first appeared in the archaeological record as non-standardized perforated aquatic shells that are thought to have been strung and worn suspended on the body (e.g., d'Errico et al., 2009; Henshilwood, 2007). The first deliberately shaped, standardized ornaments are ostrich eggshell (OES) beads that date to the end of the Middle Stone Age (MSA) (Miller and Willoughby, 2014), and which subsequently became common during the Later Stone Age (LSA) of eastern and southern Africa. While beads hold great scholarly significance because of what they represent in terms of human cognition and sociality they are also the products of a complex technological process, which is itself worthy of study. 
The present archaeometric study of the Punic black-gloss ware found at the “Roman Temple” of Nora (south-western Sardinia, Italy), dated at the end of the IV century BCE and the beginning of the II century BCE, was addressed to better define the exchanges of Punic ware, ideas and production skills within the west Mediterranean Sea. Petrographic and microstructural analyses at the scanning electron microscope (SEM) clearly indicate that the analysed pottery can be referred to two different productions, for which different base-clays were used, indicating different geological origin. On the basis of chemical composition, the black-gloss ware found at Nora can be traced back in part to the northern African area and in part to the western Sardinia coast at Tharros. None local production was identified. Moreover, on the basis of both the mineralogical composition and the microstructural features, samples produced in the northern African area and in western Sardinia definitely differ in terms of firing temperature. Therefore, on the basis of these results, a more complete scenario can be drawn on the commercial traffic active in the Western Mediterranean and in Sardinia between the end of the IV century and the first half of the II century BCE. Under a social perspective, Sardinia, thanks to its Phoenician, Punic and Italic influences, became an incubator of ideas, techniques and production knowledge that, from the interaction of the preceding cultures and their influences, gave life to many local productions, which were part of the greater phenomenon of Punic black-gloss ware in the western Mediterranean (Sardinia, Sicily, Northern Africa and the southern coasts of the Iberian Peninsula). Punic black-gloss ware, dating to the Hellenistic Age, is a ceramic class extensively found in the whole Punic (referred to the culture of Carthage) area as well as in those under the Punic influence. The development of this ceramic class finds its origins in the need of counteracting the decrease of tableware caused by the progressive downfall of the exports of Attic pottery, between the end of the IV century and the first half of the II century BCE. The term Punic refers to all local productions, the production techniques and shapes related both to the knowledge inherent to the Punic cultural substratum and to different external influences, initially from the Greeks and then from the Italics. The archaeometric study of the black-gloss ware found at the so called "Roman Temple" of Nora indicates that the pottery was imported in part from Carthage and in part form Tharros, a Punic settlement located about 150 km north of Nora. It is worth considering that the occurrence of carbonate inclusions, such as limestone, calcite and shells, is considered to improve clay body workability, shock resistance and toughness of the resulting pots (Bronitsky and Hamer, 1986; Feathers, 1989, 2006; Skibo et al., 1989; Hoard et al., 1995; Tite et al., 2001; Feathers et al., 2003; Maritan et al., 2005; Tenconi et al., 2013; Allegretta et al., 2016a). Therefore, the choice of using Ca-rich clay materials for the production of the black-gloss ware and the firing temperatures similar to those characterising the productions of Carthage, suggest artisan mobility and knowledge transfer from northern Africa to the regions of the Punic domain, analogously to what observed for the emigration of Attic potters (MacDonald, 1981). This idea of know-how and craftsmen transfers from Africa to Sardinia is well consistent with the information derived from the literary and archaeological sources which give us many clues and indicators about an ample colonial movement of people from northern Africa to Sardinia between the end of the VI century BCE and the beginning of the V century BCE.
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model. Millions of data is generated in multiple scenarios, including weather, census, health care, government, social networking, production, business, and scientific research. Such high dimensional data may increase inefficiency of classifiers, as they possess several irrelevant or redundant features. Therefore, it is necessary to preprocess the dataset before applying any classification algorithm. Feature selection is a preprocessing step to remove irrelevant and/or redundant features and offers more concise and explicit descriptions of data. Feature selection has got wide applications in data mining, signal processing, bioinformatics, machine learning, etc. (Iannarilli and Rubin, 2003; Jaeger et al., 2002; Jain et al., 2000; Kohavi and John, 1997; Kwak and Chong-Ho, 2002; Langley, 1994; Webb and Copsey, 2011; Xiong et al., 2001). 
A 52-year-old female with a longstanding history of drug-resistant epilepsy that included focal impaired awareness seizure presented at end of service of her vagus nerve stimulator (VNS) generator. She had undergone a generator replacement in 2010 without complication. However, her latest replacement was accompanied by multiple bouts of asystole. We discuss the case, possible causes of the asystole, and its relevance to the future of VNS generator replacement and epilepsy treatment. Drug-resistant epilepsy remains one of the most common neurological conditions, as well as one of the most challenging. Even as understanding of the etiology improves, epilepsy remains troublesome for both the patient and physician to treat. An estimated 1% of the population suffers from epilepsy, with one-third left uncontrolled by two or more anti-seizure medications or other possible therapies [1]. In this drug-resistant population, in which seizure freedom is unlikely, vagus nerve stimulation (VNS) remains an effective adjunct therapy. Multiple studies have demonstrated its effectiveness including Révész et al. who investigated the effects of 130 patients implanted with a VNS between 2000 and 2013 and showed an increased response rate from 22.1 to 43.8% from the first and fifth year of implantation regardless of pharmacological anti-seizure drug (ASD) treatment changes [2]. 
We develop a novel divide-and-conquer framework for image restoration and enhancement based on their task-driven requirements, which takes advantage of visual importance differences of image contents (i.e., noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components) and sparse prior differences of image contents for performance improvements. The proposed framework is efficient in implementation of decomposition-processing-integration. An observed image is first decomposed into different subspaces based on considering visual importance of different subspaces and exploiting their prior differences. Different models are separately established for image subspace restoration and enhancement, and existing image restoration and enhancement methods are utilized to deal with them effectively. Then a simple but effective fusion scheme with different weights is used to integrate the post-processed subspaces for the final reconstructed image. Final experimental results demonstrate that the proposed divide-and-conquer framework outperforms several restoration and enhancement algorithms in both subjective results and objective assessments. The performance improvements of image restoration and enhancement can be yielded by using the proposed divide-and-conquer strategy, which greatly benefits in terms of mixed Gaussian and salt-and-pepper noise removal, non-blind deconvolution, and image enhancement. In addition, our divide-and-conquer framework can be simply extensible to other restoration and enhancement algorithms, and can be a new way to promote their performances for image restoration and enhancement. Image restoration and enhancement have been significant topics in image processing and computer vision, and a large number of image restoration and enhancement algorithms are widely applied in the fields of medical image restoration (Eldaly et al., 2018; Zhang et al., 2017b), underwater image or video enhancement (Ancuti et al., 2018) and remote sensing fusion (Wang et al., 2019). The aim of image restoration is to recover an ideal image from a degraded image according to degradation principles, while the target of image enhancement is to enhance an original image by promoting useful characteristics and inhibiting uninteresting information according to specific requirements. The former is an objective process to restore an ideal image from image degradation model, while the latter is a subjective process of improving image quality by referring to human visual perception. However, their common purpose is to improve image visual quality based on respective principles. 
The clinical significance of Xp22.31 microduplication is still unclear. We describe a family in which a mother and two children have Xp22.31 microduplication associated with different forms of epilepsy and epileptiform EEG abnormalities. The proband had benign epilepsy with centrotemporal spikes with dysgraphia and dyscalculia (IQ 72), the sister had juvenile myoclonic epilepsy, and both had bilateral talipes anomalies. The mother, who was the carrier of the microduplication, was asymptomatic. The asymptomatic father did not possess the microduplication. These data contribute to delineate the phenotype associated with Xp22.31 microduplication and suggest a potential pathogenic role for an epilepsy phenotype. Xp22.31 microduplication is one of the most frequent findings in clinical cytogenetic analysis [1,2]. The frequency varies according to the criteria of sample selection, ranging from 0.04% in multicenter studies based on noninvasive prenatal testing [3], and 2.4% in patients with mental retardation [4]. In patients with epilepsy, Olson et al. [5] found at least one copy number variant on chromosomal microarray in 323 out of 805 studied cases (40%), and 30 of these (9.3%) had Xp22.31 microduplication. Recently, Addis et al. [6] found this duplication in 2.2% of patients with benign childhood epilepsy with centrotemporal spikes (BECTS). Even if the clinical significance of the rearrangement is still debated, the most recent studies confirm its possible pathogenic role, although probably not independently but instead linked to additional genetic factors [7]. The phenotype is variable is prevalent in neurocognitive and behavioral disorders, with seizures reported in 3–44% of cases [2,5–8]. Dysmorphic features, talipes anomalies, and feeding difficulties may also occur [5–8]. Severity and intensity of the phenotypes are variable; intellectual disability ranges from mild to severe mental retardation, in some patients associated with autism spectrum disorder, speech and reading difficulty, dyslexia, and attention deficit hyperactivity disorder. Also, the epilepsy phenotype varies from neonatal seizures to BECTS, Dravet-like epilepsy, and drug-resistant myoclonic epilepsy [2–8]. In the present study we analyzed four members of a family in which two children possess Xp22.31 microduplication associated with different forms of epilepsy. SECTION Material and methods The family we studied provides a contribution to the literature and help define a common phenotype related with Xp22.31 duplication, with special attention to the epilepsy. Our result underlines the need for further studies on mechanisms that influence its expressivity. SECTION Consent
Excavations conducted in the context of the Palaepaphos Urban Landscape Project (PULP) have revealed a defensive monument of the Cypro-Classical period (fifth and fourth centuries BCE), which had been preserved under an anthropogenic mound (tumulus) of the 3rd century BCE. Besides stone-work, the construction of the monumental rampart made extensive use of mudbricks. In 2016–2017, PULP introduced a pilot study based on analytical techniques (pXRF, SEM-EDS, granulometric and petrographic analysis) to address issues relating to the manufacture and construction of the earthen architecture of the rampart. The paper presents a description of the geoarchaeological analyses and their results, which have highlighted specific manufacturing practices in relation to the construction of the monument. Given that the rampart constituted a major investment of the royal authorities of ancient Paphos, the results provide new information on the production of earthen building materials and also on environmental choices with respect to raw material selection in the context of a public project carried out by a central authority circa the mid first millennium BCE. Since the 1980s numerous geoarchaeological studies in the Near East have showcased the importance of earthen architectural analysis not only in relation to the built environment but as an expression of social agency (French, 1984; Friesem et al., 2011; Goldberg, 1979; Love, 2013; Morgenstein and Redmount, 1998; Rosen, 1986). In the eastern Mediterranean island of Cyprus mudbricks were, and are, an integral part of its architectural identity. While the earliest have been recovered in the context of Neolithic sites (e.g. Aurenche, 1981; Love, 2012; Philokyprou, 1998), mudbricks are still in use today as a key component of vernacular architecture (Costi De Castrillo et al., 2017; Illampas et al., 2011). This pilot study has shed light on earthen architecture practices in relation to a recently discovered public monument that was built in the Cypro-Classical period, almost certainly around 500 BCE. Mineralogical, geochemical and archaeological data have been combined to investigate manufacturing and construction events, and they have provided manufacturing recipes and motivational choices regarding raw material selection - such as the use of sand as degreaser. Macroscopic and microscopic data, on the other hand, led to the identification of the different steps in the chaîne opératoire from raw source collection to construction techniques.
Primary Angiitis of the central nervous system is a rare and poorly understood variant of vasculitis. We narrate a case of a 46-year-old male who presented with new onset refractory status epilepticus mimicking autoimmune encephalitis. In this case we are reporting clues that could be useful for diagnosis and extensive literature review on the topic. New onset refractory status epilepticus (NORSE) is a complex disorder, characterized by status epilepticus that is refractory to treatment with no identifiable infectious, inflammatory or brain structural abnormalities [1]. NORSE poses considerable distress to physicians due to its heterogeneous etiology and devastating outcome [2]. An identifiable cause in patients with NORSE is discovered in the majority of cases, however in some the cause remains unknown despite extensive investigations [3]. Primary Angiitis of the central nervous system (PACNS) presenting with seizures was reported in the range of 7–29% [4,5]. 
Obsidian was an important resource for prehistoric people in the North American Southwest. Elemental analysis of obsidian samples from Southwestern archaeological sites has been widely successful in identifying the raw material sources used by the people in the region. X-ray fluorescence (XRF) is the most commonly used method for sourcing obsidian artifacts because it is a rapid, non-destructive analytical technique with high discriminating ability. In this study, we analyzed over 450 pieces of obsidian from several Sinagua sites near Flagstaff, AZ. Obsidian debitage and unfinished points are common on these sites. The majority of points, and almost all the debitage, are from Government Mountain, the expected primary source. A few other sources have a minor presence, and it is likely that any points made of obsidian from these sources reached the sites as completed arrowheads, rather than being made there. Our limited data from survey finds and early points on late sites suggest that the pre-Sinagua Archaic populations were more mobile and used a wider variety of stone sources. Identification of obsidian sources through chemical characterization has been a productive avenue of archaeological research for some 50 years. In the American Southwest, prominent obsidian sources were exploited over the whole span of regional prehistory, and their products widely spread. Our archaeological projects in northern Arizona focus on the prehistoric culture known as the Sinagua, who occupied a zone of cultural contacts not far from heavily-exploited obsidian sources. In sourcing a large number of obsidian artifacts from our sites, we hoped to see patterns of change through time and reflections of the Sinaguas' place in regional systems of exchange and cultural contact, as previous work in the region has suggested (Brown, 1991; Horn-Wilson, 1997; Rondeau, 1979; Shackley, 2005). SECTION The Sinagua Rather than travel large distances or engage in long-distance exchange for obsidian, the Sinagua exploited the closest sources, especially the plentiful and good quality obsidian at the Government Mountain source, the closest to our sites. They also used the RS Hill source, a neighboring source in the San Francisco Volcanic Field, to a lesser extent because it was more difficult to collect obsidian at this source. Sinagua relations with the Cohonina, the archaeological culture around the obsidian sources, are currently not well understood. They probably changed through time, but as far as we can tell, the supply of obsidian and the sources used did not change greatly.
This paper examines the pre-contact history of the eulachon fishery on the northern Northwest Coast of North America through multiple lines of evidence: zooarchaeological, ethnographic, and oral historical. The eulachon fishery and eulachon oil production was central to Northern Tsimshian socio-political relations, systems of ownership, and trade during the contact-and-post-contact period in the region. We bring together the results of an analysis of 15 fine-screened faunal assemblages collected from village sites in Prince Rupert Harbour and compare these with published northern coast village and camp fine-screened faunal assemblages. Our results show that eulachon and other smelt taxa are present in these assemblages, suggesting a deep history to the eulachon fishery. We suggest also that the paucity of eulachon remains at some sites could be explained by eulachon oil production and consider what lines of evidence are needed to explore the limitations of zooarchaeological data and the history of eulachon oil production in the future. Archaeologists have long understood the critical role of coastal and anadromous resources in Northwest Coast (NWC) subsistence practices, but recent research is illustrating the historical depth, variability, and complexity of Indigenous fisheries in the region (Brewster and Martindale, 2011; Cannon et al., 2011; McKechnie and Moss, 2016; Moss, 2012, 2016; Moss and Cannon, 2011; Orchard and Szpak, 2015). Evidence for salmon and herring fishing, for example, exceeds 10,000 years at some sites (McKechnie et al., 2014), while coast-wide studies document the enormous diversity that existed in pre-contact-period fisheries (McKechnie and Moss, 2016; Moss and Cannon, 2011). An important contribution of these works is that they challenge conventional progressive-evolutionary models regarding NWC fishing, which have tended to emphasize salmon at the expense of other fish and to couch explanations for broader societal change in terms of intensification (e.g. Ames and Maschner, 1999; Matson and Coupland, 1994). These researchers have shown, for example, that salmon were not necessarily the “prime mover” in the development of social complexity in California, the Coast Salish area, or in Haida Gwaii (Bilton, 2014; Orchard and Clark, 2014; Orchard and Szpak, 2015; Tushingham and Christiansen, 2015); that typical indicators of intensification such as fish weirs, domestic dwellings, and other examples of the built environment have deep historical roots (Grier, 2014; Martindale et al., 2017a; Smethhurst, 2014); and that activities surrounding the capture, processing, storage, and consumption of resources other than salmon also influenced settlement practices, were central to systems of ownership, and structured socio-political relations (Cannon et al., 2011; McKechnie and Moss, 2016; Tushingham and Christiansen, 2015). 
Microwear analysis of lithic debris offers an aid to field archaeologists in need of focusing excavation activity at the parts of Stone Age sites that are least disturbed by post-depositional processes. In this progress report we describe the ridge-wear approach and report on its application to the submerged Mesolithic settlement of Orehoved, Denmark. Test-pitting had provided worked flints from two distinctive layers within an area of ca. 2.8 ha, located 4–7 m below present sea level. Through analysis of wear on dorsal ridges on flint flakes we demonstrate that the assemblages of artefacts from both strata spanned the whole range from minimally-rounded to extremely-rounded. Spatial and statistical analyses indicate that both layers contain redeposited artefacts. Major parts of higher ground areas were therefore considered so disturbed by sea-level rise that further excavation should be avoided here. Follow-up excavation in a more low lying area located a third and stratigraphically deeper layer with numerous artefacts in stone, bone, and plant material in situ. Lithic microwear analysis is commonly applied when studying wear patterns on the edges of stone tools to infer their prehistoric use. Wear, however, can occur on the surfaces of stone artefacts through a variety of natural as well as cultural processes. Here, we report on the examination of wear on artefacts collected by a test-pit survey in 4–7 m of water in a locality of expected Kongemosian habitation near the port of Orehoved, Denmark (Fig. 1). The results showed that artefacts from the two excavated stratigraphic layers were redeposited most likely from wave action and by erosion from water currents. 
Christianson syndrome (CS) is an X-linked intellectual disorder caused by mutations in the SLC9A6 gene. Clinical features of CS include an inability to speak, truncal ataxia, postnatal microcephaly, hyperkinesis, and epilepsy. Almost all patients with CS develop drug-resistant epilepsy—its most serious complication. We report two cases of CS with drug-resistant epilpesy associated with the Lennox–Gastaut syndrome (LGS). One patient experienced generalized tonic seizures since 9 months of age with cognitive regression, which evolved to include atonic seizures at the age of 7 years. Electroencephalography (EEG) showed generalized slow spike–wave complexes and generalized paroxysmal fast activity. Seizures remained drug-resistant despite multiple anti-seizure drugs. The second patient experienced generalized tonic seizures since the age of 17 months and arrested development. EEG showed generalized slow spike–wave complexes, with frequent atonic seizures since the age of 6 years. Electrical status epilepticus during slow-wave sleep (ESES) developed at the age of 7 years. Our cases illustrate that CS may cause LGS in addition to other developmental and epileptic encephalopathies of the neonatal and infantile period. We suggest that generalized tonic or tonic–clonic seizures and generalized slow spike–wave complexes in interictal EEG be included as potential electroclinical features of epilepsy in CS. Christianson syndrome (CS) is an X-linked condition that was first reported by Christianson et al. in South African families who displayed severe intellectual disability, epilepsy, mild craniofacial dysmorphology, ophthalmoplegia, speech absence, acquired microcephaly, and cerebellar and brainstem atrophy [1]. Later, patients were also reported from other regions in Europe [2]. SLC9A6 was identified as the causative gene; moreover, clinical features in patients with CS may overlap with those seen in Angelman syndrome [2]. Pescosolido et al., based on a study on 12 families note the characteristic symptoms of CS include an inability to speak, moderate to severe intellectual disability, epilepsy in 100% of patients, truncal ataxia, postnatal microcephaly or growth arrest of head circumference, and hyperkinesis [3]. Female carriers are known to manifest mild to moderate intellectual disabilities, behavioral issues, and psychiatric illnesses [4]. 
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. Orduña, located in the central-northern extension of the Iberian Peninsula (see Fig. 1) shows a long economic and commercial history that has had in its custom system and in its vicinity to major trade routes the basis of its existence (Salazar and Llano, 2005). It is located in the bottom of a valley surrounded by mountains reaching 1000 m. Both its strategic location and its topography, have allowed and enhanced its pivotal role connecting the Spanish plateau with the Basque coastline villages (Salazar and Llano, 2005; Angulo, 1995). Moreover, from the first medieval settlements in the 13th century, several trade agreements have ensured the nature of the village as an important exchange point of goods (Salazar Arechalde, 1995). The archaeometrical characterization performed by ICP-MS, XRD and SEM-EDS techniques, along with the subsequent chemometric treatment, has allowed identifying six compositional groups linked to the local pottery production of Orduña during the 17th and 19th centuries and one exogenous: four tin-lead glazed (ORD-A, ORD-B, ORD-C, ORD-D), two honey glazed (ORD-MEL-A and ORD-MEL-B) and one micaceous (ORD-MIC). All tin-lead glazed have shown Ca rich pastes (9.31 wt%–19.7 wt% CaO), whereas the other groups have very low concentrations of Ca.
Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy. The growing access of people to information and communication technologies is contributing to reach the so-called “big data era”, where the pervasiveness of data is a major input for increasingly personalized and automated online services. One of such services is online advertising, which aims at selecting and directing ads to the right potential customers (personalization) at the right time (real-time), built on multiple parameters, while users browse the Web (Smith, 2014a; Real-time bidding protocol, 0000; Yuan et al., 2012). Undoubtedly, the main privacy concerns regarding online advertising come from the great capability of third parties to aggregate user data. Due to the inherent opacity of this ecosystem, the most known approaches to face such concerns build on radical ad blocking solutions. By entirely blocking ads and partly stopping the leakage of data from the user side, these radical approaches are threatening the current economic model of the Web. On the other hand, with the aim of balancing the trade-off between revenue and the number of invited DSPs (looking for more user privacy), we propose to modify part of the ad delivery model. Our technique arises as a strategy of bid request suppression where interactions carrying user data can be reduced, by design, to offer more privacy, while slightly affecting the revenue of the system. More specifically, we come up with a controlled distribution of bid requests among DSPs in order to reduce the amount of user data shared with said third parties. Nevertheless, our approach comes at the expense of revenue loss incurred by lowering the number of participants within ad auctions. Since this technique would be applied directly in the core of ad platforms, more overwhelming and less harmful results could be obtained.
The main focus of the present study was to explore the longitudinal changes in the brain executive control system and default mode network after hemispherotomy. Resting-state functional magnetic resonance imaging and diffusion tensor imaging were collected in two children with drug-resistnt epilepsy underwent hemispherotomy. Two patients with different curative effects showed different trajectories of brain connectivity after surgery. The failed hemispherotomy might be due to the fact that the synchrony of epileptic neurons in both hemispheres is preserved by residual neural pathways. Loss of interhemispheric correlations with increased intrahemispheric correlations can be considered as neural marker for evaluating the success of hemispherotomy. Drug-resistant epilepsy is one of the most common neurological disorders in childhood, and it is viewed as a disease of hypersynchronous neuronal activity in its electrophysiological substrates [34]. The normal developmental trajectory of the pediatric brain can be affected by epilepsy [21]. Achieving permanent seizure freedom is the ultimate goal in clinical practice. Surgical intervention is the most common treatment of drug-resistant epilepsy. For patients with multilobar or hemispheric drug-resistant epilepsy hemispheric disconnection has been long used in clinical practice. This surgical tool can be classified into two types: anatomic hemispherectomy and functional hemispherectomy [17]. Anatomic hemispherectomy was the first procedure of choice [2]. This procedure has classically involved removal of the temporal, frontal, parietal, and occipital lobes, sparing the thalamus, basal ganglia and insular cortex. Although seizure outcome was excellent after anatomic hemispherectomy, the technique has been abandoned in many centers because of the high incidence of delayed potentially fatal complications. To reduce the risk of complications, a new technique called functional hemispherectomy (hemispherotomy) was developed. Hemispherotomy is a procedure to maximally disconnect the white matter connecting the diseased hemisphere, while performing minimal cortical resections [5]. Previous studies have shown that hemispherotomy is an important treatment for seizure reduction in patients with unilateral drug-resistant epilepsy [12,15,19]. As is known, the two hemispheres of the brain are mainly connected with the corpus callosum and subcortical pathways. The pathophysiologic basis for the use of hemispherotomy is based on the hypothesis that the spread of seizure activity between the two hemispheres of the brain is mainly transmitted through the corpus callosum and subcortical pathways [1,12,18]. Therefore, severing these major cortico-cortical connections between the two hemispheres should dramatically reduce seizures. Over the years, numerous studies in humans have proved the potential advantageous effects of this procedure [13,24,26,32,39]. This study used a multimodal neuroimaging method to study the longitudinal effect following hemispherotomy surgery in two individuals with epilepsy. The results demonstrated that the brain connection pattern in children with drug-resistant epilepsy is altered following hemispherotomy. Longitudinal multimodal connection analyses of ROIs in brain ECS and DMN found that the children with different surgical outcomes demonstrated different trajectories of their brain reorganization. The child with seizure freedom showed a loss of interhemispheric correlations and increased intrahemispheric correlations in the unaffected hemisphere. Our findings support the contention that residual interhemispheric pathways is a primary reason for seizure recurrence in children after unsuccessful hemispherotomy. Our present study provided a non-invasive means of demonstrating persistent connectivity between the two hemispheres. The longitudinal outcome analysis and multimodal technology can be used for predicting the need for repeat hemispherotomy. Using multimodal neuroimaging our method may have potential in identifying patients with a high risk of seizure recurrence after hemispherotomy. SECTION Conflict of interest
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. We are in the midst of a paradigm shift in our conception of human-environmental relationships and explanatory models for cultural change on the north Pacific coast of North America. At the time of European contact, relatively large concentrations of people lived in substantial plankhouses, participated in elaborate ceremonies, and were organized into ranked social units—attributes generally associated with agriculturalists. Until recently, anthropologists viewed the hunter-fisher-gatherer peoples of the Pacific Northwest as atypical foragers. Accounting for this outlier status became the focus of attention. Explanations first emphasized inherent abundance of resources such as salmon, then the spatial and temporal patchiness of resources and the importance of technological and social means of increasing productivity (Ames and Maschner, 1999; Matson and Coupland, 1995). Competition among social groups for prestige, recruitment, resource rights, and control of storable commodities, exchanged through feasting or trade, were viewed as key factors in driving changes in social organization and subsistence adaptation (Coupland, 1985; Hayden, 1995). Changes in the use of animals were routinely used to explain many of these trends, for example, increased use of salmon (Oncorhynchus spp.) or marine mammals in some locations has been used as an explanation for observed changes in artifact distributions and/or household organization (Matson, 1992; Matson and Coupland, 1995). Our paper presents key findings from the Čḯxwicən faunal analysis project. Drawing on the H.E. framework, we explored the long-term dynamic relationships between people, animals, and the environment at the traditional village of the LEKT, located in coastal Washington, U.S.A. Because of its wide-ranging scope, H.E. research is an ambitious undertaking (Fitzhugh et al., in this issue). Our complex, multi-pronged effort to integrate faunal records, high resolution geochronology and independent environmental data illustrate these ambitions. We hope the framework can serve as a model for future research on coastal foragers in the Pacific Northwest and beyond.
A 21-year-old male with an SCN1A mutation died of cerebral herniation 3 h after a seizure occurring during physical activity. Cases of fatal cerebral edema in patients with SCN1A mutations after fever and status epilepticus have been recently reported raising the question whether sodium channel dysfunction may contribute to cerebral edema and thereby contribute to the increased premature mortality in Dravet Syndrome. We report on our patient and discuss whether the combination of hyperthermia and ion channel dysfunction may not only trigger seizures but also a fatal pathophysiological cascade of cerebral edema and herniation leading to cardiorespiratory collapse. Patients with Dravet Syndrome (DS) are at a significantly increased risk for premature death (16–17 per 1000 patient years), with sudden unexpected death in epilepsy (SUDEP) representing the leading cause of death in childhood followed by status epilepticus [1,2]. Despite sporadic reports of acute encephalopathy in DS, cerebral edema is not traditionally considered a key pathological mechanism contributing to high mortality in DS. Recently Myers et al. report 5 fatal cases of cerebral edema occurring days after status epilepticus in children with DS. We were recently confronted with a similar case in our emergency department (ED). This was remarkable because our DS patient experienced fatal transtentorial herniation within three hours of a generalized convulsive seizure despite being immediately aborted with buccal midazolam. We therefore find it relevant to report on this case and bring to attention that there may be circumstances under which fatal cerebral edema develops rapidly in patients with DS. SECTION Case: 21 year-old male with SCN1A mutation 
Morphological and morphometric bone variation between archaeological wolves and the oldest domestic dogs commonly are used to define species differences. However, reference data often have been based on small numbers, without robust statistical support. We consulted the literature on these matters in all possible languages and tested many of the proposed species differences by examining wolf and dog skeletons from several collections, accompanied by an extensive synthesis of existing literature. We thus created large reference groups, assessing data distributions and variability. We examined mandible height, width, length, and convexity; contact points of the skull on a horizontal plane; caudal shifting of the border of the hard palate; skull size; carnassials tooth size reduction; micro-anatomical differences in teeth, snout, and skull height; and snout length and width. Our results show that skull length and related size; skull height; snout width; orbital angle; P4 and M1 mesio-distal diameter can help (albeit to a limited extent) to distinguish the oldest archaeological dogs from wolves. Based on our observations, we re-evaluated recent large Pleistocene canids reported as Paleolithic dogs and concluded instead that they fit well within the morphomentric distributions seen with Pleistocene wolves. The research presented here reflects the recent trend to critically re-evaluate axiomatic assumptions about wolf-dog differences, and to rephrase the morphological and morphometric definition of an early archaeological dog in a more suitable manner. These results are important to the international archaeological community because they place historical reports in a newer context, and create a robust (although narrow) framework for further evaluation of archaeological dogs and wolves. Researchers have examined the origins of dogs for over a century (Galton, 1865; Gaudry and Boule, 1892; Huxley, 1880; Nehring, 1888; Rütimeyer, 1861; Studer, 1901; Verworn et al., 1919; Wolfgram, 1894) and have used specific morphometric and morphological criteria to assign specimens as dog or wolf. We propose that these generally accepted criteria require reanalysis based on larger datasets. Here, we address this need for critical re-evaluation by combining new and prior data to create an updated reference framework including several of the older publications, often not in English, that have been poorly consulted by the English scientific community. We explain questions of robustness, our measurements, and the outcomes, and illustrate our findings by reconsidering earlier reports of large Pleistocene canids (Camarós et al., 2016; Germonpré et al., 2009; Germonpré et al., 2012; Germonpré et al., 2015; Germonpré et al., 2017; Ovodov et al., 2011; Pidoplichko et al., 2001; Sablin and Khlopachev, 2002). SECTION Materials and methods
Duplication of the methyl-CpG-binding protein 2 gene (MECP2) is a rare condition that results in epilepsy in half of the cases. Although this condition has been well characterized in the literature, there is a lack of research on MECP2 duplication-related epilepsy and its management. We present the case of an eleven-year old male with MECP2 duplication and epilepsy, who was resistant to polytherapy. The patient responded well to valproic acid (VPA) initially and upon re-challenge. This case report provides evidence for the use of VPA as an initial monotherapy for treatment of drug-resistant MECP2 duplication-related epilepsy. Duplications of Xq28 involving the methyl CpG binding protein 2 gene (MECP2) causes 0.5 to 2% of X-linked developmental disabilities and are predominantly inherited with full penetrance in males [1]. More severe mutations in MECP2 are lethal in males. Females with point mutations in MECP2 present with Rett syndrome. MECP2 duplication syndrome has been well-described in patients with mental retardation, absent to minimal speech, hypotonia replaced by progressive spasticity and/or ataxia, mild facial dysmorphisms (brachycephaly, large face, midface hypoplasia, depressed nasal bridge, upturned nares), severe recurrent respiratory infections, and in some cases, death before 25 years of age [2]. 
The composition, mineralogy, and textures preserved in scoria from ancient fires provide constraints on the firing temperature, the source and nature of the fire, and its potential social and cultural implications. Analyses of four scoria fragments preserved in a posthole of an Iron Age longhouse at Store Tovstrup, West Denmark, by scanning electron microscopy, electron microprobe, and laser ablation ICP-MS show these to consist of rounded quartz and orthoclase grains, gas vesicles, and carbonaceous material bonded together by a silicate- and potassium-rich (SiO2 67–69 wt% and K2O 11–14 wt%) melt (now glass). Given the presence of vesicles and carbonaceous material, the fire is indicated to have occurred under restricted air-flow and to have involved decomposition of biomass and soil. The initiation of melting occurred during what was presumably an event of short duration. Simplified ternary phase equilibria point to localized melting initiated around 700–800 °C and continuing to about 1000 °C. The main structure succumbed to char at lower temperatures. Calculations suggest that a mixture of 50% sandy soil, 41% barley straw, and 9% oak branches best explains the low Al2O3, Fe2O3, and Na2O concentrations in the melt phase. The scoria at Store Tovstrup most likely originated from a short duration burning with restricted air-flow resulting in the collapse and charring of daub walls. The fire was intentional and set after the house had been cleared of household goods. The Store Tovstrup Iron Age site is located on the Jutland peninsula in Western Denmark on the bank of the minor meandering Savstrup Å (creek), approximately 1.2 km east of the point where it meets the larger river Storå (literally translated ‘big river,’ the second longest river in Denmark) (Fig. 1A). The site was excavated in 2014–16 by Holstebro Museum in preparation for major road constructions in the area planned to be completed in 2018. The location is only 1.5 km south of the maximum extend of the Weichselian ice sheet (Fig. 1A), marked by a large outcrop of clayey till, on a glacial alluvial outwash plain made of broad sandy river valleys (known as ‘Karup hedeslette’ or ‘outwashed sandy plain’ on Fig. 1A; Milthers, 1948; Houmark-Nielsen, 1999; Christensen, 2016). The surface deposit of the immediate area consists of quarts-rich sand, gravel, and, occasionally, clayey sediments. Immediately south of the site is an eroded riverbank and a river valley, made-up of fresh water, lacustrine deposits. With the final retreat of the Weichselian ice, both sea level and land surface rose to reach a maximum during late early pre-Roman Iron Age (400–300 BCE), resulting in the local formation of lakes and the stabilization of waterways around the Tovstrup site (Behre, 2007; Odgaard and Dalsgaard, 2014). Silicate scoria fragments are found at a Store Tovstrup pre-Roman to Roman longhouse site in western Jutland, Denmark. They were recovered from postholes of a late Roman longhouse (~100 CE), but are associated with the burning of a pre-Roman structure dated at about 500–100 BCE. The association of bubbled-up silicate glass containing quartz sand particles and charred material suggests formation as a result of burning principally of daub covering a wattle of woven oak branches. The glass compositions and trace element content of the scoria suggest that the main components contributing to the daub was the local sandy soil, cereal straw, and oak wood components, but with restricted contributions from livestock manure. The main component from the soil appears as unreacted quartz sand particles.
The lack of consensus surrounding the macroscopic determination of high-quality black flint discovered at the Aldenhoven Plateau sites (Rhineland, North-Western Germany) from the beginning of the Middle Neolithic has far-reaching consequences for the anthropological understanding of the socio-cultural dynamics involved in the neolithization of North-Western Europe. This flint has been assigned to Western Belgian 'Obourg' flint type and is used as a key indicator of strong links between populations from West Belgium (Mons Basin) and the German Rhineland at the beginning of the 5th millennium BC. Here, we present an integrated study of this flint using geochemical and lithic technological approaches. This work rules out attribution of the analysed flint artefacts to the Upper Cretaceous flint sources of the Mons Basin; however, the exact origin of the black flint used in the Rhineland remains unanswered. Our results do not support the hypothesis of intensive contact between populations from West Belgium and the German Rhineland and highlights the urgent need for further combined petrographic and geochemical analyses in the region, particularly on geological samples, in order to build up an extensive and reliable comparative reference collection. The Neolithic developed in temperate Europe with the Linear Pottery Culture (hereafter, LPC, second half of 6th millennium) and spread from Transdanubia (Hungary) to the Paris Basin (France). In Central-Western Europe, it was characterized by substantial uniformity in architecture, ceramic styles, and funerary practices, to name only a few. At the turn of the sixth and fifth millennium BC, a historical turning point occurred in continental neolithization of Europe as the Linear Pottery culture broke up into a mosaic of cultural entities. The northern half of France and Belgium were occupied by the Blicquy/Villeneuve-Saint-Germain culture (BQY/VSG). The BQY group was initially discovered in Belgium (two areas, Hainaut in western Belgium and Hesbaye in eastern Belgium) while VSG group was recognized in France; however, their chrono-cultural characteristics, strongly inherited from LPC, are quite similar (Constantin, 1985). In west Germany, the first post-LPC occupations are linked to the emergence of the Hinkelstein culture on the middle and upper Rhine regions which was followed by the Grossgartach then Planig-Friedberg and Rössen cultures (see Fig. 1). The Aldenhoven Plateau, in the north-western studied part of Germany, hasn't delivered sites from the beginning of this Middle Neolithic sequence (German chronology). These Middle Neolithic Rhineland cultural entities are partially contemporaneous from the Blicquy/Villeneuve-Saint-Germain culture (e. g. Constantin and Ilett, 1998; Gehlen and Schön, 2007; Nowak, 2013; Zimmermann et al., 2006) and are approximately dated from 4950/4900 to 4750/4650 BCE. 
Corpus callosotomy (CC) is used in patients with drug-resistant seizures who are not candidates for excisional surgery and failed neurostimulation. We examined ictal scalp and intracranial electroencephalogram (iEEG) recordings in 16 patients being evaluated for anterior CC alone or CC in combination with focal resection, to determine the role of the iEEG in predicting postoperative seizure outcomes. In our cohort, CC improved generalized atonic seizures and focal seizures with impaired awareness but did not alter outcomes for generalized tonic–clonic or tonic seizures. Invasive EEG prior to CC did not refine the prediction of postsurgical seizure outcomes in patients with inconclusive scalp EEG. Introduced by van Wagenen in 1940, corpus callosotomy (CC) is a palliative disconnection procedure for patients with drug-resistant epilepsy who are not suitable candidates for excisional surgery. Anterior callosotomy, the most commonly used modification of this procedure, involves interruption of the anterior mid-body of the corpus callosum that carries interhemispheric motor connections [1] thought to be essential for the generation of generalized atonic and tonic–clonic seizures (GTCs) [2,3]. In epilepsies with these seizure types, the successful postoperative outcomes have been consistently demonstrated. However, the treatment responses in other seizure types have not been well understood. While the previous applications of CC were largely restricted to the patients with disabling generalized seizure syndromes, such as Lennox–Gastaut syndrome (LGS) and infantile spasms, the indications have recently expanded to the patient populations with other epilepsy etiologies [3]. These include drug-resistant focal epilepsies in patients without identifiable lesions or those with multiple lesions which are not amenable for resection [4]. From the present data, we conclude that multistage surgical approaches that involve intracranial EEG recordings prior to CC improves the success of seizure lateralization but does not refine the prediction of postsurgical seizure outcomes in patients with inconclusive scalp EEG. Furthermore, the absence of the organized gradient of seizure discharges in the recordings from frontal grid electrodes suggests that anatomical distribution of these patterns within the frontal cortex has no association with outcomes after CC. As an effective treatment for generalized atonic seizures and focal seizures with impaired awareness, CC should be considered in patients with generalized and multifocal drug-resistant epilepsies. Further data are required before surgical consideration is indicated for patients with other seizure types. In the current series of patients, the benefits of performing combined iEEG in patients considered for CC were not apparent; and therefore further studies involving larger cohorts of patients are needed.
Compared with North America or northern Africa, prehistoric basketry has rarely been studied in East Asia. Even in Japan where plant remains excavated from lowland sites have been studied extensively in the past thirty years, the study of basketry lagged behind that of other wooden artifacts, and materials of less than 1000 baskets of the Jomon period have been identified versus more than 21,300 wooden artifacts and 32,500 natural woods of the same period so far identified. The Higashimyou site in Saga prefecture of western Japan yielded over 700 baskets including basket fragments of the initial Jomon period from 8000 to 7300 cal BP and provided a good material to examine the material selection and basketry technique of the incipient basketry manufacture in Japan. The identification of basket materials revealed that large baskets were made mostly with splints of two arboreal species, Sapindus mukorossi and Ficus erecta, and that small ones were made mostly with stems of two vine species, Sinomenium acutum and Trachelospermum. The basketry techniques used at this site showed that the bodies of the baskets were made by various types of twill, weave, and twining, and that the bodies of large ones were usually made of two kinds of technique across belts in the middle of the bodies. The employment of various basketry techniques at this site showed that most of the basketry techniques used in later Jomon and following periods were already established in the initial Jomon period. Occurrences of remains of the material plants around the site, the archaeological record of these plants in Japan, and floristic studies of present Japan indicated that appropriate materials used for the baskets were not easily available around the site and that some kind of resources management for basketry materials must have existed around the settlement. Among archaeological wooden objects, basketry is regarded as a kind of portable objects that can have important information for understanding subtle prehistoric cultural and social differences (Sands, 2013). Prehistoric basketry has so far been studied extensively in North America (Adovasio, 2010; Carriere and Croes, 2018) and northern Africa (Wendrich, 2000; El Hadidi and Hamdy, 2011; Di Lernia et al., 2012; Wendrich and Holdaway, 2018), and their studies showed that prehistoric baskets can provide information on regional differences in communities and their historic changes. The basket materials in these areas clearly showed local differences in material selection, such as roots, boughs, splints, and bark of Thuja plicata D. Don, Picea sitchensis (Bong.) Carrière, Acer circinatum Pursh, and Prunus emarginata (Douglas) Walp.? and leaves and stems of monocots in North America (Croes and Hawes, 2013; Carriere and Croes, 2018), Phoenix dactylifera L., Hyphaene thebaica Mart., Cyprus papyrus L., and others taxa in Egypt (Wendrich, 2000; El Hadidi and Hamdy, 2011), and monocot leaves or stems and animal fibers in Libya (Di Lernia et al., 2012). By using cluster and cladistic analyses of basketry attributes and types, Croes (2001, 2015), Croes et al. (2009), and Carriere and Croes (2018) further proposed a hypothetical history of stylistic and ethnic continuity from basketry remains found in the Northwest Coast of North America. In northern Africa, use of baskets is discussed in relation to the availably and storage of wild cereals and resource exploitation (Di Lernia et al., 2012) or to mobility and sedentism versus long-term caching by basketry lined storage pits (Wendrich and Holdaway, 2018). Compared with the studies in these areas, few studies have been carried out on prehistoric basketry in East Asia including Japan, and regional or cultural differences and their historical changes have not been clarified (Matsunaga, 2015). 
The process of dating ancient obsidian artifacts converts the quantity of surface diffused molecular water to a calendar age using an experimentally derived diffusion coefficient predicted from glass composition. The internal structural water content of rhyolitic obsidian has been identified as a highly influential variable that controls the rate of water diffusion at ambient temperature. We demonstrate through the use of infrared spectroscopy and specific gravity (density) measurements on samples from 34 obsidian sources that total structural water (H2Ot) concentrations between sources can range from 0.07% to 1.66%. Structural water concentration within individual sources may also vary significantly and impact the accuracy of estimated ages for artifact manufacture if not properly monitored. A calibration for the water determination on individual samples by density measurement is developed here and the impact of structural water variation on obsidian chronometric dates is discussed. Geological flows of rhyolitic glass, or obsidian, were routine sources of tool raw material for humans in the Lower and Middle Paleolithic (Adler et al., 2014; Merrick and Brown, 1984) and afterward (Shackley, 1998). Easily accessible from primary surface outcroppings, or within secondarily deposited erosional contexts (Doleman et al., 2012), obsidian was knapped into functional tools, symbols of cultural interconnectedness (Torrence, 2011), or markers of social inequality (Pierce, 2015). The abundance of obsidian in the archaeological record, created by its use within many types of cultural contexts, provides an opportunity for obsidian hydration age determinations to play a significant role in archaeological interpretation. High resolution temporal reconstructions of past events benefit from chronometric assays where the standard errors associated with dates are small. Rogers (2008b, 2010) discusses four major error sources that can impact the final age determination: hydration rim measurement precision, estimation of temperature history (effective hydration temperature), variation in the experimentally determined source-specific diffusion coefficient, and intra-source variability in intrinsic water content; all of which contribute to the final age uncertainty. The fourth variable, the variability in obsidian structural water content, and its influence on the dating outcome, is the factor explored here. 
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. The capacity to react in the moments immediately after a catastrophe, whether this is natural, such as an earthquake, or man-made, such as that caused by a nuclear reactor meltdown, is critical for minimizing the caused human and material damages. Unfortunately, in situations as extreme as these ones, it is highly complicated to tackle the chaos that usually arises. On occasions, this is due to the lack of information in such moments. Taking an earthquake that has affected an urban area as an example, with different types of buildings, and the difficulty associated with obtaining information about what is tracking place and which parts of the environment are in a critical state. In these situations, solutions based on independent robots which cooperate to obtain information about the environment and to analyze what is happening are especially important. If these are damaged in the rescue process, losses will only be economic ones. In recent years, research work linked to the use of independent robots used as a rescue tool in catastrophes and emergency situations has gained great importance, with one outstanding aspect of this being their independence in relation to human intervention and coordination (Murphy, 2004). Two more illustrative examples are shown by the intervention of independent robots in the collapse of the World Trade Center in the United States (Casper and Murphy, 2003) and in the Fukushima power station meltdown in Japan (Nagatani et al., 2013). 
The Mesolithic burial from Brunstad, Vestfold, Eastern Norway, dating to c. 5900 cal BC, represents rare evidence of Mesolithic mortuary practice in Norway. While Mesolithic settlement finds are abundant in the region, evidence of mortuary ritual is virtually absent in the record. In this article we present the method and the results of the multidisciplinary excavation, on-site and in the laboratory. The challenging Brunstad find was excavated in two steps, and later reconstructed, involving osteology, 3D photogrammetry and conservation. Moreover, the burial is discussed in its local, regional and supra-regional context. While the inhumation of an adult individual in a flexed body position is rare in its regional context, it exhibits typical features known from Mesolithic graves in Scandinavia as well as from other parts of Europe. These include the shore-based island location and proximity to a settlement site, the body position, as well as certain features of mortuary ritual. In the light of Mesolithic mortuary practices in Scandinavia and the Baltic region, from where many Mesolithic graves are known, we suggest that Mesolithic hunter-gatherers in Eastern Norway and the Skagerrak region handled and disposed of their dead in various ways, some of which might not have left archaeological traces. We conclude that the Brunstad grave represents a distinct mode of burial at the time.  
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. Goal recognition, the ability to recognize the plans and goals of other agents, enables humans, AI agents or command and control systems to reason about what the others are doing, why they are doing it, and what they will do next (Sukthankar et al., 2014). Until now, goal recognition system works well in many applications like human–robot interaction (Hofmann and Williams, 2007), intelligent tutoring (Min et al., 2014), system intrusion detection (Geib and Goldman, 2001) and security applications (Jarvis et al., 2005). However, the inability to take full use of the goal recognition results poses another challenge for domains like Game AI (Synnaeve and Bessiere, 2012) and Command and Control system (Xu et al., 2017). 
Perampanel, a selective, non-competitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor antagonist, is approved for adjunctive treatment of focal seizures, with or without secondarily generalized seizures, and for primary generalized tonic–clonic seizures in patients with epilepsy aged ≥ 12 years. Perampanel was recently approved for monotherapy use for focal seizures in the U.S.A. Anti-seizure drug monotherapy may be preferable to polytherapy, which is generally associated with increased toxicity, non-compliance, and cost. Here, we report cases where patients had converted to perampanel monotherapy during open-label extension (OLEx) portions of 9 Phase II and III studies. Of 2245 patients who enrolled in the OLEx studies, we identified 7 patients with drug-resistant focal seizures who discontinued all non-perampanel anti-seizure drugs and were maintained on perampanel monotherapy for ≥ 91 days until the end of data cut-off. Patients received perampanel monotherapy for up to 1099 days (157 weeks), most at a modal dose of 12 mg. Seizure data were available for 6 patients, of whom 5 had a ≥ 90% reduction in overall seizure frequency between baseline and their last 13-week period of monotherapy (3 were seizure-free). Perampanel monotherapy was generally well tolerated and the safety profile during perampanel monotherapy was consistent with clinical and post-marketing experience in the adjunctive setting. This analysis included a small proportion of patients with highly drug-resistant focal seizures who converted to monotherapy during OLEx studies. While these limited data are encouraging in suggesting that perampanel might be useful as a monotherapy, further studies are required to explore outcomes in a less drug-resistant population, where a larger proportion of patients might benefit from monotherapy. Perampanel, a selective, non-competitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor antagonist, is approved for adjunctive treatment of focal seizures, with or without secondarily generalized (SG) seizures, and for primary generalized tonic–clonic seizures in patients with epilepsy aged ≥ 12 years [1,2]. Perampanel was recently approved for monotherapy use for focal seizures in the U.S.A. 
Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research. The phenomenon concerning road traffic crashes has become a major concern worldwide. According to the global status report on road safety conducted by the World Health Organization (WHO) in 2015, 1.25 million traffic-related fatalities occur annually worldwide, with millions more sustaining serious injuries and living with long-term adverse health consequences; road traffic injuries are currently estimated to be the leading cause of death among young people, and the main cause of death among those aged 15–29 years. (“WHO | Global status report on road safety 2015”, 2015). Road safety perception cannot be detached from the analysis of the driver behavior (DB) as the major part of traffic accidents is caused by human factors as it was inferred that they took part in the manifestation of 95% of all accidents (Evans, 1991, 1996). For these purposes, analyzing DB can aid assessing driver performance, enhance traffic safety and, furthermore, endorse the development of intelligent and resilient transportation systems. 
Silver and gold artefacts from a 12–13th c. CE tumulus in Senegal were recently analysed by means of XRF and LA-ICP-MS. The identification of major, minor and trace elements allowed gaining a rare insight into the composition of precious metals circulating in that part of Africa in medieval times. The results show that all objects were made of polymetallic alloys. Comparisons with analyses from other West African as well as North African and European silver and gold artefacts suggest that the metal from the studied objects most probably originate from outside West Africa. At least in the case of the gold artefacts, this is quite surprising, as we expected to come across pure West African gold. The results hint at a hitherto little known facet of the medieval African gold trade. It is concluded that, in addition to the acknowledged mass export of West African raw gold toward North Africa, there was also a thus far widely ignored traffic of alloyed gold (and silver) into West Africa. Gold is among the most legendary export commodities of the African continent. Probably first evoked by Herodotus' 6th century BCE account of ‘silent’ bartering for gold by Carthaginians somewhere along the Atlantic coast, the myths of abundance and easy acquisition of this metal from inner Africa never ceased to exist. There is still much scholarly debate about the true antiquity, scale and impact of that trade (e.g., Devisse, 1972; Cahen, 1979; Garrard, 1982; Devisse, 1993; Nixon et al., 2011). Archaeology has had thus far relatively little to contribute to the debate. On the one hand, this is a result of the still comparatively low number and range of archaeological studies specifically devoted to investigate the exploitation, processing and trade of precious metals in the continent. On the other, gold and silver surprisingly appear to be late additions to the material canon of African, but especially West African, cultural expressions. In contrast to Eurasia, where gold was already in use by the mid- to late 5th millennium BCE (e.g., at the Bulgarian sites of Durankulak and Varna, see Krauß et al., 2014; Leusch et al., 2015), none of the archaeological gold or silver finds yet known from West Africa stems from a context older than the 8th century CE, and the majority is indeed considerably younger (Table 1). 
Anti-N-methyl-d-aspartate receptor (anti-NMDAR) encephalitis, the most recognized type of autoimmune encephalitis, manifests with rapid cognitive decline, psychosis, and seizures that develop in 78–86% of patients. Recently, anti-NMDAR encephalitis was reported in association with demyelinating diseases which are accompanied by a characteristic clinical phenotype, imaging abnormalities, and the presence of antibodies against myelin oligodendrocyte glycoprotein (MOG-IgG) in bodily fluids. The patient presented herein suffered from bilateral optic neuritis followed by recurrent encephalitis with focal seizures and demonstrated anti-NMDAR and MOG-IgGs in the cerebrospinal fluid and serum, respectively. Her symptoms responded to immunotherapy and antiseizure medication. The recognition of the novel syndrome of MOG antibody-associated demyelination (MOGAD), encompassing the overlapping anti-NMDAR encephalitis and other MOG-IgG associated disorders, is important for the successful management of these patients. Anti-N-methyl-d-aspartate receptor (anti-NMDAR) encephalitis, an immune-mediated encephalopathy, has been recently reported in association with central nervous system (CNS) demyelinating diseases including acute disseminated encephalomyelitis (ADEM) [1], myelitis [2] and neuromyelitis optica (NMO) [3]. A demyelinating disease can manifest along with anti-NMDAR encephalitis or occur at a distant time [3]. In a recent case series of 691 patients with serologically confirmed anti-NMDAR encephalitis, an antecedent or subsequent episode consistent with NMO spectrum disorder was identified in 12 patients, all of whom had demyelinating or multifocal hemispheric or brainstem lesions [3]. The understanding of the interplay between the adaptive immune system and processes contributing to central demyelination is evolving. We report a patient with an isolated optic neuritis that preceded the manifestation of autoimmune encephalitis and seizure onset by several months. The long-lasting remission of symptoms in both conditions was achieved with immunotherapies. SECTION Case report 
In the recent years, a rapid growth of IoT devices has been observed, which in turn results in a huge amount of data produced from multiple sources towards the most disparate cloud platforms or the Internet in general. In a typical cloud-centric approach, the data produced by these devices is simply transmitted over the Internet, for consumption and/or storage. However, with the exponential growth in data production rates, the available network resources are becoming the actual bottleneck of this huge data flowing. Therefore, several challenges are appearing in the coming years, which are mainly related to data transmission, processing, and storage along the so-called cloud-to-thing continuum. In fact, one of the most critical requirements of several IoT applications is low latency, which often hinders raw data consumption to happen at the opposite endpoint with respect to its production. In the context of IoT data stream analytics, for instance, the detection of anomalies or rare-events is one of the most demanding tasks, as it needs prompt detection to increase its significance. In this respect, Fog and Edge Computing seem to be the correct paradigms to alleviate these stringent demands in terms of latency and bandwidth as, by leveraging on re-configurable IoT gateways and smart devices able to support the distribution of the overall computational task, they envisage to liquefy data processing along the way from the sensing device to a cloud endpoint. In this paper, we will present IRESE, that is a rare-event detection system able to apply unsupervised machine learning techniques on the incoming data, directly on affordable gateways located in the IoT edge. Notwithstanding the proposed approach enjoys the benefits of a fully unsupervised learning approach, such as the ability to learn from unlabeled data, it has been tested against various audio rare-event categories, such as gunshot, glass break, scream, and siren, achieving precision and recall measures above 90% in detecting such events. Improved cost-effectiveness and miniaturization of sensing devices have increased their utility in various domains of daily human life such as healthcare, transport, education, agriculture, and security. In a typical IoT environment, these sensing devices are connected to the Internet and responsible to continuously sense their surroundings and then transmit data to a cloud station for further processing. In the past few years, an exponential growth in IoT devices have been observed in the form of smart products. Based on the context, these devices of various varieties produce a huge amount of data at varying rates. According to an estimate by Cisco Global Cloud Index, the data produced by a variety of data sources will reach to around 500 ZB by 2019, whereas the internet infrastructure will be capable to handle 10.4 ZB by that time (Cisco, 2016). Similarly, according to CISCO Internet Business Solutions Group the number of devices connected to the internet will reach around 50 Billion by 2020 (Evans, 2011). These factors (variety, amount of data, and variable data rate) have raised serious concerns in an IoT environment, which mainly relates to data transportation, data storage, data processing, and security. The first concern, transportation of data, needs a high-speed Internet, which can quickly and efficiently transmit data to the destination. The second concern, storing huge amount of data, needs cloud services and other necessary networking infrastructure. The third concern, data processing, is important to be handled because raw data is not meaningful and it is required to transform raw data into meaningful information (Ackoff, 1989). The fourth concern, security must be addressed for critical applications in which IoT data can be stolen or intruders can attack the system. Cloud-based paradigms are widely used in IoT systems, in which the data is pushed to the cloud and after computations, the outcome is delivered back to the local system. However, due to the proliferation of IoT, an increased amount of data is produced at the edge of the network. The limited network bandwidth is unable to meet the requirements of low-latency transportation of data coming at a high speed. Therefore, one can conclude that Cloud Computing alone is not efficient enough to handle the IoT generated data in the coming years (Antonini et al., 2018). Since data production at the edge of a network is increasing, an adequate choice is to perform the necessary processing on an edge device; near the source. The edge devices are becoming more powerful and resource friendly with optimal utilization of resources such as memory and energy. 
Numerous autoantibodies are implicated in the pathogenesis of autoimmune epilepsy. In the past decade, many case series reported the association of glutamic acid decarboxylase 65 (GAD 65) antibodies with epilepsy. Conjoint presence of GAD 65 antibodies with antinuclear, anti-thyroid, and anti-parietal cell antibodies has often been demonstrated. However, concomitant elevated levels of GAD 65 and P/Q voltage gated calcium channel (VGCC) antibodies is rare. We report a case of autoimmune epilepsy with conjoint GAD 65 and P/Q VGCC antibodies in the absence of malignancy. This report highlights a possible role of P/Q VGCC antibodies in the pathogenesis of autoimmune epilepsy. Autoimmune epilepsy has become a fact in modern medicine. The new ILAE classification of the epilepsies recently added immune epilepsy as one of the etiological groups [1] Accumulating data support the presence an immune-mediated pathogenesis in drug-resistant epilepsy even in the absence of limbic encephalitis or malignancy. Examples of antibodies implicated in this disorder include N-methyl-d-aspartate receptor (NMDA) antibodies, voltage gated potassium channel complex (VGKCc) antibodies, collapsin response mediator protein 5 (CRMP-5) antibodies, ganglionic acetylcholine receptor antibodies, and GAD 65 antibodies [2]. Voltage gated calcium channel antibodies, on the other hand, have not yet been linked to autoimmune epilepsy. SECTION Case 
This paper presents the microstratigraphic analysis of the protohistoric site El Calvari del Molar, dated to the 8th century and the first quarter of 6th century BCE. It focuses specifically on Room 8, located in the northern part. The sedimentary record covers different stratigraphic units, including several floors with their layers of preparation and domestic structures. The study of these floors, mainly through micromorphology, allows us to determine the composition, the processing and the technical treatment as well as the possible origin of the lithological materials used in their manufacture. We highlight that the floors are composed of earth construction, made of local carbonated materials, mainly clayey fine sands. Mixing this material with water would cause a precipitation of CaCO3 favoured by the impermeability of their preparation layers, mostly silty clay aggregates, leading to the semi-cementation of the floor, which appears to have been the aim of the builders of El Calvari. This type of earth floor brings together a series of characteristics at the construction level such as cohesion among particles and tenacity, similar features to lime mortars. In short, we have the opinion that the micromorphological analysis is an essential tool for the study and interpretation of domestic architectural elements. In this case, it has allowed us to identify and characterize the techniques and construction strategies of this type of floor by these first protohistoric builders of the NE Iberia. The geoarchaeological study of occupation surfaces and the search for floors at archaeological sites is becoming an increasingly common practice, especially in settlements, and there are different methodological approaches to help improve their interpretation. One of them is micromorphology, of which examples from as early as the 1990s provide us with detailed information on site formation processes (e.g. how floors were made and used). In the different handbooks and syntheses of Geoarchaeology and Soil Micromorphology we find sections referring to building materials, in which reference is made to the floor and usage levels of the sites or to certain indications of floors, like Goldberg and Macphail (2006), Macphail and Goldberg (2010, 2018), Milek (2014), Friesem et al. (2017) or Rentzel et al. (2017). From the analysis carried out, we believe that micromorphology is essential for the characterization of construction techniques and the recognition of domestic activity associated with occupation surfaces and especially constructed floors.
To describe seizures occurring in 3 healthy adults with influenza infection. Seizures associated to influenza infection are rare in adults without encephalitis. Clinical observations of 3 healthy adult patients with influenza A and B infection and seizures. We present here 3 healthy adult patients with seizures related to influenza A or B infection without evidence encephalitis, encephalopathy or any other cause for seizures. Prognosis was excellent. Seizures can occur in healthy adults with influenza infection without evidence of encephalitis, a possibility to be borne in mind to avoid potentially harmful therapeutic and diagnostic procedures. Influenza is an acute infectious respiratory disease with great public health impact because of its worldwide distribution and morbidity. Although most influenza complications are pulmonary, extrapulmonary complications can occur, including neurological disturbances. Encephalopathy, followed by seizures or status epilepticus are the main neurological complications, occurring more frequently in children during natural influenza infection [1]. Other, less common complications include acute disseminated encephalomyelitis, Reye's syndrome, Guillain–Barre syndrome, post-viral parkinsonism, cerebellitis, and acute necrotizing encephalitis [1]. They can occur during infection, where most fatal and severe cases are observed, or after vaccination, which usually conveys a more favorable prognosis. 
Archaeologists have paid substantial attention to the social transformations coinciding with the widespread adoption of bow and arrow technologies. Social network analysis (SNA) is used to examine stone tool assemblages from the Salish Sea. SNA while widely applied a wide range of problems in lithic technologies has been an underutilized approach in the Pacific Northwest. Based on an application of cultural transmission theory, ethnography, and Coast Salish ontology, that haft styles reflect corporate group connections. Changes in the social networks are examined as reflected in haft styles from 3500 to 1000 BP, a time of shifts towards large plank house villages and the emergence of hereditary forms of social inequality in the region. Five social networks were constructed, each covering a 500-year period, to assess shifts in regional connections through time. There appears to be increased elaboration of social networks throughout the Salish Sea until 1600 BP, when the bow and arrow become widely adopted. These data suggest SNA of lithic haft styles shows a shift in hunting organization from a collective corporate group level activity to an individualized pursuit. The findings show the utility of SNA to address oscillations in Salish Sea society over time. New directions for future studies to examine shifts in corporate group relations in other aspects of precontact Coast Salish society are also provided. Social network analysis (SNA) has been applied to examine social transformations in non-state societies (Borck et al., 2015; Brughmans, 2010; papers in Knappett, 2013; Mills et al., 2015; Mills, 2017) such as demographic collapse (Birch and Hart, 2018; Knappett, 2011). Many of these studies have emphasized ceramic styles, as they are argued to reflect active social symbols, while others have integrated ceramic sourcing approaches to their studies (Gjesfjeld, 2014, 2015). In terms of lithic studies, analyses of lithic material sources have been employed by archaeologists in varied settings to assess the relationships between assemblages (Buchanan et al., 2016; Golitko et al., 2012; Golitko and Feinman, 2015; Mills et al., 2013; Phillips, 2011). Linking SNA with a geographic information system (GIS) has enabled archaeologists to engage with data in new ways using multiple spatial and temporal scales to understand social connections. Social transformations resulting from the widespread adoption of bow and arrow technologies has gained renewed interest in the literature. This study contributes to that larger discussion through applying SNA to stone tool stylistic diversity. The spatial and temporal distribution of hafted stone tools was examined during a period of fundamental social transformations in the Salish Sea, using a cultural transmission framework that was critically informed by ethnography and Coast Salish ontology. Strong network ties were noted throughout the region which crossed language groups until 1600 BP, at which time the social network of stone tool producers appears to exhibit less developed regional ties.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. In the analysis of dynamical systems, time plays a key role. In fact, all the related signals evolve over time and any information that has to be inferred from data, ranging from mathematical models, to fault isolation, to systems use mode detection, can be considered as the result of a learning process with time series as predictors. 
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. In real life, there are many real-world engineering optimization problems that need to be solved. The actual engineering problem has some practical constraints and one or more objective functions (Dhiman and Kumar, 2018b). The purpose is to find a set of parameter values to minimize the value of the objective function, where the constraint can be either an equality constraint or an inequality constraint. Due to the large number of these problems and their practical application value, it has become one of the research hotspots in recent years. Based on the characteristic of CS and DE, a hybrid meta-heuristic algorithm, named CSDE, is proposed in this paper. CSDE can enhance the ability of exploitation and avoid the population trapping into local optima by employing division population, grouping update and combination population to exchange the useful information between individuals. CSDE does not introduce extra operators except those operators in original CS and DE, which make it simple and efficient.
A set of 37 overfired ceramic samples was collected from the dump of two kilns sited in the productive area FF1 in the acropolis of Selinunte (south western Sicily), being specifically active in the period 409–250 BCE. The ceramic samples were analysed by thin-section petrography and chemical analysis, with the aim to establish a valuable ‘reference group’ representative of the ceramic produced at Selinunte during the Punic phase. The petrographic and chemical analyses allowed to state that the ceramic manufactures from the kilns operating in the FF1 insula are characterized by rather homogeneous textural/compositional features. The daily-use common ware here produced is characterized by aplastic inclusions mainly falling in the size classes of coarse silt and medium sand, with relative abundance ranging between 15 and 25% area. The inclusions are composed of monocrystalline quartz and, subordinately, of calcareous bioclasts, polycrystalline quartz, K-feldspar, plagioclase, chert, sandstones and acid rock fragments. The relatively low total chemical variability of the ceramic sample set reflects the specific incidence of the above-mentioned mineralogical and textural features. The variable amount of quartz-rich sand used for tempering the local raw clays produces slight variations in the SiO2/CaO concentration ratio. Nonetheless, the chemical ‘reference group’ defined through this study seems to be consistent and characterized by satisfactory low standard deviations and it is fully congruent with the geo-lithological background of the area. This new chemical ‘reference group’ might be applied to studies that are aimed to define the trade networks in that time in south western Sicily. It could also represent a useful starting point for future systematic studies concerning various ceramics classes (i.e. tableware, cooking ware, transport amphorae, etc.), taking into account the consumption and insular/extra insular trade dynamics of the ceramic products of Punic Selinunte. Selinus (or Selinous, modern: Selinunte) is one of the rare “second generation” Sicilian Greek colonies. It is in fact related to the Greek mother-city Megara Nysaea and its first Sicilian colony Megara Hyblaea, laying the foundation of the sub-colony Heraclea Minoa as well. Selinunte was the farthest western outpost of the Greek civilization in the central Mediterranean being located on the border of the Phoenico-Punic and the Elymian territories (Fig. 1). The production area of the insula FF1, located in the south west part of the acropolis of Selinunte, certainly represents an exemplary case for establishing the textural and compositional characteristic features of the ceramic production during the Punic period of the settlement. The stratigraphic studies on the entire FF1 area, as well as on the production facilities (Fourmont, 1991, 2005), together with the exceptional condition of ceramic wastes, especially for kiln F2, made possible to select highly representative assemblage of samples for the archaeometric study and the definition of a reliable ‘compositional reference group’. This condition of work has also took advantages by the recently acquired archaeometric data concerning both the clayey raw materials available in the territory immediately surrounding the site (Montana et al., 2018) and the ceramic productions (down to 409 BCE) of the Archaic furnaces already brought to light by the excavation mission led by the University of Bonn in the Cottone River valley (Bentz et al., 2013). As a result, the Pleistocene clayey deposits outcropping just outside the fortification were established to be the raw materials used by the potters of ancient Selinous.
We present a unique case of a patient with drug-resistant focal epilepsy undergoing stereoelectroencephalography (sEEG) who developed an acute posttraumatic intracranial hemorrhage during monitoring, first detected by changes on sEEG. Our case demonstrates the evolution of electrographic changes at the time of initial hemorrhage to the development of ictal activity. We conducted spectral analysis of the sEEG data to illustrate the transition from an interictal to ictal state. Initially, delta power increased in the region of acute hemorrhage, followed by sustained regional reduction in frequency variability. Our findings provide further information on the development of epileptiform activity in acute hemorrhage. Traumatic brain hemorrhage (TBH) can be associated with lifelong functional, cognitive and emotional impairment. The magnitude of tissue damage following trauma is determined by both the primary injury, which is dependent on the kinetic energy at the time of impact, and the secondary injury that accumulates over time due to the response of the brain tissue surrounding the primary injury site [1]. Among different factors that contribute to secondary injury are seizures and hemorrhagic progression of contusion (HPC) [2]. HPC can present as progressive hemorrhagic injury or as delayed intracerebral hematoma 48 h after the trauma. HPC may result in increased seizure burden, although it is unclear if seizures and HPC are corelated or occur independent of each other. Growing evidence suggests that epileptiform transients and seizures are independent predictors of the development of chronic spontaneous seizures (posttraumatic epilepsy; PTE) [3,4]. Therefore, the management of TBH includes monitoring and treatment of HPC and the development of new-onset seizures. Capitalizing on a traumatic HPC following a seizure-related head injury in a patient with drug-resistant focal epilepsy admitted for sEEG monitoring, we demonstrate the temporal evolution of epileptiform activity and the associated spectrographic changes that were localized adjacent to the area of acute hemorrhage. The hemorrhage was noted to be outside of and contralateral to the area eventually determined to be the seizure onset zone (SOZ). SECTION Materials and methods 
In absence of organic remains suitable for radiocarbon dating, archaeomagnetic dating can contribute significantly to the better understanding and rescue of our past and cultural heritage. Nowadays, in the archaeological research is fundamental to obtain as much information as possible, including precise dates of the archaeological sites; indeed, the rapid expansion of urban areas implies the destruction of archaeological structures, causing the loss of knowledge about our past. In this effort, we report an archaeomagnetic and rock-magnetic study on burnt archaeological samples from two kilns discovered at the monastery of San Pelayo del Cerrato (Cevico Navero, Palencia, Spain). This was a rescue excavation where arose the possibility of studying archaeomagnetically two ovens for the manufacture of tiles and bells before their destruction due to the restoration works of the monastery. Given their suitable conditions and the lack of independent absolute dating, the aim of the study is to archaeomagnetically date their last use. The main magnetic carrier is pseudo-single domain (PSD) Ti-poor titanomagnetite, indicating that the magnetic signal is stable. The comparison of mean full-vector values (mean direction and absolute archaeointensity determinations) with the SCHA.DIF.14k model determined their last use between 1301–1391 CE (bells´ kiln) and 1295–1404 CE (tiles´ kiln) at 95% confidence level, respectively. These dates agree well with the archaeological evidence indicating that the last use of both kilns occurred almost simultaneously or closely confined in time (14th century AD). The dating resolution obtained is comparable to the radiocarbon with the benefit that archaeomagnetism dates the last use (abandonment) of the structures and no material associated to it which might be slightly younger. During the last years, the archaeomagnetic research has experienced a great development. The improvement of archaeomagnetism as a dating method as well as our knowledge of the Earth's magnetic field variations in direction and intensity for the last millennia mainly depends on our ability to obtain high-quality data from a wide suite of geological or archaeological records. With regard to archaeological remains, burnt materials such as kilns, baked clays or hearths are particularly suitable to this aim. They usually have independent dating information, are generally well preserved and reached high temperatures (>600 °C) in the past, potentially carrying a thermoremanence or TRM. Consequently, multiple secular variation (SV) curves have been published in different countries or regions worldwide. It is worth mentioning, for instance, the American southwest (e.g.: Goguitchaichvili et al., 2018; Hagstrum and Blinman, 2010; Lengyel, 2010), the Near East (e.g.: Stillinger et al., 2015) and mostly Europe covering the last 2–3 millennia (e.g.: Gómez-Paccard et al., 2006; Schnepp and Lanos, 2005; Tema et al., 2006; Gallet et al., 2002; Hervè et al., 2013; Batt et al., 2017), and occasionally even older chronologies (e.g.: Palencia-Ortas et al., 2017; Kapper et al., 2014; Carrancho et al., 2013; Tema and Kondopoulou, 2011). Archaeomagnetic directional, archaeointensity and rock-magnetic study carried out on two kilns at the monastery of Cevico Navero has revealed different conclusions summarized as follows:
Following the 14th century depopulation of the Savannah River Valley of modern Georgia and South Carolina, the neighboring coastal region of Georgia exhibited significant changes in settlement. In the absence of other demographic proxies, Georgia state site file data show that there was a greater accumulation of archaeological components during the 14th century than in any other preceding period. Exploratory Bayesian modeling of settlement practices using a compiled set of legacy radiocarbon dates demonstrates that much of this expansion of settlement happened concomitant with the depopulation of the Savannah River Valley. Climatic instability is one of the greatest natural challenges facing human groups. Drought, in particular, has been a major stressor of subsistence and sociopolitical systems. Droughts vary in geographic and temporal scale, sometimes intermittent and local, and occasionally decadal and regional. Human responses to drought vary in kind. For example, in the American Southwest, a significant disruption in normal rainfall patterns between 1250 CE and 1450 led to the abandonment of the Kayenta and Greater Mesa Verde regions (Cameron, 1995; Dean and Funkhouser, 1995; Hill et al., 2004; Lekson and Cameron, 1995; Mills et al., 2015). Decades of study have shown that these two cases of abandonment and migration led to very different social outcomes for both migrants and locals based on the cultural and environmental contexts of their respective homelands and destinations (Clark and Lyons, 2012; Mills, 2011; Ortman, 2012; Ortman et al., 2014; Stone, 2015). 
For centuries, and until a few years ago, it was considered that the distillation process had been brought to the new world by the Spaniards, who in turn learned it from the Arabs. For this reason, it was believed that the only alcoholic beverage of the Mesoamerican societies was pulque - a ferment of maguey. However, recent archaeological investigations revealed that the alcohol distillation was known in Mesoamerica long before the arrival of Europeans, for at least 25 centuries. The direct evidence comes from the ceremonial and administrative center of Xochitécatl-Cacaxtla (state of Tlaxcala) with several ovens where cooked maguey remains were discowered. The corresponding archaeological context was radiometrically dated from 600 to 400 BCE. Here, we report a detailed archaeomagnetic study on burned archaeological artifacts found in these cooking ovens. 35 specimens belonging to two pottery fragments, one burned rock and two burned soil samples were pre-selected for archaeointensity experiments. Pottery samples exhibited essentially reversible behavior during thermomagnetic experiments pointing to Ti-poor titanomagnetite (almost magnetite phase) as main magnetic carrier while two ferromagnetic phase seems to co-exist in burned soils. In contrast, burned rock samples exhibited some instabilities during the heating at high temperatures and indirect evidence of the presence of antiferromagnetic hematite grains. In total, 29 specimens allowed the estimation of absolute geomagnetic intensity recorded during the last use of the furnace. Archaeomagnetic dating yielded two possible time intervals between 878 to 693 BCE and 557 to 487 BCE. These new data reinforce the initial hypothesis and corroborate the temporality of these pre-Hispanic kilns. The pre-Hispanic evidence of distilled beverages still remains highly controversial in Mesoamerica. A fermented drinks from the agave plant known as pulque (agave wine) was systematically consumed in central and northern Mesoamerica before European conquest. Tequila, however, was first produced in the 16th century in the town of Tequila (State of Jalisco, Western Mexico). The distillation procedure of Tequila involves only the blue agave (tequiliana weber), while any type of agave may be used to produce Mezcal. Thus, Tequila is a type of Mezcal. It is produced in almost everywhere in Mexico, being Oaxaca State the major Mezcal producer. 
Geological and archaeological analysis of stone masonries in standing structures helps reveal information about use of natural resources. At the same time, the study of historical materials is useful for conservators and cultural heritage management. Geochemical and petrographic analysis of building material types is usually done through destructive analysis on a few selected samples and can be problematic due to the costs of operations and the size of buildings themselves. This paper demonstrates that the combination of hyperspectral imaging portable Near Infrared (NIR) spectroscopy and Energy Dispersive X-ray Fluorescence (ED-XRF) spectroscopy can be useful for analysing types of raw materials used in distinct construction phases of the inner defensive wall in the citadel of Carcassonne (Aude, France). Stratigraphic analysis of the architecture, short-range spectral remote sensing and portable ED-XRF measurements were combined in an interdisciplinary approach to classify sandstone elements. The experimental protocol for in situ non-destructive analysis and classification of the masonry types allows the investigation of the monument in a diachronic perspective, collecting information to delineate raw materials varieties and their use or re-use through time. A global project 
Glioma-associated epilepsy is associated with excessive glutamate signaling. We hypothesized that perampanel, an amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA)-type glutamate receptor antagonist, would treat glioma-related epilepsy. We conducted a single-arm study of adjunctive perampanel for patients with focal-onset glioma-associated seizures. The most common related adverse events were fatigue and dizziness. Three out of 8 participants had self-reported seizure reduction and an additional 3 reported improved control. Of these 6, 5 had isocitrate dehydrogenase 1 mutant gliomas. We conclude that perampanel is safe for patients with glioma-related focal-onset epilepsy. Further study into the association between AMPA signaling, IDH1 status and seizures is warranted. Nearly one-third of patients with primary brain tumors experience a seizure as the presenting symptom and another 30–50% develop seizures during their disease course [1]. Moreover, many of these patients have epilepsy that adversely affects quality of life. In fact, seizure control is the most important predictor of quality of life in recurrent low-grade glioma patients [2]. A challenge of treating brain tumor-related epilepsy is that anti-seizure drug (ASD) options are limited due to interactions with chemotherapy and side effects including cognitive dysfunction in an already susceptible patient population. Therefore, there is a real need to find effective ASDs that are without cytochrome P450 enzyme induction properties that are well tolerated. In conclusion, this study is consistent with the current literature demonstrating adjunctive perampanel to be a safe option for glioma patients with focal-onset seizures. However, a larger study is recommended to more fully address efficacy within this population. The observation that most responders also had IDH1 mutant tumors raises the possibility that perampanel might be effective in patients whose tumors harbor the mutation due to alterations in glutamatergic pathways. Additional study regarding IDH1/2-HG and AMPA receptor signaling is necessary. SECTION Conflict of interest
Residue analysis can be a useful way to determine the past functions of archaeological tools, particularly when teamed with other functional investigations such as usewear and technological analyses. The most common approach for residue analysis is through the use of optical microscopes, which can be used to visually identify in situ residues directly from the tool surface (with reflected light microscopes, RLM), as well as in water extractions sampled from the utilised tool edges (with transmitted light microscopes, TLM). Recently, the scanning electron microscope with energy dispersive X-ray spectroscopy (SEM-EDS) has shown great potential for archaeological residue analysis as it can provide high-resolution images at very high magnifications as well as elemental analysis of adhering material. An advantage of this instrument is that it is capable of operating in low vacuum or “environmental” mode, allowing specimens to be examined uncoated and without additional preparation, so that residues can be documented/analysed in situ on the stone. In this paper, we propose a sequential protocol for the identification of tool residues using various optical light microscopes in combination with the SEM-EDS, on residues documented both in situ on stone tools and those removed from the stone substrate in solvent extractions. We also propose a new method for analysing extracted residues using the SEM-EDS that permits high resolution images of micro-residues, particularly starch and other fibres. We argue that, although both methods have limitations and instrumental challenges, when used in combination, provide a complementary means for documenting tool residues. The identification and characterisation of tool residues makes an important contribution to studies of functional analysis and the comprehension of past tool use activities. While usewear analysis allows identifying the use motion of the stone tool and the worked material (e.g. bone, wood, antler, hide, pigments, etc.), the correct identification of a residue can potentially provide more specific information about the processed material, including the plant/animal taxa/species, or the geological origin of coloured pigments, on the condition that the functional cause of residue deposition is correctly identified (Rots et al., 2016). 
We report a 15-year-old female with POLG-related mitochondrial disease who developed severe multifocal epilepsia partialis continua, unresponsive to standard anti seizure drug treatment and general anesthesia. Based on an earlier case report, we treated her focal seizures that affected her right upper limb with 20-min sessions of transcranial direct current stimulation (tDCS) at an intensity of 2 mA on each of five consecutive days. The cathode was placed over the left primary motor cortex, the anode over the contralateral orbitofrontal cortex. Surface electromyography (EMG) were recorded 20 min before, 20 min during, and 20 min after four of five tDCS sessions to measure its effect on the muscle jerks. The electroencephalography (EEG) was recorded before and after tDCS to measure the frequency of spikes. Our results showed no statistically or clinically significant reduction of seizures or epileptiform activity using EEG and EMG, with this treatment protocol. To our knowledge, this is only the second time that adjunct tDCS treatment of epileptic seizures has been tried in POLG-related mitochondrial disease. Taken together with the positive findings from the earlier case report, the present study highlights that more data are needed to determine if, and under which parameters, the treatment is effective. Mitochondrial diseases are a group of genetic disorders affecting about one in 5000 people [1]. The symptoms are diverse but since mitochondria produce energy for body tissues through production of adenosine triphosphate (ATP), organs with high energy consumption, such as the brain, are often affected. For example, as many as 35% to 60% of people with mitochondrial disease develop seizures [1]. In POLG-related mitochondrial disease, a genetic mutation interferes with a catalytic subunit of the mitochondrial DNA polymerase gamma, which replicates mitochondrial DNA [2], leading to depleted mitochondrial DNA [3]. Once the resulting neuronal energy failure reaches a critical point, neuronal death ensues, causes atrophy and potentially acts as the trigger for epilepsy that in turn increases neuronal loss [4]. A study found mitochondrial dysfunction in one third of patients with epilepsy that underwent metabolic testing [5], emphasizing that drug-resistant seizures are a frequent problem in mitochondrial disease, and that new treatments need to be developed. In a previous case report, focal seizures in a patient with POLG-related mitochondrial disease ceased after two weeks of transcranial direct current stimulation (tDCS) [6]. Since these seizures are often refractory to medical treatment and the technique is non-invasive, we tested tDCS using similar parameters as in Ng et al. [6] in a patient with POLG-related mitochondrial disease and drug-resistant multifocal epilepsy. SECTION Case report 
The automatic regulation of blood glucose for Type 1 diabetes patients is the main goal of the artificial pancreas, a closed-loop system that exploits continue glucose monitoring data to define an optimal insulin therapy. One of the most successful approaches for developing the artificial pancreas is the model predictive control, which exhibits promising results on both virtual and real patients. The performance of such controller is highly dependent on the reliability of the glucose–insulin model used for prediction purpose, which is usually implemented with classic mathematical models. The main limitation of these models consists in the difficulties of modeling the physiological nonlinear dynamics typical of this system. The availability of big amount of in silico and in vivo data moved the attention to new data-driven methods which are able to easily overcome this problem. In this paper we propose Deep Glucose Forecasting, a deep learning approach for forecasting glucose levels, based on a novel, two-headed Long-Short Term Memory implementation. It takes in input the previous values obtained through continue glucose monitoring, the carbohydrate intake, the suggested insulin therapy and forecasts the interstitial glucose level of the patient. The proposed architecture has been trained on 100 virtual adult patients of the UVA/Padova simulator, and tested on both virtual and real patients. The proposed solution is able to generalize to new unseen data, outperforms classical population models and reaches performance comparable to classical personalized models when fine-tuning is exploited on real patients. Type 1 Diabetes (T1D) is a chronic metabolic disease characterized by high Blood Glucose (BG) level, known as hyperglycaemia. Hyperglycaemia can cause long-term complications including damage to blood vessels, eyes, kidneys, and nerves and it is caused by the dysfunction of pancreatic β-cells responsible for the production of insulin. This hormone regulates the BG concentration by allowing cells and tissues to absorb glucose from the bloodstream. T1D patients need exogenous insulin injections to keep the glucose concentration in the euglycemic range. Their goal is to minimize diabetes complications related to hyperglycemia and simultaneously avoid hypoglycemia, a condition that could be caused by excessive insulin administration. The automatic regulation of the BG concentration for people affected by T1D through exogenous insulin administrations (Cobelli et al., 2011; Cameron et al., 2011; Thabit and Hovorka, 2016) is the main purpose of the so-called artificial pancreas. The artificial pancreas is a closed-loop system that exploits the glucose measurements obtained via Continuous Glucose Monitor (CGM) to compute and automatically deliver the proper amount of insulin via subcutaneous insulin pump. The core of the artificial pancreas is the control algorithm that defines the optimal insulin amount to infuse. The Model Predictive Control (MPC) resulted into one the most promising approach to this problem in the last years, obtaining successful results both in silico and in vivo (Renard et al., 2016; Thabit et al., 2015; Kropff et al., 2015; Anderson et al., 2016; Bergenstal et al., 2016; Dassau et al., 2012; Pinsker et al., 2018). The MPC approach exploits a glucose–insulin model to forecast the BG values in order to compute the optimal insulin therapy. For this reason, the predictive performance of the model plays a key role in the overall control performance. Classical mathematical model used in these applications are not able to fully describe the nonlinear glucose–insulin dynamics. In order to overcome this limitation, the complexity of the model has to be increased and new effective identification techniques are required. Recently, a branch of the research was moved towards new identification techniques in order to have more effective models to be used for both the control algorithms and the safety systems. A complete review can be found in Zarkogianni et al. (2015). Data-driven approaches have been successfully applied to real-life applications (Baghban et al., 2019; Samadianfard et al., 2019; Wu and Chau, 2011; Moazenzadeh et al., 2018). Depending on the task at hand, the aim of these approaches is to learn a model directly from the data. Thanks to the availability of a huge amount of data collected during long-period trials in free-living conditions new data-driven approaches have also been studied in the artificial pancreas research field, with promising results (Toffanin et al., 2018, 2019). However, their performance are limited by the use of a fixed and simple structure of the chosen model. Data-driven approaches based on deep learning architecture have received an increasing attention in the last few years mainly because of the remarkable performance obtained in several research fields (Krizhevsky et al., 2012; Ronneberger et al., 2015). Among these approaches, recurrent neural networks represent a family of deep learning architectures which have been explicitly designed to model the evolution over time of a phenomenon. In particular, given an input composed of a sequence of observations from a signal, such as the BG level in our scenario, these models try to predict its future value or values. A novel solution which follows a therapy-driven approach based on deep learning has been introduced in order to predict a trend of future glucose concentration in T1D patients. The solution entails multiple models trained on the in silico adult patients of the UVA/Padova simulator. Each model is used to predict a glucose profile for a fixed prediction horizon and the individual predictions are then aggregated to obtain a profile of future glucose levels.
A 54-year-old man was admitted to the intensive care unit with an aneurysmal subarachnoid hemorrhage and subsequently underwent mechanical ventilation and received neuromuscular blocking drugs to control refractory elevated intracranial pressure. During quantitative EEG monitoring, an automated alert was triggered by the train of four peripheral nerve stimulation artifacts. Real-time feedback was made possible due to remote monitoring. This case illustrates how computerized, automated artificial intelligence algorithms can be used beyond typical seizure detection in the intensive care unit for remote monitoring to benefit patient care. Since the introduction of digital electroencephalogram (EEG) and computer-based microprocessing, methods of EEG recording have evolved from paper records to dedicated EEG digital servers that can be accessed remotely, similar to telemedicine in the intensive care unit (ICU) [1,2]. Further, raw EEG data can now be processed via computerized software by Fast Fourier transform techniques into condensed quantitative EEG (QEEG) displays with numerous mathematical derivatives for seizure detection and even surrogate cerebral blood flow inferences. The technology for EEG data analysis has advanced rapidly in the last decade, using an array of sophisticated software and artificial intelligence algorithms for seizure detection based on the EEG waveform morphology (i.e., spike detection) combined with spike frequency (> 2–3 Hz), or on a combination of amplitude, morphology, and frequency (seizure detection and artifact rejection). The use of ICU EEG has grown significantly and has received attention as a method to detect seizures in critically ill patients. Despite monitoring for a primary reason such as seizure recognition, near-real-time feedback generated by EEG analysis software can on occasion result in alternative benefits that change patient management. This patient's case highlights a new means of identifying PNS artifact using computerized ICU EEG algorithms with subsequent changes in the site of stimulation and decrease in NMB dose.
Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design. Mechatronic systems are multidisciplinary products, that incorporate an interactive and synergistic application of various domains such as mechanics, electronics, controls, and computer engineering. Due to the large number of couplings and dynamic interdependencies between subsystems and components, the design of mechatronic systems is considered to be a challenging and complex task, which requires a cross disciplinary design thought process (Torry-Smith et al., 2013; van Amerongen, 2003). This calls for a more systematic and multi-objective design approach to mechatronics (Mohebbi et al., 2014d). More precisely, a concurrent and integrated design method is needed to obtain more efficient, reliable and flexible products in less complex ways and at a lower cost (Mohebbi et al., 2014a). A number of research efforts have demonstrated that designing the structure and control concurrently, improves the system’s performance and efficiency (Cruz-Villar et al., 2009; de Silva and Behbahani, 2013; Li et al., 2001; Van Brussel, 1996; Zhang et al., 1999). Although, in most of these efforts, the mechanical structures of the system were usually determined in advance without considering the future aspects of the controller design. Therefore, a perfect control action may be far from practical concerns, due to limitations imposed by the poorly designed mechanical structure. 
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. Modeling and analysis of students’ performance is a common task that is carried out by researchers and educational institutes (Natek and Zwilling, 2014; Pena-Ayala, 2014; Shahiri et al., 2015; Hoffait and Schyns, 2017; Miguèis et al., 2018). The goal is to identify important factors that affect the learning process. Based on these factors, educational policies can be updated and teachers may be encouraged to adopt new educational approaches that enhance and improve the overall learning process. Common examined factors include the student’s personal background, the acceptance criteria and performance attributes. 
We describe a case of mesial temporal extraventricular neurocytoma (mtEVN) in a 23-year-old male presenting with drug-resistant seizures and review the literature on this rare tumor. A PubMed search was queried using the MeSH term “neurocytoma” and key search terms “extraventricular”, “temporal”, and “epilepsy”. Titles and abstracts were screened for temporal neurocytomas. References were reviewed to identify further studies. Twenty case reports were selected comparing the presentation, radiological, histopathological, and surgical outcomes of neocortex temporal EVNs (ntEVN) and mtEVNs. Gross total resection of mtEVNs under intraoperative electrocorticography monitoring typically affords an excellent prognosis and successful seizure control. Drug-resistant focal impaired awareness seizures (FIAS) are most commonly associated with mesial temporal sclerosis [1]. Fortunately, epilepsy secondary to mesial temporal sclerosis is amenable to surgical treatment with satisfactory seizure control and improvement in life of quality. However, 20–35% of patients with temporal FIAS have an underlying brain tumor responsible [2]. Correctly identifying the etiology of FIAS is imperative to obtaining a successful outcome and seizure control [3]. Thus, the diagnostic dilemma of mesial temporal FIAS has been a challenge for epileptologists and epilepsy surgeons. 
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. The ability to process food using various techniques is, along with mastery of fire and cooking, one of the key improvements that differentiated early humans from their antecedents and other animals (Wrangham, 2009; Wollstonecroft, 2011; Zink and Lieberman, 2016). Food processing facilitates the removal of undesirable substances in the raw material (Stahl et al., 1984; Johns, 1999; Stahl, 2014), helps to enhance the flavours, and extends the preservation period of foods (Caplice and Fitzgerald, 1999). 
Intraoperative electrocorticography recording is recommended for treating cavernoma-related epilepsy. However, interictal paroxysmal epileptiform activity is generally able to be recorded, but is not always identical to the epileptogenic zone. We surgically treated a 15-year-old girl with drug-resistant epilepsy associated with radiation-induced cavernoma in the right lateral temporal lobe. Electrocorticography revealed paroxysmal activities in the cortex around the cavernoma. Additionally, continuous subclinical “ictal” discharges with high-frequency oscillations confined to the histologically non-sclerotic hippocampus were recorded. Following additional hippocampectomy, a good seizure outcome was obtained. Intraoperative electrocorticography and high-frequency oscillation analysis revealed high epileptogenicity in the non-sclerotic hippocampus of this patient. Surgery for an epileptogenic cerebral cavernous malformation (cavernoma) in the temporal lobe, and the necessity of additional hippocampectomy remains in dispute [1]. When the cavernoma directly involves the hippocampus, the decision to undertake medial resection is relatively straightforward [2,3]. However, when medial structures are not involved the potential risks and benefits of medical resection must be considered carefully, on a case-by-case basis [1]. Previous electrophysiological/histopathological assessment in patients with lateral temporal cavernoma suggested a proclivity for the epileptogenic zone to encompass the medial structures, and for hippocampal sclerosis to be present [4]. Even if the extrahippocampal cavernoma is the initial origin of an electroencephalography (EEG) abnormality and/or epilepsy, it is postulated that the hippocampus is more vulnerable to secondary epileptogenesis after repeated EEG abnormality with or without seizures. This theory is referred to as “dual pathology” when coexistence of hippocampal sclerosis with an extrahippocampal cavernoma is present [1,5]. To evaluate these phenomena, guidance using intraoperative electrocorticography (ECoG) recording has been recommended [1,3,6]. Although intraoperative ECoG does not require placement of depth electrodes, it is generally accepted that the area that generates paroxysmal epileptiform activity on ECoG recorded during the operation is not always identical to the epileptogenic zone [7,8]. We surgically treated a patient with drug-resistant focal epilepsy associated with a cavernoma in the lateral temporal lobe, in whom continuous “ictal” discharges with high-frequency oscillation, confined to the ipsilateral non-sclerotic hippocampus, were recorded on the intraoperative ECoG after lateral temporal resection. Good seizure outcome was obtained following additional hippocampectomy. Herein, we discuss the epileptogenic mechanisms of this patient. SECTION Case reports 
Archaeobotanical studies carried out in the southern Sierras Pampeanas of Argentina (San Luis Province) are reported, which add new information about the presence of cultigens in the area during the late Holocene and the variety of wild species used in this period. The presence of starch grains of Zea mays (corn), Cucurbita sp. (squash), undifferentiated tubers, and Phaseolus sp. (beans), as well as phytoliths of Panicoideae, Chloridoideae, Arundinoideae, Bambuseae, Cyperaceae, Asteraceae, Arecaceae and woody dicotiledons are documented from analyses on knapped tools. The obtained data allow discussing the diversity of the resources utilized and the importance of cultigens in prehispanic times in a context that is currently considered the southern limit of prehispanic food production economies. Ever since the archaeological research of González (1952, 1960), the Sierras Pampeanas of Argentina have been considered an archaeological region with its own characteristics, distinguishable from the northwest of Argentina and from other areas. In this context, even today it is proposed that the historical trajectories of local groups are related (Berberián et al., 2013; Laguens and Bonnín, 2009). However, there is a lack of balance regarding archaeological data between the two main sectors of the Sierras Pampeanas, namely Sierras de Córdoba and Sierras de San Luis (Cattaneo et al., 2013; Heider and Curtoni, 2016). 
Primary debris found in an Iron Age bath-shaped vessel excavated from Zincirli, ancient Sam'al, Turkey, was subjected to organic residue analysis by gas chromatography–mass spectrometry (GC–MS). Bath-shaped vessels are usually interpreted as burial coffins, but recently these vessels have been linked to crafts manufacture, specifically wool processing. To test this hypothesis, we compared the total lipid extract (TLE) of the debris to six reference standards drawn from a study of Old World wool processing practices. Key components that may reflect the remains of date palm kernel oil, a substance known to be used in wool washing, were identified. In this study, gas chromatography coupled with mass spectrometry (GC–MS) was used to analyze the total lipid extract (TLE) of pebble debris found inside an Iron Age bath-shaped vessel (see Section 2.1). Residues in ceramic sherds and other porous structures may be analyzed to deconvolve their use (Evershed, 2008). The recent discovery of primary debris in a bath-shaped vessel that was discovered in-situ in a domestic context at Zincirli, Turkey, provides an opportunity to test the hypothesis that bath-shaped vessels were used for wool-processing (e.g. scouring raw wool and fulling woven woolen textiles). The identification of date palm kernel oil in the Zincirli bath-shaped vessel lends support for alternative uses for these bath-shaped vessels other than burial and for the hypothesis that bath-shaped vessels were used as wool processing installations. In that context, it is interesting to note a graffito of a date palm and a weaving loom on an Iron Age stone bath-shaped vessel from Khirbet al-Mudayna (Chadwick et al., 2000: 260; Boertien, 2013) that points to a relationship among these three elements.
Case studies can generate hypothesis based on unique clinical patient encounters and provide guidance among populations with limited numbers of patients. However, case studies are not blinded and are susceptible to a variety of factors that can influence study outcomes. One potential solution to minimize this bias is to use an N-of-1 trial. N-of-1 trials are a double-blinded randomized crossover trial within a limited number of patients, often as small as a single patient. These trials borrow many concepts from randomized controlled trials (RCTs), which in turn increases the validity of findings compared with a case report. Situations best suited for an N-of-1 trial include chronic disease states and therapies with quick onset and offset, such as in patients with seizures. There are many opportunities to use N-of-1 trials among patients with epilepsy, and providers are encouraged to explore and employ these methods. The purpose of this article was to describe N-of-1 trials along with considerations for conducting, publishing, and evaluating N-of-1 trials. Case studies have an important place in the medical literature. They can generate hypothesis based on unique clinical patient encounters and provide guidance among populations with limited numbers of patients. Case studies are usually a detailed description of one or more patient experiences. However, case studies are not blinded and are susceptible to a variety of factors that can influence study outcomes [1–4]. These include the placebo effect, the patient's desire to please their provider, patient and provider expectations of the therapy, and natural waxing and waning of the condition [2–4]. One potential solution to minimize bias found in case studies is to employ the methods from a single-case design, better known as an N-of-1 trial (Table 1). 
The Hepu Han Tombs, located on the south coast of Guangxi Zhuang Autonomous Region, in southern China, bordering the Beibu Gulf, can date back to the Han dynasty (206 BCE–220 CE), when Hepu was an important international trade port of the maritime Silk Road, spreading strong and persistent cultural influence to neighboring countries from ancient China, and also connecting China with African, European and other Asian countries in national migration, religious communication, political, trade, technical and cultural exchanges, and so on. The research of the Hepu Han Tombs can offer opportunities to understand the burial customs and the Han culture in the Chinese Han Dynasty. With the aim of detecting the location, depth and geometry of two burial mounds at Jinjiling Site of Hepu Han Tombs, an electrical resistivity tomography (ERT) survey was performed in cooperation with archaeological team to calibrate the results with detailed information from the limited drillings in the area. Besides, series of 2-D ERT sections, acquired above the larger mound, were combined into a “Pseudo 3-D” volume, and iso-resistivity surfaces were further calculated to emphasize the location and lateral variations within the data volume, expected towards a more detailed and quantitative interpretation. Both the 2-D and 3-D resistivity imaging gave a clear evidence of structural details of the burial mounds. The results have shown a general consistency between the geophysical work and traditional archaeological drilling explorations, improving our knowledge on the un-excavated mounds, and allowing detailed pre-excavation plan at the Hepu Han Tombs. Hepu County is located on the south coast of Guangxi Zhuang Autonomous Region, in southern China, bordering the Beibu Gulf (Fig. 1). The name of “Hepu” (合浦) means intersection of rivers and sea in Chinese, based on the local geographical features, as the North-South Nanliu River flows into the Beibu Gulf in the region. In the year 111 BCE, the Hepu district was conquered by the Emperor Che Liu of Han Empire, and soon after became a big city, within the area ten times larger than today. This region also became an important international trade port and a starting point of Maritime Silk Road, spreading strong and persistent cultural influence to neighboring countries from ancient China, and connecting China with African, European and other Asian countries in national migration, religious communication, political, trade, technical and cultural exchanges, and so on in the historical period. Tombs may contain important findings of great historical and economical values and they have great archaeological significance, while the characterization of burial mounds is an especially challenging geophysical problem, due to uneven topographical terrain, complicated surface environment, and complex distributions of the burial archaeological features. However, our study reveals a reliable and satisfactory result: the high-quality 2-D ERT images allow a detailed and quantitative interpretation prior to archaeological drillings, and pseudo 3-D iso-resistivity analysis also reveals the general geometry of M-1. But we have to recognize that obvious artificial errors, caused by interpolation from sparse data, affect the 3-D interpretation seriously. Although dense data coverage can enhance the modeling results and increase the success of the interpretation, complicated surface environments are always challenging for data acquisition. Anyway, our study demonstrates that the integrated approaches of ERT surveys and limited drillings are suitable and effective to characterize the burial tomb.
Cingulate epilepsy is a rare form of epilepsy. Seizures from the anterior cingulate may present with mood change, fear, hypermotor activity, and autonomic signs, while posterior cingulate seizures resemble temporal lobe seizures. We describe a child with cingulate epilepsy who experienced unpleasant/painful sensory phenomenon. The sensations were described as spiders crawling on his forehead/right leg, ladybugs causing right ear pain and bees stinging his head/right extremities. Unpleasant sensory phenomenon/pain are rarely reported in cingulate epilepsy. Recognizing the role of the cingulate in producing pain/unusual sensory phenomenon is important, and may have localizing value when evaluating children for epilepsy surgery. Cingulate gyrus epilepsy is a rare and diagnostically challenging form of epilepsy, with a myriad of clinical manifestations [1–4]. The diagnosis is challenging given the location of the cingulate gyrus, capacity of scalp electroencephalogram (EEG) to record epileptiform discharges and ictal data from this region, as well as its overlap clinically with frontal lobe epilepsy [1,2,4]. Semiologically, anterior cingulate seizures are characterized by: fright, vocalizations, hypermotor activity, complex motor manifestations, automatisms, autonomic signs and changes in mood/affect [1–4]. Gelastic seizures have also been reported to arise from the anterior cingulate gyrus [5–6]. Seizures from the posterior cingulate are less well described and may resemble temporal lobe seizures [1]. In conclusion, cingulate seizures represent a rare and semiologically diverse form of epilepsy. The presence of sensory phenomenon, which manifested in our case as both unpleasant sensations and pain may represent a unique symptom associated with seizures arising from this area, or represent spread to distinct structures. The presence of such sensory symptoms may have potential localizing value when evaluating children for possible epilepsy surgery. However, given the small number of cases published, ongoing collaboration is necessary to further delineate this. SECTION Conflict of interest
From the very beginning sheep keepers (un-)consciously selected animals with traits amenable to life and reproduction in an anthropogenic environment. Over the millennia sheep lineages developed which were adapted to the diverse landscapes and climates currently inhabited by the species. With time, human selection also fostered desired traits, such as docility, higher average milk production, abundant fleece or excessive fat depots, amongst others. However, verifying their appearance in the archaeological record is quite difficult applying standard archaeozoological methods. Here we present a Geometric Morphometric (GMM) approach that can shed light on these developments. Our study compares astragali of three wild and five domestic sheep populations. They come from prehistoric contexts in SW Asia dating to the Early Neolithic, Middle Chalcolithic and Late Bronze Age as well as modern reference series housed in natural history collections. We observe that ancient and modern wild sheep share the same astragalar morphology. Moreover, domestic sheep from Middle Chalcolithic Central Anatolia still resemble closely their Neolithic SE Anatolian wild relatives. Modern sheep populations from the United Kingdom and NW Germany form a distinct cluster, whereas sheep populations from Late Bronze Age Syria and particularly modern Karakul sheep clearly position away from all of the aforementioned groups, underscoring their distinct morphology. We conclude that the phenotypes preserved in the astragalar size and shape are at the same time a reflection of the respective population's geography, chronology and genetics. In terms of heads, sheep is the second most important mammal livestock species in the world today after cattle (FAO Statistical Pocketbook, 2015: Table 5). Sheep is probably also the livestock species with the most diverse breeds selected for particular traits including wool, milk, meat and fat. Regardless whether securing food supply was the primary reason for raising sheep in an anthropogenic environment, osseous remains from domestic sheep in early Neolithic contexts clearly illustrate that the animals' meat and fat were systematically exploited by these communities. The breeding of sheep directed towards higher milk, fat and wool yields, however, required profound knowledge of their reproductive biology and behaviour as well as breeding skills, since wild sheep – like their early domesticated relatives – did not possess substantial fat depots or woolly coats. In addition, improvement of handling techniques helped lowering stress levels in animals raised in captivity, which was essential to stimulate reproductive success and milk let-down in ewes. Since economically valuable traits such as ample wool or fat cannot be inferred directly from archaeological bones, literature continues debating the spatio-temporal origins of sheep lineages possessing these qualities (Russell, 2012: 211–256). SECTION Methodological considerations First and foremost, our comparative study of astragalar shape demonstrates the potential of GMM to separate wild from domestic sheep and to document phenotypic differences in sheep. We could show that differences and similarities in shape first addressed in a pilot study (Pöllath et al., 2018) did not relate to the sex and age of the animals investigated but reflected their affiliation to the respective population. However, the picture obtained initially separating nicely wild sheep and domestic Karakul became less clear when including additional populations. Based on the larger dataset our analyses strongly suggest that besides a domestic or wild status, shape differences also reflect the species' phylogeography, more precisely the geographically structured genetic signal in domestic sheep, as well as chronological patterning. We therefore conclude that GMM is an important tool for tracing the cultural trajectory of prehistoric sheep. Of course, many questions regarding the emergence of sheep types possessing desired traits will need to be answered applying biomolecular analyses, but for framing spatio-temporal aspects of the process, GMM seems particularly useful. The approach moreover is archaeologically advantageous, since it is non-invasive, able to process large numbers of specimens and – above all – needed to step in the breach when DNA preservation is poor, which is the case in many regions of Mesopotamia witnessing hot and arid climatic conditions. Finally, by combining this kind of GMM analysis with traditional archaeozoological approaches new vistas can be opened up. By integrating morphotype analysis and demographic profiles in single populations, for instance, we expect new light to be shed on the transition from ancient Near Eastern sheep economies focussing on meat (and milk) to more advanced production systems exploiting the species also for additional lifetime and primary products, such as wool and fat.
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. Direction of movement prediction is an important research topic in financial time series studies. Many traders prefer to know about the direction of movement of a market rather than a precise value for the prices or indices (Yao and Tan, 2000). A traders’ decision of hold, buy, or sell comes from his/her perception of future direction rather than exact values of price or index. That is the reason for development of several direction of movement prediction models in many researches like Kia et al. (2018), Li and Liao (2017), Patel et al. (2015), Imandoust and Bolandraftar (2014), Kara et al. (2011) and Huang et al. (2005). 
An important aspect of studying ancient empire formation is the role of local political economies throughout imperial fluctuations. Such insight can help us understand how imperial powers may or may not have exerted control over their subjects, and the broader impacts of imperial change on local populations. This study uses geochemical analysis (INAA) of ceramic samples and raw clays from Angamuco, located in the Lake Pátzcuaro Basin, Michoacán. Angamuco was occupied before and throughout the development of the Purépecha Empire (1350–1530 CE) and is thus an important case study for evaluating the impacts of political change on material production and manufacturing. We identify four compositional groups, two of which match previously identified groups elsewhere in the lake basin. We argue that Angamuco ceramics were largely locally produced and that raw material use remained relatively stable over long periods. The results of this study contribute to our understanding of ceramic production processes at Angamuco and will be compared to archaeometric studies in Western Mesoamerica and elsewhere. An enduring question in studies of empire formation is the role of local and regional political economies throughout broader political fluctuations (Brumfiel and Earle, 1987; Costin and Hagstrum, 1995; Hirth, 2009; Roux, 2003; Sinopoli, 2003). This question is particularly salient in Postclassic (c. 900–1530 CE) central-western Mexico, where the Purépecha Empire is thought to have operated via a top-down centralized political, economic, and social system beginning in the fourteenth century CE (e.g. Pollard, 2008; Smith and Berdan, 2003). There is limited scholarly attention to the Purépecha compared to central and southeastern Mesoamerican societies, and additional work is necessary to evaluate the impacts of the emerging empire on new subjects. Previous research indicates that the idea of a centralized empire may have been more complicated in terms of resource control and distribution (Cohen, 2016; Hirshman, 2008; Hirshman et al., 2010), but that some top-down control over metal and obsidian resources likely occurred (Maldonado, 2008; Pollard, 2016; Rebnegger, 2010; Walton, 2017). Although the study of regional political economies and the role of tribute have been important in Purépecha archaeology, there has been less attention to local political economies at one site. The examination of local political economies provides an important window into the impacts of political change on existing subject populations. Although the Late Postclassic Purépecha Empire may have controlled resources such as metal and obsidian, and they may have exerted influence over sociocultural behaviors such as religion and funerary practices (e.g. Maldonado, 2008; Pollard, 1993, 2008), control over ceramic manufacturing may have been more complicated. Based on the four compositional groups identified in our ceramic sample, pottery production at Angamuco was not substantially reorganized during the Late Postclassic period. Further, it appears raw materials were locally available and acquired throughout at least the Postclassic period; i.e. before and throughout empire formation. The results of the Angamuco ceramic sample support (Hirshman's (2003), Hirshman, 2008; Hirshman et al., 2010) previous conclusions that there was no substantial reorganization of pottery production during Purépecha imperial development.
In this paper, a new configuration of Interval Type-2 (IT2) fuzzy Proportional–Integral (PI) or fuzzy Proportional–Derivative (PD) controller of Takagi–Sugeno (TS) type is presented. An attempt is made to generalize the IT2 fuzzy PI/PD controller structure using multiple fuzzy sets. Fuzzification of the inputs is done with three or more fuzzy sets having triangular/trapezoidal membership functions. The rule base consists of only three rules to reduce the number of tuneable parameters of the controller. Minimum (Min) triangular norm and Bounded Sum (BS) triangular co-norm are used as conjunction and disjunction operators to reduce the number of rules. Karnik–Mendel (KM) type reducer and Weighted Average (WA) defuzzifier are considered to derive the analytical structure of the fuzzy controller. Properties and gain variations of the fuzzy controller are investigated. Simulation study is carried out on nonlinear dynamical systems to verify the applicability of the fuzzy controllers. Over the years fuzzy controllers have gained importance due to their ability to control complex dynamical systems without precise knowledge of the processes. Initially fuzzy control activity started with type-1 fuzzy sets and then has been extended gradually to deal with IT2 fuzzy sets. Both type-1 and IT2 fuzzy controllers are capable of handling complex control situations while the problem of uncertainty is addressed only with IT2 fuzzy sets. IT2 fuzzy sets have a very unique property that allows incorporating the uncertainty in the membership function, which is termed as Footprint of Uncertainty (FoU). IT2 fuzzy controllers (Wu and Tan, 2006, 2007) have been developed by modifying type-1 fuzzy controllers and their performance has been evaluated. The IT2 fuzzy controllers were proved to be superior over type-1 fuzzy controllers but this superiority cannot be guaranteed always. Hence a thorough understanding and rigorous mathematical analysis are required to develop a strong background for IT2 fuzzy controllers. Also a thorough understanding of FoU is required to analyze the design aspects of IT2 fuzzy controllers. In this paper we have proposed a general structure for IT2 fuzzy controller. The structure is general in the sense that we have the liberty to choose the number of fuzzy sets as well as the membership functions (trapezoidal/triangular). The structure of IT2 fuzzy controller is derived using only three rules which in turn help to reduce the number of tuneable parameters of the fuzzy controller. IT2 fuzzy controller is a variable gain PI/PD controller where the gains vary with the input variables. The controller can become linear if and only if a|α|=a|α+1|=a|α+2| and b|α|=b|α+1|=b|α+2|. The IT2 fuzzy controller becomes linear even when the FoU is nonzero. It is also shown that the structure of type-1 fuzzy controller in Raj and Mohan (2018a) is a special case of the general IT2 fuzzy controller model developed in this paper. A comparative performance analysis of linear, type-1 fuzzy and IT2 fuzzy controllers has been carried out through simulation study on three nonlinear dynamical systems. IT2 fuzzy controller can handle disturbances and uncertainties efficiently as compared to linear or type-1 fuzzy controller.
The key factor in selecting appropriate forecasting model is accuracy. Given the deficiencies of single models in processing various patterns and relationships latent in data, hybrid approaches have been known as promising techniques to achieve more accurate results for time series modeling and forecasting. Therefore, a rapid development has been evolved in time series forecasting fields in order to access accurate results. While, numerous review papers have been concentrated on the use of hybrid models and their advantages in improving forecasting accuracy versus individual models in wide variety of areas, no study is concerned to categorize and review papers from the structural point of view in numerous developed studies. The main goal of this paper is to analyze hybrid structures by surveying more than 150 papers employed various hybrid models in time series modeling and forecasting domains. In this paper, the classification of hybrid models is made based on three main combination structures: parallel, series, and parallel–series. Then, reviewed papers are analyzed comprehensively with respect to their specific features of employed hybrid structure. Through reviewed articles, it can be observed that combined methods are viable and accurate approaches for time series forecasting and also the parallel–series hybrid structure can obtain more accurate and promising results than other those hybrid structures. Besides this paper provides the viable research directions for each hybrid structure to help the researchers in time series forecasting area. Time series forecasting is a broad and active research area which is drawn considerable attention from wide variety of fields such as finance, engineering, statistics and etc. Therefore, a large amount of literature has focused on approaches that can get accurate forecasts in numerous practical applications. In general, two main ways can be traced in the literature in order to achieve desired and accurate results and/or improve the accuracy of obtained results: (1) developing and proposing new forecasting models and (2) hybridization of existing forecasting models. Hybridization is generally performed due to the lacking of the comprehensive individual model in capturing various patterns in the data, concurrently. Undoubtedly, one model is not sufficient to deal with complex real world systems with unknown mixed patterns. Combining different models is one of the most common remedy introduced in the literature aiming to take the strength of individual models in patterns modeling and recognition applied in large amount of time series forecasting articles. Pioneer researchers such as Bates and Granger (1969), Clemen (1989); Granger and Ramanathan (1984), Hibon and Evgeniou (2005), Timmermann (2006), Winkler and Markakis (1983), Bunn (1989) and Armstrong (2001) claimed that combining different models can enhance forecasting accuracy compared with individual models used separately. 
The need to protect underwater cultural heritage from biodegradation is paramount, however with many sites needing funding and support, it is hard to prioritise, thus the ability to identify high risk sites is crucial to ensure resources are best placed. In doing so a clear understanding of environmental conditions acting upon a site and abundance and composition of species present is essential to this identification. Therefore, the aim of this study was to assess the rate of biodegradation on four underwater cultural heritage sites in different marine environments by placing a series of wooden test panels in direct contact with the exposed structure on the sites. Upon recovery, test panels were photographed, X-rayed, and wood boring and sessile fouling species were identified and counted. The damage attributed to each species was recorded with CAD software. Results indicated a significant difference between sites, with HMS Invincible having the highest abundance of marine wood borers and the highest rate of surface area and volume degradation; whilst vestigial evidence of marine wood borers was found on the London, it would appear the environmental conditions had significantly impeded their survival. The study indicated further factors such as sediment type and coverage, availability of wood and the proximity of other colonised sites were also determining factors controlling the abundance of marine wood borers and the rate of biodegradation. For centuries, timber has been a focal material for shipbuilding, with its wide spread availability, natural hardiness and workable qualities (Eaton and Hale, 1993); furthermore, numerous timber species are available, dependant on region, with elm, oak and pine most commonly used in European shipbuilding until mid-19th century (Couper, 2000; McGrail, 2001). England has derived a prominent relationship with the sea, building global trade networks and developing a strong naval force (Smith, 2009; Vego, 2016); as such, the English Channel has an extensive history of trade, transport and warfare reflected in the many underwater cultural heritage (UCH) sites along the coastline (Pater, 2007). However, many of these wooden UCH sites are at risk of degradation by marine wood borers within the Amphipoda, Isopoda and Myodia (Borges, 2014). 
Several optimization algorithms have been developed in the past for application in a variety of complex real world problems. Herein, a new hybrid variant of Genetic Algorithm (GA), utilizing the sample space reduction technique of Cohort Intelligence (CI) Algorithm, referred to as Adaptive Range Genetic Algorithm (ARGA) is presented and discussed. The correctness of the proposed algorithm is established by solving 50 standard test functions. Also, the practical applicability of the proposed algorithm is demonstrated by applying it to the design and economic optimization problem of a Shell and Tube Heat Exchanger. Heat exchangers are critical industrial components used in a wide range of industrial applications. Owing to their high manufacturing costs and complicated design criteria, their economic efficiency of operation depends heavily on optimization of their design. A relative assessment, comparing the performance of the proposed method of solution to other existing algorithmic methods, is conducted. The quality and robustness of the proposed solution method highlights its adeptness in solving real world computational problems in core mechanical engineering applications. Metaheuristics are high-level problem independent frameworks that can be used for solution of multiple categories of problems without taking into account the specific mathematical structuring of the problem. In the field of Artificial Intelligence (AI), metaheuristic algorithms are a critical part of modern global optimization algorithms. In the recent past, a large number of AI based optimization metaheuristics were developed and tested for applicability in complex real-life engineering problems. One class of algorithms is Nature inspired Algorithms (NIAs) such as Ant Colony Optimization (ACO) (Dorigo, 1992), Particle Swarm Optimization (PSO) (Eberhart and Kennedy, 1995), Artificial Bee Colony Algorithm (ABC) (Karaboga, 2005), and Firefly Algorithm (FFA) (Yang 2009). The Physics-based methods include Simulated Annealing (Kirkpatrick and Vecchi, 1983), and Gravitational Search Algorithm (Rashedi et al., 2009). Other optimization algorithms include Random Optimization Algorithm (Matyas, 1965), Random Search Algorithm (Rastrigin, 1963), and Backtracking Search Algorithm (Patterson et al., 1990). Genetic Algorithms (GA) (Goldberg and Holland, 1988) and Memetic Algorithms (Moscato, 1989) are two examples of very well established Evolutionary (EA) methods. GA is an optimization algorithm whose basis is the Darwin’s principle of evolution (Goldberg and Holland, 1988). The artificial chromosomes form the population in GA and each chromosome constitutes a possible way out to a problem. The fitness function governs the quality of a solution. The randomly generated population of chromosomes, fitness-based selection followed by recombination is used to populate a next generation of solutions. In this way, GA evolves a near optimal solution even to those problems which were considered computationally hard nuts to crack. One of the characteristics and drawbacks of GA is that there is always a chance that there is a better chromosome in the search space. GA possesses good convergence characteristics; however, global optimum is not guaranteed. 
This study compared temporal lobe epilepsy (TLE) patients with amygdala lesion (AL) without hippocampal sclerosis (HS) (TLE-AL) with patients with TLE and HS without AL (TLE-HS). Both subtypes of TLE arose from the right hemisphere. The TLE-AL group exhibited a lower Working Memory Index (WMI) on the Wechsler Adult Intelligence Scale, third edition (WAIS-III), indicating that the amygdala in the right hemisphere is involved in memory-related function. [ 18F]fluorodeoxyglucose positron emission topography (FDG-PET) showed glucose hypometabolism limited to the right uncus for the TLE-AL group. The results suggest the importance of considering cognitive functions in the non-dominant hemisphere to prevent impairment after surgery. The most common cause of drug-resistant mesial temporal lobe epilepsy (TLE) is hippocampal sclerosis (HS) [1–3]. Epileptogenesus originating from the pathological hippocampus, however, often involve the amygdala that is anatomically adjacent to the hippocampus [4]. For example, Graebenitz and his colleague reported that epileptiform network of TLE with HS involved the amygdala [5]. In some cases, amygdala sclerosis coexists with HS in patients with TLE [6]. Wieser, furthermore, reported a patient with amygdala epilepsy without HS [7]. Recent reports have indicated that enlargement of the amygdala without HS can be epileptogenic [8–10]. This study explored the features of TLE with a unilateral lesion in the right amygdala, and characterized the impairment in cognitive functions. The TLE patients with an amygdala lesion in the non-language dominant hemisphere exhibited noticeable decline of WM compared with the TLE-HS patients. Areas in the non-dominant hemisphere are typically regarded as not crucial in language-related functions. Our results, however, indicated the existence of memory-related function in the amygdala in the right hemisphere.
An accurate short-term traffic forecasting model serves as an integral part to enhance the efficiency of vehicle rerouting and traffic light control strategies. The information exchange (pheromone) behavior of ants has been applied to forecast traffic conditions in existing pheromone models. These models were developed to forecast congestion on roads with signalized intersections by considering only green and red phases. Motivated by this issue, three short-term traffic forecasting models are proposed: (i) Extended Pheromone Model (EPM), (ii) Extended Pheromone Model with epsilon-Support Vector Regression (εSVR-EPM), and (iii) Extended Pheromone Model with Artificial Neural Network and Particle Swarm Optimization (ANNPSO-EPM). It is worth noticing that EPM is an algorithmic model whereas the other two are machine learning models. In all proposed models, a new color pheromone concept is proposed with two significant contributions. First, the color pheromone concept is developed to capture stochastic traffic conditions on the roads with non-signalized intersections. Second, the proposed concept is further extended to include all three color phases (red, yellow and green) to forecast dynamic changing traffic behaviors for roads with signalized intersections. The proposed color pheromone concept in EPM, εSVR-EPM, and ANNPSO-EPM is different from the existing models as it dynamically switches its computation techniques based on traffic light phases. All three proposed models can be realized through a Pheromone-based Multi-Agent System composed of Vehicle Agents and Intersection Agents coordinating locally with one another To promote practicality, Singapore City Hall map is employed in a microscopic simulator of Simulation of Urban Mobility (SUMO), showing that all proposed models outperform the other existing pheromone models. Background 
Los Bifaces site, located in the Upper Tar River Basin, comprises an assemblage of at least 11 bifaces that were recovered on a surface of 2 m2. They show intermediate stages of manufacture and would have been deposited at the same time and in a grouped manner. The use of raw materials available in the region –despite the fact that the bifaces were not made at the site- and the high percentage of intentional fractures should be noted. Technological characteristics and their relationship with regional bifacial technology are described, and different hypotheses related to the artifactual assemblage formation are discussed, taking into account both natural and cultural aspects. Los Bifaces site is understood as a cache formed during the first moments of the settlement of the basin, and fractures are related to a ritual behavior. This broadens the considerations on caching in southern Patagonia and about hunter-gatherer occupations in the glacial basin of lakes Tar and San Martín. The discussion regarding the peopling of Patagonia has been enriched by the gradual incorporation of new studied places. One of these is the basin of lakes Tar and San Martín, located in the southwest of Santa Cruz province, Argentina, at the foot of the Andes mountain range, which occupies the western end of a large glacial valley that operates as a cul-de-sac. First hunter-gatherer settlement of the basin began around 11,200–10,800 cal years BP under arid conditions at the onset of the Holocene. The region seems to have been only occasionally occupied until ca. 5500 cal. years BP. By the Late Holocene and until historical times, the frequency of radiocarbon dates increases coinciding with the stabilization of local environmental conditions (Belardi et al., 2010, 2013). 
The Zayukovo (Baksan) source is the only obsidian source known in the Northern Caucasus. We report new data, collected in 2017–2018, about exploitation of the Zayukovo (Baksan) source in the Paleolithic, including results of analysis of 34 new samples from Saradj-Chuko grotto, Mezmaiskaya cave, Sosruko rockshelter and Kasojskaya cave. This is the largest series of obsidian samples ever analyzed for the Northern Caucasus Paleolithic sites. Obsidian transport long distances indicates hominid mobility and cultural contacts within the Northern Caucasus in the Paleolithic. Obsidian is one of the most widely exploited raw materials in the Paleolithic of Eurasia and the Stone Age of Africa (McBrearty and Brooks, 2000; Negash and Shackley, 2006; Negash et al., 2006; Piperno et al., 2009). Long-term studies (e.g., Glascock et al., 1998; Shackley, 2005) indicate that the chemical composition of obsidians is unique for each eruptive event. Obsidian then becomes an especially valuable raw material for understanding human mobility in the Paleolithic. Obsidian was exploited by early hominids since the end of Oldowan as indicated by transportation long distances. In the Middle Paleolithic (MP) obsidian was transported >100 km in Central Europe (Féblot-Augustins, 1997) and >200 km in the Caucasus (Doronicheva and Shackley, 2014). In the Upper Paleolithic (UP), transport distances of obsidian increased (Féblot-Augustins, 1997; Ono, 2014; Kuzmin, 2017), sometimes >700 km (Frahm and Hauck, 2017). The obsidian source provenance research adds important data to current debates about cultural similarity, contacts, and mobility of MP Neanderthals and UP modern humans in the Northern and Southern Caucasus.
This paper addresses sequential hypothesis testing for Markov models of time-series data by using the concepts of symbolic dynamics. These models are inferred by discretizing the measurement space of a dynamical system, where the system dynamics are approximated as a finite-memory Markov chain on the discrete state space. The study is motivated by time-critical detection problems in physical processes, where a temporal model is trained to make fast and reliable decisions with streaming data. Sequential update rules have been constructed for log-posterior ratio statistic of Markov models in the setting of binary hypothesis testing and the stochastic evolution of this statistic is analyzed. The proposed technique allows selection of a lower bound on the performance of the detector and guarantees that the test will terminate in finite time. The underlying algorithms are first illustrated through an example by numerical simulation, and are subsequently validated on time-series data of pressure oscillations from a laboratory-scale swirl-stabilized combustor apparatus to detect the onset of thermo-acoustic instability. The performance of the proposed sequential hypothesis tests for Markov models has been compared with that of a maximum-likelihood classifier with fixed sample size (i.e., sequence length). It is shown that the proposed method yields reliable detection of combustion instabilities with fewer observations in comparison to a fixed-sample-size test. Recently there has been an unprecedented increase in the volume and speed of temporal data being generated by physical systems, due to the improvements in low-cost sensing and high-speed computation & communication. Typical machine learning tools, used for monitoring of physical processes, extract features from a fixed number of consecutive observations and pose a supervised learning problem for detection or classification using those features (Bishop, 2006). Similarly, fixed-sample-size (FSS) tests from the estimation theory literature (Poor, 1994) compute likelihood or posterior probability of an observed sequence of fixed length and select the class which maximizes this statistic. However, in order to perform well on all feasible sequences, the chosen sample length in these approaches is made longer than needed for most of the easily separable cases. For example, in time-critical online monitoring systems such as detection of combustion instabilities in aircraft gas-turbine engines, an early detection can enhance mitigation of structural damage in engines by avoiding the thermo-acoustic resonance and may even prevent accidents due to engine shutdown. A dynamic data-driven approach (Darema, 2004) for sequential detection and classification is needed, which can adapt the observed length of time series without any significant computational burden. Sequential hypothesis tests offer one such efficient and adaptive framework, which would allow online detection of anomalies, faults, and mode transitions in dynamical systems, where high-speed streaming data are generated. This paper presents a sequential hypothesis testing procedure for a class of Markov models of temporal data inferred by using symbolic time-series analysis (STSA) (Daw et al., 2003; Beim Graben, 2001). 
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. Archaeologists have long been interested in identifying the rock outcrops that provided the raw materials used to make stone tools. The distribution and characteristics of raw material sources are fundamental to understanding the strategies people employed to ensure that they had raw materials and/or tools available where and when they needed them (Andrefsky, 1994). The types of stone artefact assemblages found in different parts of the landscape can provide information about the extent of people's home range, the form in which raw material was carried around the landscape and the stone working activities that were undertaken in different locations (Isaac, 1986). Stone artefacts discarded at greater distances from raw material sources tend to be used more intensively, and are therefore smaller, than stone artefacts discarded near their source (Newman, 1994). Deviations from this pattern provide the opportunity to investigate the influence of other social and ecological factors, such as access to other resources and the presence of trade and exchange networks (e.g. Blumenschine et al., 2008; McBryde, 1984). The ability to determine the origin of stone raw materials provides the basis for interpreting patterns of stone artefact discard in these terms. 
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. The automatic identification of diatoms in water samples is a challenging problem that has a high impact on water quality assessment. Diatoms are a type of plankton called phytoplankton, a type of microscopic algae that live in water areas like oceans, rivers or lakes and which are used as a bioindicator of its quality (Blanco and Bécares, 2010). Diatom identification and quantification in water samples are currently done manually, which is a time consuming task. A comparison between semantic segmentation and instance segmentation is carried out to detect and quantify microscopic algae (diatoms) of 10 different taxa. This is the first time that the use of deep learning approaches is demonstrated for the identification and quantification of diatoms in images with multiple diatom shells and for more than one taxon.
The no-wait job shop where no waiting time is allowed between two successive operations of a job has a strong industrial background, especially in steel-making industry and concrete manufacturing. This study formulates the no-wait job shop problem with a total flow time criterion based on time difference and decomposes the problem into timetabling and sequencing subproblems. Several timetabling methods are designed for the total flow time criterion to generate a sequence timetable. By adopting favourable features of the iterated greedy algorithm, the population-based iterated greedy (PBIG) algorithm for the sequencing subproblem is proposed. The individuals in the population evolve by means of a destruction and construction perturbator and an insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. In order to generate starting solutions with a certain quality and diversity, the Nawaz–Enscore–Ham-based heuristics for flow shop scheduling are extended in no-wait job shops. In computational experiments based on well-known benchmark instances, timetabling methods are investigated, and it is shown that the left timetabling is superior to other timetabling methods for the total flow time minimisation. Computational results also show that the proposed algorithm significantly outperforms several state-of-the-art metaheuristics, and it could be applicable to practical production environment. In recent years, shop scheduling has been extensively studied, and enormous progress has been made in complicated flow shop and job shop scheduling (Li et al., 2019, 2014). In the traditional job shop scheduling problem (JSP), it is assumed that when a job completes a prior operation, the posterior operation does not have to be processed promptly because the job is allowed to be stored. However, there are numerous real production environments where a job has to be processed continuously until its last operation once it is started. Hence, no waiting time is allowed between any two consecutive operations of a job. Such no-wait constraints widely exist in the steel-making industry (Pinedo, 2016; Tang et al., 2000), concrete manufacturing (Grabowski and Pempera, 2000; Deng et al., 2015), chemical and pharmaceutical industries (Rajendran, 1994), food industry (Hall and Sriskandarajah, 1996), etc. A steelmaking continuous casting process generally consists of several production stages, including the steel melting process and the steel solidifying process. The heated steel must continuously go through a sequence of operations before it is cooled. Another example of the no-wait constraints is the food industry, where the canning operation has to immediately follow the cooking to ensure freshness. The job shop problem with the no-wait constraint is called the no-wait job shop scheduling problem (NWJSP), which is considerably different from the JSP because of the former’s no-wait characteristics. This study deals with the no-wait job shop scheduling problem to minimise the total flow time, which is a criterion not well-investigated previously. Based on the decomposition concept for the makespan criterion, the problem is decomposed into sequencing and timetabling subproblems and formulated with a feasible time difference. Several timetabling methods, including the non-delay, enhanced, left, and enhanced left timetabling, are developed to generate a schedule from a given job sequence. A population-based iterated greedy algorithm is proposed for the sequencing subproblem. The individuals in the population of the PBIG algorithm evolve by the key elements of the iterated greedy algorithm, namely the destruction and construction perturbator and insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. The Nawaz–Enscore–Ham-based heuristics originally presented for the flow shop scheduling problem are extended to the no-wait job shop scheduling problem.
We report a case of neuronal intranuclear inclusion disease (NIID) confirmed by detection of intranuclear inclusions in a skin biopsy specimen. Brain magnetic resonance imaging showed mild cerebral atrophy and linear hyperintensities at the corticomedullary junction on diffusion-weighted images. This patient developed nonconvulsive status epilepticus with generalized periodic discharges on electroencephalography after recurrent symptoms of paroxysmal nausea and slowly progressive cognitive decline. There have been no previous reports of NIID with nonconvulsive status epilepticus to our knowledge. Since adult patients with NIID display a wide variety of clinical manifestations, skin biopsy should be considered in patients who have leukoencephalopathy of unknown origin. Neuronal intranuclear inclusion disease (NIID) is a rare and slowly progressive neurodegenerative disease that has been reported as both sporadic and familial cases. The cardinal symptoms of NIID are slowly progressive dementia associated with parkinsonism, cerebellar ataxia, and peripheral neuropathy [1–3]. In addition, the disease is also associated with various types of seizures in adult and pediatric patients [2,3]. Immunohistochemical examination of biopsy specimens from the skin or subcutaneous abdominal fat was recently reported to be useful for diagnosis of NIID [4,5]. Case reports about patients with this disease have been gradually increasing, demonstrating considerable variation of its symptoms and clinical course [2,3]. However, genetic analysis for NIID or diagnostic criteria for this disease has not been established to date. 
Pyrometamorphic rocks produced by natural coal combustion appear at archaeological sites across North America but have received little archaeological attention regarding provenance studies. Tertiary Hills Clinker is a distinct pyrometamorphic rock from Subarctic Canada utilized by hunter-gatherers from 10,000 years ago to European contact. We employ X-ray diffraction, thin section analyses, and electron probe microanalyses to characterise Tertiary Hills Clinker and inform archaeometric studies of rock produced by combustion metamorphism. We geochemically compare pyrometamorphic rocks used by pre-contact people across North America to demonstrate that Tertiary Hills Clinker can be sourced using portable X-ray fluorescence. Results indicate that Late Pleistocene/Early Holocene exchange networks in North America were larger than previously thought. A later change in the distribution of Tertiary Hills Clinker may relate to a Late Holocene volcanic eruption (White River Ash east) that fragmented modes of lithic exchange and associated social networks with potential stimulus for a subsequent large-scale migration of northern hunter-gatherers across the continent. Provenance studies of pyrometamorphic artifacts offer untapped opportunities to study social networks in coal-bearing regions across the world. Pyrometamorphism (also called combustion metamorphism) generally occurs when coal, oil, or gas burn with sufficient energy to bake or fuse neighbouring rock (Allen, 1874; Bentor et al., 1981; Cosca et al., 1989; Grapes, 2011:21; Stracher et al., 2010). Beds of fused rock were targeted for stone tool production because of the raw materials' internal uniformity (Cinq-Mars, 1973; Clark, 1986; Curran et al., 2001; Fredlund, 1976). Pyrometamorphic rock has received geological attention in North and South America (Hefern and Coates, 2004; Henao et al., 2010), Europe (Žáček et al., 2015), Russia (Sokol et al., 1998), China (Song and Kuenzer, 2017), and Africa (Pone et al., 2017). Despite the global distribution of rock produced by coal combustion and the use of it by people, comparatively few efforts have been made by archaeologists to formally identify pyrometamorphic rock in archaeological assemblages (Estes et al., 2010; Hughes and Peterson, 2009; Le Blanc, 1997; Vapnik et al., 2015). 
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. Background and motivation In this study, the customer relationship management in the automobile repair and maintenance business is analyzed for 4S shops. In particular, we focus on the customer churn and absence behaviors. A dataset of records obtained over a period of more than six years is employed and analyzed; it includes the lifecycles of more than 15 000 customer absence and presence behavior records. To our knowledge, this study seems to be the first to utilize such a large dataset to investigate the CRM in the 4S shop aftermarket.
The Late Viking Age Swedish runestones are commonly acknowledged as early Christian monuments. Using geostatistical techniques and descriptive statistics, we systematically investigate the regional-to-local spatiotemporal patterns of 1302 ornamentally dated Swedish runestones regarding the timing and speed of the Christianisation process. After quantitative geostatistical analyses of the age distribution patterns of Swedish runestones, we evaluate whether the observed patterns correspond to the pace and pattern of Christianisation, as represented by the presence of mission bishoprics, early church sites, late pagan grave sites and royal estates. We identify seven distinct age groups of runestones and statistically significant regional-to-local spatiotemporal differences in the age and age spread of runestones. The oldest runestones, with the smallest age spread, are found in south-western medieval Sweden, and the youngest, as well as the largest age spread, in the north-east, respectively. We find that runestones are significantly older close to early ecclesiastical sites, regardless of the analytical level, and significantly younger near to late pagan graves. The results obtained are inconclusive as to whether runestones are older near royal estates. Our results support that the spatiotemporal patterns of runestone sites mirror the timing of the Christianisation process and that geostatistical approaches to larger archaeological or historical data sets can add new dimensions to the understanding of the spatial dimensions of past societal changes. Background 
In this work, twenty-three pieces of high-fired glazed samples from the archaeological excavations of the Dong Xia sites in Russia's Primorye region were studied. In addition to visual observation, energy dispersive X-ray fluorescence (EDXRF) and field emission scanning electron microscopy (FESEM) techniques were jointly applied to the analysis of the chemical composition and microstructure of the samples. The major emphasis was placed on the provenance identification of the white, opalescent glazed, and celadon samples. Seven excavated fine whiteware samples were confirmed as the products from the Ding Kiln in the Jin Dynasty, and nine samples as the Jun-series wares. The most important conclusion out of the study is that two celadon samples with opaque glaze were proved to be the Ru Kiln wares. This finding implies that the Ru ware is identified for the first time in the Dong Xia sites of Russia's Primorye region. This work is a successful case of interdisciplinary cooperation in the medieval archaeometry of the Far East. This work is devoted to the high-fired glazed wares excavated from the Late Jin Dynasty (Dong Xia State) sites in Russia's Primorye Region. The Jin Dynasty lasted for 120 years (1115–1234 CE) and was first founded by Jurchen people. During its peak evolution, the Jin Empire covered an extensive area geographically, including the north of Huaihe River, the northeast of Qinling Mountains in China, and Russia's Far East of today. The economy of the Jin Dynasty was inherited mainly from the Northern Song Dynasty (960–1127 CE), and the ceramics and iron-smelting industries were prosperous. Russia's Primorye region of today was once under the jurisdiction of “Xu Pin Lu”, an administrative division of the Jin Dynasty. From 1215 to 1233, this region was controlled by the Dong Xia State, a short-lived kingdom established in Northeast China by Jurchen warlord Puxian Wannu in 1215 during the Mongol conquest of the Jin Dynasty. It was eventually conquered by the Mongolians and was later put under the Liaoyang province by the Yuan Dynasty (1271–1368 CE). The locations of the capital of “Xu Pin Lu” of the Jin Dynasty and the capital Kai Yuan City of the Dong Xia State were tracked down to today's Krasny Yar of Ussuriisk in the downstream of the Suifen River. Over 30 mountain-walled towns, such as Krasny Yar, Shaiga, and Anan'evka etc., make up the main body of the relic sites of Russia's Primorye region (Artemieva and Usuki, 2010; Khorev, 2012). Most of the mountain-walled towns discovered in Russia's Primorye region were dated to the period of the Dong Xia. The high-fired glazed wares excavated from these sites shed new light on understanding people's lives in “Xu Pin Lu” and the Dong Xia State, and their commercial and cultural exchanges with the inland regions of China (Peng, 2016). 
Focal neuroinflammation is considered one of the hypotheses for the cause of temporal lobe epilepsy (TLE) with amygdala enlargement (AE). Here, we report a case involving an adult female patient with TLE-AE characterized by late-onset seizures and cognitive impairment. Anti-N-methyl-d-aspartate receptor (NMDAR) antibodies were detected in her cerebrospinal fluid. However, administration of appropriate anti-seizure drugs (ASD), without immunotherapy, improved TLE-AE associated with NMDAR antibodies. In the present case, two clinically significant observations were made: 1) anti-NMDAR antibody-mediated autoimmune processes may be associated with TLE-AE, and 2) appropriate administration of ASD alone can improve clinical symptoms in mild cases of autoimmune epilepsy. In recent years, an increasing number of reports have indicated an association between amygdala enlargement (AE) and temporal lobe epilepsy (TLE) [1]. Focal neuroinflammation is considered one of the hypotheses for the cause of TLE-AE. However, the etiology is not fully understood, and there is no consensus regarding its treatment. Here, we describe a patient with TLE-AE associated with Anti-N-methyl-d-aspartate receptor (NMDAR) antibody. Administration of appropriate anti-seizure drugs (ASD), without immunotherapy, improved her clinical symptoms. SECTION Case 
The ‘Euphrates Monochrome Painted Ware’ (henceforth EMPW) is a ceramic style attested in the Middle Euphrates region in northern Syria at the beginning of the Early Bronze Age, ca. 2900–2700 BCE. This style is not an isolated phenomenon; rather, it must be understood in the context of a general, albeit short-lived, re-introduction of painted ceramics into local assemblages of Greater Mesopotamia. In the present study, we investigate the technology and provenance of the painted pottery from Tell el-'Abd (North Syria) and its relation to contemporary ceramics retrieved at this site. We apply a combination of macroscopic observations, ceramic petrography, and micro X-ray diffraction (μ-XRD2) in order to reconstruct the manufacturing process and to define the mineralogical and chemical composition of the sherds as well as of the pigments used for the painted decoration. The results of these analyses are then compared to the local geology in order to identify possible raw material sources. Based on the evidence, we provide the first interpretation of the provenance and technology of the Euphrates Monochrome Painted as well as unpainted ceramics of the assemblage. In Greater Mesopotamia, the Early Bronze Age (EBA), roughly corresponding to the time span between 3100 and 2000 BCE, constitutes a period of dramatic social, political, and economic change (Akkermans and Schwartz, 2003: 211–287; Cooper, 2006). This led to the rise and successive consolidation, at least in some areas, of a new urban model which through alternate phases would have characterised the history of the region in the millennia ahead. This study has shown that a technological approach combining macroscopic observations, ceramic petrography, and non-destructive μ-XRD2 is crucial for the reconstruction of provenance and aspects of manufacturing as well as the characterisation of pigments of ancient pottery (Quinn, 2013; Tite, 1999). The examination of the macrotraces revealed that vessels with different macroscopic fabrics and functions were manufactured employing the same forming techniques. By defining compositional groups and comparing them to the local landscape, it was possible to suggest a local, i.e. Middle Euphrates, origin for the painted and plain pottery retrieved at Tell el-'Abd. Moreover, the evidence, together with the presence of by-products of pottery making (over-fired sherds and wasters), support the proposal of a local, on-site production for the EMPW. Taken together, our results validate the original macroscopic classification of the sherds and allow to ascertain commonalities in raw materials and manufacturing among different ceramic products. Finally, it is hoped that additional, future archaeometric analyses of EBA assemblages from the Middle Euphrates will enrich our knowledge of the ceramic technology of this period and further contribute to our understanding of the cultural contacts among the sites and communities living at the time in the area.
Locating the subtle and uneven deposition of human activities across the landscape continues to challenge archaeologists. Existing tools (e.g. excavation, shovel testing, pedestrian survey, and terrestrial geophysics) have proven effective at locating many types of archaeological features but remain time-consuming and difficult to undertake on densely vegetated or topographically complex terrain. As a result of these limitations, key aspects of past communities remain largely outside of archaeological detection and interpretation. This flattening of past lifeways not only affects broader understandings of these communities, but can also negatively impact the preservation of archaeological sites. This paper presents the detection of archaeological features through an analysis of drone-acquired thermal, multispectral, and visible light imagery, alongside historical aerial photography, in the area surrounding Middle Grant Creek (11WI2739), a late prehistoric village located at Midewin National Tallgrass Prairie in Will County, IL. Our investigations discovered a probable housing area and a ritual enclosure, increasing the area of the site from 3.4 ha to 20 ha. The proposed housing and ritual areas of this village also help contextualize finds from the ongoing archaeological excavations at Middle Grant Creek. More broadly, results demonstrate the valuable contributions that these relatively new archaeological survey methods have in shaping our understandings of the archaeological landscape and highlight the importance of integrating them into the archaeological toolkit. Archaeologists agree that narrowly defined sites, areas with high concentrations of artifacts, are small components of the complex and textured ways in which people make use of and interact with broader landscapes (e.g. Basso, 1996; Erickson, 2008; Gordillo, 2014; Ingold, 1993; Lelièvre, 2017; Yaeger and Canuto, 2000). However, documenting the varied traces of past human activities, which are often spread over large areas and produce little readily identifiable surface signatures, remains a perennial challenge. These challenges are heightened in locations where materials used for prehistoric construction (e.g. wood) easily degrade and where industry, housing, and agriculture have heavy impacts. Locating the often fragmented archaeological record in these contexts has previously required time intensive and expensive survey techniques, like pedestrian survey and shovel testing, that are poorly suited to documenting broadly dispersed archaeological landscape features. This paper demonstrates the value of advances to a suite drone-based sensor technologies alongside experimental image processing techniques for archaeological prospection at the Middle Grant Creek site in northern Illinois, USA. Collecting visible light, multispectral NIR, and thermal infrared imagery at various times of day and night across a diurnal cycle, and analyzing these data in concert with historical and modern aerial photography, we have been able to discern a range of previously unrecognized prehistoric features. Analysis reveals numerous probable housing structures across an area of >20 ha, just east of an activity and processing area currently being excavated by the Kankakee Protohistory Project. If the village interpretation is correct, Midewin would be the largest and most complete Huber site ever located.
This paper provides the preliminary results from an integrated study of the Hellenistic and early Roman glass vessel assemblage from the large domestic context at Paphos named the House of Orpheus. The homogeneous appearance of the omnipresent slumped and cast vessels in late Hellenistic and early Roman contexts within the entire Mediterranean makes it fairly complex to define the origin of specific assemblages when only studying the external features. We present here the results of a combined study of in-situ optical analysis by means of absorption spectroscopy and chemical analyses with SEM-EDS and LA-ICP-MS. In a first step 107 fragments were optically analysed to categorize possible glass groups and 54 selected pieces were sampled to characterize their chemical composition. Four optically defined glass types can be distinguished with respect to the use of a decolouring or colouring agent and the impact of the furnace condition. The chemical analysis techniques define four distinct subgroups within the homogeneous cluster of Levantine glass. Since 1983 (with a long break between 1992 and 2009) excavations have been conducted by Demetrios Michaelides in Nea Paphos at the site of the House of Orpheus. These have brought to light a sequence of buildings, the earliest of which dates to the late 4th c. BC, and is more or less contemporary with the foundation of Nea Paphos. The latest of the structures is a habitation dating to the mid-Roman period, which has been named the House of Orpheus, after the most important of its mosaics, which depict the mythical hero charming the beasts with his music (Michaelides 1991). These different structures show that the site remained continuously occupied all through the Hellenistic and the Roman Imperial period up to the 4th c. AD, when Nea Paphos was devastated by destructive earthquakes. After this date the area of the House of Orpheus appears to have been abandoned (Michaelides, 1983–1998, 1986, 1991a, 1991b). 
Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed. Saliency detection in computer vision is the process to determine the most prominent and conspicuous parts of an image. Selective attention is embedded in our cognitive system and a lot of the tasks we do in every day life depend on it. Saliency detection has applications in a variety of supervised and unsupervised tasks (Frintrop and Jensfelt, 2008; Walther and Koch, 2006; Zdziarski and Dahyot, 2012; Bi et al., 2014; Hong et al., 2015; Mahadevan and Vasconcelos, 2009; Ren et al., 2013). For example, salient object detection can provide informative prior knowledge to objectness detection. The extracted bounding box locations which are more prominent and salient in an image would be more likely to contain the objects of interest (Han et al., 2018). Due to this fact, some objectness detection methods use saliency cues to detect objects of interest (Alexe et al., 2012; Erhan et al., 2014). 
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. Prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL) (Zio and Di Maio, 2010; Zio, 2012; Liao and Köttig, 2014; Palacios et al., 2015; Prytz et al., 2015; Lei et al., 2018). Prognostic methods are classified into model-based and data-driven (Schwabacher and Goebel, 2007; Zio and Di Maio, 2010). Model-based approaches, which are based on the use of physics-based models of the degradation processes, are typically applied to safety-critical and slow-degrading equipment whose degradation mechanisms have been extensively studied (Cai et al., 2017b). By contrast, data-driven approaches are typically used when accurate physics-based models of the, possibly competing, degradation processes which the components of industrial systems are subjected to are not available (Hu et al., 2012a, b; Medjaher et al., 2012; Rigamonti et al., 2017; Sardá-Espinosa et al., 2017; Huang et al., 2019). Since they require a large amount of run-to-failure degradation trajectories for model training (Hu et al., 2012a, b), data-driven approaches are typically applied to non-safety critical systems characterized by relatively short mean times to failure. They are distinguished into two approaches (i) degradation-based, which indirectly predict the system RUL by estimating the future evolution of the component degradation until a failure threshold is reached (Rigamonti et al., 2016a, b; Lim et al., 2017) and (ii) direct RUL prediction-based, which predict the system RUL by developing a direct mapping from the condition monitoring signals to the system RUL (Khelif et al., 2017). Although degradation-based approaches are closer to physics-based reasoning, they are more difficult to develop than direct RUL prediction-based approaches when the quantification of the component degradation and the identification of a failure threshold are not straightforward (Medjaher et al., 2012). 
The easy accessibility and simplicity of Short Message Services (SMS) have made it attractive to malicious users thereby incurring unnecessary costing on the mobile users and the Network providers’ resources. The aim of this paper is to identify and review existing state of the art methodology for SMS spam based on some certain metrics: AI methods and techniques, approaches and deployed environment and the overall acceptability of existing SMS applications. This study explored eleven databases which include IEEE, Science Direct, Springer, Wiley, ACM, DBLP, Emerald, SU, Sage, Google Scholar, and Taylor and Francis, a total number of 1198 publications were found. Several screening criteria were conducted for relevant papers such as duplicate removal, removal based on irrelevancy, abstract eligibility based on the removal of papers with ambiguity (undefined methodology). Finally, 83 papers were identified for depth analysis and relevance. A quantitative evaluation was conducted on the selected studies using seven search strategies (SS): source, methods/ techniques, AI approach, architecture, status, datasets and SMS spam mobile applications. A Quantitative Analysis (QA) was conducted on the selected studies and the result based on existing methodology for classification shows that machine learning gave the highest result with 49% with algorithms such as Bayesian and support vector machines showing highest usage. Unlike statistical analysis with 39% and evolutionary algorithms gave 12%. However, the QA for feature selection methods shows that more studies utilized document frequency, term frequency and n-grams techniques for effective features selection process. Result based on existing approaches for content-based, non-content and hybrid approaches is 83%, 5%, and 12% respectively. The QA based on architecture shows that 25% of existing solutions are deployed on the client side, 19% on server-side, 6% collaborative and 50% unspecified. This survey was able to identify the status of existing SMS spam research as 35% of existing study was based on proposed new methods using existing algorithms and 29% based on only evaluation of existing algorithms, 20% was based on proposed methods only. This study concludes with very interesting findings which shows that the majority of existing SMS spam filtering solutions are still between the “Proposed” status or “Proposed and Evaluated” status. In addition, the taxonomy of existing state of the art methodologies is developed and it is concluded that 8.23% of Android users actually utilize this existing SMS anti-spam applications. Our study also concludes that there is a need for researchers to exploit all security methods and algorithm to secure SMS thus enhancing further classification in other short message platforms. A new English SMS spam dataset is also generated for future research efforts in Text mining, Tele-marketing for reducing global spam activities. The continuous escalation of mobile devices over the years has given users an unbeatable communication experience which has increased users’ performance efficiently (Tully and Mohanraj, 2017). The positive impact of mobile devices on users has made information literally in the palm of everyone (Davidow, 2011). However; users are yet to move with the changes associated with understanding and managing these devices. The most popular and widely used service of the Global System for Mobile communication (GSM) is the Short Message Services known as SMS. This fast and ever-growing service has reached more than 6 billion users globally with approximately 9.5 trillion SMS sent globally in 2009 (Fernandes et al., 2015; Ezpeleta et al., 2016). Thus, SMS service has shown its uniqueness and proven its reliability as it remains one of the most primary communication tools among mobile users based on its ease of use, cheap, compatible and real-time services. 
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. Many real-world optimization problems are increasingly becoming challenging as they often concern a big number of decision variables, complex nonlinear constraints and objective functions. The global optimization using traditional approaches like numerical methods becomes less powerful especially when objective functions or constraints have multiple peaks. Metaheuristic algorithms, powerful tools for handling challenging optimization problems, are increasingly becoming popular. The popularity drives from the following aspects. In terms of the intelligent foraging behaviors of manta rays, a new optimization approach, named Manta Rays Foraging Optimization (MRFO), is proposed in this study. This algorithm has three foraging operators to mimic manta rays’ hunt for food, including chain foraging, cyclone foraging, and somersault foraging. This approach, with few adjustable parameters, is easy to implement, which in turn makes it very potential for applications in many engineering fields. A diverse set of benchmark functions including unimodal, multimodal, low-dimensional and composition functions are used to confirm the performance of MRFO from different aspects. The comparisons show MRFO is often superior to other well-known competitors.
Artificial hydrocarbon networks (AHN) – a supervised learning method inspired on organic chemical structures and mechanisms – have shown improvements in predictive power and interpretability in comparison with other well-known machine learning models. However, AHN are very time-consuming that are not able to deal with large data until now. In this paper, we introduce the stochastic parallel extreme artificial hydrocarbon networks (SPE-AHN), an algorithm for fast and robust training of supervised AHN models in high-dimensional data. This training method comprises a population-based meta-heuristic optimization with defined individual encoding and objective function related to the AHN-model, an implementation in parallel-computing, and a stochastic learning approach for consuming large data. We conducted three experiments with synthetic and real data sets to validate the training execution time and performance of the proposed algorithm. Experimental results demonstrated that the proposed SPE-AHN outperforms the original-AHN method, increasing the speed of training more than 10,000x times in the worst case scenario. Additionally, we present two case studies in real data sets for solar-panel deployment prediction (regression problem), and human falls and daily activities classification in healthcare monitoring systems (classification problem). These case studies showed that SPE-AHN improves the state-of-the-art machine learning models in both engineering problems. We anticipate our new training algorithm to be useful in many applications of AHN like robotics, finance, medical engineering, aerospace, and others, in which large amounts of data (e.g. big data) is essential. Machine learning is continuously releasing its power in a wide range of applications. Large and big data paradigms allow these algorithms to make more accurate and timely predictions; but it carries a cost that involves challenges in model scalability and distributed computing. Moreover, machine learning and data mining tasks in big data includes different nature of inputs that typically exhibit high dimensionality, e.g. more than 1000 features, far from current acceptable scales computing in a single machine (Bekkerman, 2012). This work proposed a stochastic parallel extreme artificial hydrocarbon networks, an algorithm for fast and robust training of supervised AHN models in high-dimensional data. The proposed training algorithm comprised of three key components: a generalized inverse implementation for learning parameters in molecules, parallel-computing based on PSO for near optimal convergence of centers of molecules, and stochastic learning approach for consuming large data.
The late antique/early medieval age in Central Italy is a well-suited context to verify the implications of the end of the natron glass supplies, and to explore the beginnings of the new plant-ash glass technology. We present the results of a LA-ICP-MS analysis campaign conducted on archaeological glass finds excavated at the Santa Maria della Scala hospital site in Siena and in Donoratico. This provided us with major, minor and trace element quantitative data for 49 glass samples belonging to drinking vessels and lamps, dated mainly between the 5th and the 8th century. On the basis of these data, we have sought to identify the working processes and possible glassware trade that are reflected in the glass composition. Major and minor element contents revealed that most samples, also at the later boundary of the explored timeframe, fit well within known late Roman glass classifications (e.g. HIMT, Levantine). Trace element analysis provided further information on the raw materials that were used in the glassmaking process, indicating the use of coastal sands as a silica source and allowing us to formulate different hypotheses on the materials used for the colouring process. This study aims to increase the insight into the working processes and glassware trade in Italy during the transition time between the end of the Roman period and the beginning of the Middle Ages, including the type of materials used for glassmaking and for improving its quality. Both Roman and medieval glass have been extensively studied in the past (see e.g. Janssens, 2013) and further studies have been dedicated to the exploration of the effective onset of the new processes leading to typical Medieval glass compositions„ i.e., based on vegetable ash as a flux in place of natural evaporites. Recent studies on late Antique and early Medieval glass from Italy have revealed co-existence of several glass compositions from the 4-5th century onwards, almost all natron-based and mainly compatible with the previously identified HIMT, Strong HIMT, Levantine and Série 3.2 glass groups, next to earlier Roman blue-green glass (Gliozzo et al., 2015, 2016, 2017; Silvestri et al., 2017; Gallo et al., 2014; Maltoni et al., 2015, 2016; Silvestri and Marcante, 2011), while a few objects dated to the 6-7th century, and 6-11th, excavated in the Tuscan site of San Genesio already featured a vegetable ash glass composition (Cagno et al., 2012). In order to expand our knowledge on this transition, in particular in the Tuscan area, we have selected glass samples that are precisely dated to the period from the 5th to 8th century and originate from the Santa Maria della Scala site in Siena, Italy, next to a small set of later, clearly early medieval glasses dated to the 10–11th century from the same site and to the 11-12th century from the village of Donoratico, in the same region. SECTION The historical sites of Santa Maria della Scala and Donoratico 
This paper examines the stone tool technology from the site of Kenure, Co. Dublin, on the East coast of Ireland. Collected in the 1940s by the avocational archaeologist Gwendoline C. Stacpoole, Kenure represents an extremely large surface assemblage likely belonging to the later phase of the Irish Late Mesolithic period (~7000–5500 BP). The results of this analysis affirm the likelihood that the Kenure lithic assemblage does, in fact, date to the Late Mesolithic. Core reduction was conducted using hard hammers and was generally expedient, usually involving the splitting of locally occurring glacial till chert cobbles followed by the use of single platform core reduction strategies. Retouched tools were characterized by a range of scraper, notch, denticulate, and borer forms. Moderate frequencies of convergent flakes were also present, with many fitting the typological definition of Bann flakes. In addition, there were noticeable numbers of small blades, though these generally lack the specialized technical features of Mesolithic blades from the Irish Early Mesolithic and Mesolithic sites in other regions of Western Europe. Moderate frequencies of pieces derived from bipolar percussion were also present in the Kenure assemblage and this paper offers an explanation of the use of bipolar percussion in relation to the small size of locally available lithic raw materials. Finally, this paper concludes with a consideration of the implications of the technological features of the Kenure assemblage for the organization of Irish Late Mesolithic foraging technology, as well as the potential for future research on the Kenure collection. For nearly a century, archaeological remains dating to the Irish Later Mesolithic (~7000–5500 BP; hereafter ILM) have been documented along Ireland's coastlines and waterways. Yet, ILM lithic industries remain known mostly from a small number of excavated sites and a scattering of surface assemblages. This paper offers an assessment of a somewhat odd source of information on variability in ILM lithic technology: the surface collections made by Gwendoline C. Stacpoole in the 1940s at locations along the Leinster coast north of Dublin (Liversage, 1961; Stacpoole, 1963; Woodman, 1977a, 1978). These assemblages are obviously not without some serious problems in terms of provenience. Their collection was poorly documented and their chronology is ambiguous relative to systematically excavated sites from this time period. Yet, what the Stacpoole collections lack in documentation quality, they perhaps make up for in quantity. Housed in the National Museum of Ireland in Dublin, these collections likely include hundreds of thousands of lithic artifacts.1 While it is clear that not all of these lithics belong to the ILM, most of them probably do. In particular, on the basis of index typology at least, the Stacpoole collection from the site of Kenure, Co. Dublin, appears to be dominated by ILM artifacts. Thus, while far from perfect, the Kenure collection would seem to be a useful and under-explored sources of information about the ILM. 
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. The main task of a Credit Scoring system is the evaluation of new loan applications (from now on named instances) in terms of their potential reliability. Its goal is to lead the financial operators toward a decision about accepting or not a new credit, on the basis of a reliability score assigned by the Credit Scoring system (Morrison, 2004). In a nutshell, the Credit Scoring system is a statistical approach able to evaluate the probability that a new instance is considered reliable (non-default) or unreliable (default), by exploiting a model defined on the basis of previous instances (Mester et al., 1997; Henley, 1994). Banks and credit card companies use credit scores to determine who qualifies for a loan, at what interest rate, and at what credit limits. Therefore, Credit Scoring systems reduce losses due to default cases (Henley et al., 1997), and, for this reason, they represent a crucial instrument. Although similar technical issues are shared, Credit Scoring is different from Fraud detection, which consists of a set of activities undertaken to prevent money or property from being obtained through false pretenses. 
Archaeological materials in museum collections provide an excellent opportunity for researchers to investigate social, cultural, and environmental change. However, the precision of the archaeological analysis and interpretation is dependent on a firm understanding of the site chronology. The Par-Tee site (35CLT20), located on the northern Oregon Coast, produced a large archaeological collection including artifacts and faunal remains excavated in the 1960s and 1970s. Radiocarbon dates have been obtained on materials from the Par-Tee collections by several different researchers since the 1970s, but these data have not been adequately assessed for chronometric hygiene. To establish a reliable chronology for the Par-Tee site, we obtained new high-resolution accelerator mass spectrometry (AMS) radiocarbon dates and collagen peptide mass fingerprinting of cervid bones from throughout the site. We evaluate these new radiocarbon dates along with previous radiocarbon dates from the site, using chronometric hygiene assessments and Bayesian statistics to build a refined chronology for the Par-Tee site and museum collection. Previous research suggests site habitation occurred between 350 cal BC to cal AD 1150. Our reassessment of the site chronology suggests the primary site habitation occurred from cal AD ~100–800. We also identified evidence of subsequent site occupation around cal AD ~1490–1635 supporting previous interpretations of site habitation after the primary shell midden forming occupation. The latter occupation may be associated with a change in site use from a semi-sedentary village to a cemetery. Museum-based archaeological research offers access to unique archaeological materials sometimes unobtainable through modern studies—a result of the often large-scale of late-nineteenth and early twentieth-century excavations. Although concerns of decreasing museum repository space are increasing (Bawaya, 2007), anthropological research of museum collections is addressing a diverse range of anthropological, biological, and conservation issues (Sholts et al., 2016). Collections-based studies, however, can be complicated by the differential excavation, recovery, and curation histories of museum collections (Bawaya, 2007; Pearce, 1994). As Luby et al. (2013) note, these early museum assemblages endure as legacy collections rather than as “old” excavation materials. The term legacy highlights the inherent value and the importance of museum collections from the perspective of stewardship, research, and education. 
In 2003, the excavation of the ancient site Baochuanchang Shipyard was carried out in Nanjing, China and this shipyard was believed to have been the workshop where the huge vessels of Zheng He's fleet were built and maintained. Several pieces of ancient putty were found here, and a small piece was sampled and analyzed in this paper to study its components and structure. The results of X-ray diffraction (XRD) showed that the putty was mainly composed of calcite (CaCO3), while pyrolysis-gas chromatography–mass spectrometry (Py-GC–MS) analysis indicated that tung oil was used in making the putty. In addition, plant fibers inside the putty were identified as jute by the means of polarizing microscope and scanning electron microscope (SEM) analysis. These findings confirmed the putty as the so-called chu-nam putty, which was a traditional sealing material used in ancient shipbuilding. Moreover, the surface morphology of the putty was obtained by SEM, and pore size distribution was measured by gas adsorption-desorption analysis. The analytical results suggested that the putty was quite compact, which could perform well in sealing huge wooden ships. This research revealed the shipbuilding skills of the Baochuanchang Shipyard. It may provide reference for studying Zheng He's vessels and make further a contribution to the conservation and restoration of ancient wooden ships of the Ming Dynasty. China used to hold the leading position in shipbuilding technology before the industrial revolution (Fang et al., 2013; Li, 2006). The invention of the sealing material with high waterproofing and bonding properties made a great contribution for ancient Chinese craftsmen to build huge ships for oversea voyage. Historical sources describe this material as a mixture of boiled tung oil, lime and oakum (optional) (Fang et al., 2013; Song, 1637). It was called “nianliao” in Chinese while Worcester (1971) called it “chu-nam putty”. Zheng He's expedition crossed the Indian Ocean was a milestone recorded in Chinese marine history. However, due to a lack of historical records, the shipbuilding technologies used on Zheng He's ships especially the sealing material remains unknown. In this paper, the ancient putty excavated from the site of the Ming Dynasty Baochuangchang Shipyard was analyzed. The results proved that the putty is the so-called chu-nam putty mainly consisted of calcite, quartz and carboxylates. Tung oil was applied as waterproofing agent and the mixture of jute was used to prevent the putty from crack. In addition, employed with high content of tung oil would improve the waterproofing and binding properties of the putty, which could effectively ensure a long-time oversea voyage. These results overall revealed the sealing material that used in shipbuilding at the Baochuanchang Shipyard. It may also provide reference for studying Zheng He's vessels and further make contributions to the conservation and restoration of ancient wooden ships of the Ming Dynasty.
The eruption of the Laacher See volcano ca. 13.000 years ago profoundly influenced the lifeways of Final Palaeolithic foragers inhabiting the fallout area. Apart from the substantial devastation that affected the proximal area around the eruptive centre (<50 km), substantial amounts of tephra covered the medial (50–500 km) and distal (500–1000 km) zones of the eruption. In particular, substantial amounts of volcanic ash were transported towards the northeast across Germany and into the Baltic region. In order to find new sites that would allow us to investigate the far-field effects of this cataclysmic event in detail, a predictive model using a legacy dataset of rock shelters in the Federal State of Hesse in Central Germany was developed. Hitherto, only few sites where Laacher See tephra is directly stratigraphically associated with Final Palaeolithic archaeology are known in the region. Following the in silico evaluation of the archaeological potential, two survey campaigns were conducted which resulted in the discovery of several locations that in turn will be subject to keyhole excavations in a subsequent field campaign. Environmental change is often regarded to be one of the major triggers of prehistoric culture change and technological adaptation. Particularly, long-term changes and corresponding human responses are typically at the centre of archaeological investigation. However, intense and rapid changes, such as environmental disruptions triggered by volcanic events, may also induce shifts in human lifeways (e.g. Cashman and Giordano, 2008; Oppenheimer, 2011; Riede, 2016b; Sheets, 2015). 
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. Recently, the real-world optimization problems (OPs) has got more attention in many fields includes function optimization, information science, operational research, engineering design, and their applications. In general, there are two categories of methods used to solve the OPs, the first one is traditional methods and the second is the meta-heuristic methods. The traditional methods such as gradient descent and Newton which are easy to implement, however, these methods are time-consuming, also, in each run there exists only one solution and their efficiency depending on the type of the given problem (such as constraints), variables (integer, continuous, binary), fitness function (linear, non-linear), the search space (convex, non-convex) and the number of the variables (Baykasoglu, 2012). In order to overcome these drawbacks, the meta-heuristic (MH) optimization methods, that considered as a branch of artificial intelligence, were proposed as global optimization approach (Ewees et al., 2018). The properties of MH methods are low complexities, highly robust, and high efficiency. It also overcomes the high complexity and the weakness problem in the searching process of the traditional methods (El Aziz et al., 2018a, 2017). 
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. Computational scientific discovery is a major research topic since the early days of artificial intelligence. Scientific discovery has been approached with general heuristic methods for problem solving (Langley et al., 1987) or with combinations of knowledge representation formalisms and reasoning methods (Lindsay et al., 1993). Recent technological advances in measurement, i.e., data collecting equipment, on one hand, and data storage and processing equipment on the other hand, reestablish the importance of computational scientific discovery. Kitano (2016), following the paradigm of discovery informatics (Gil et al., 2014), establishes scientific discovery as a grand challenge for artificial intelligence, calling for development of “an AI system that can make major scientific discoveries”. The contributions of the presented work are twofold. First, we overcome the combinatorial explosion issue of ProBMoT, a method for process-based modeling that employs exhaustive search through the space of candidate model structures. We map the space of candidate model structures into a fixed-length vector representation that can be exploited by an arbitrary combinatorial search algorithm. With that, we also extend the scope and the scale of applications that can be addressed by process-based modeling.
Chronic cannabis use impacts memory functioning, even while users are not acutely intoxicated. The impact of cannabis use on Wada or intracarotid amobarbital testing (IAT) has not previously been described. We reviewed cannabis consumption in epilepsy patients undergoing IAT during pre-surgical work-up. Of 58 patients reviewed, 16 patients (28%) indicated regular use. During IAT, five regular cannabis users with suspected temporal lobe epilepsy exhibited poor memory while testing their presumptively healthy temporal lobe (i.e., the side opposite that targeted for epilepsy surgery), indicating the potential for an amnestic syndrome post-operatively. It was suspected that the pattern of IAT results for these patients was attributable to the deleterious impact of cannabis use on cognition. Thus, three of the five underwent repeat IAT after a period of enforced abstinence. On repeat IAT, each of the three patients exhibited improved memory performance while testing their healthy temporal lobe, suggesting that the healthy temporal lobe of each mediated sufficient memory ability to allow for epilepsy surgery. These findings raised concerns that frequent cannabis use may alter IAT results, leading to incorrect assessments regarding potential post-operative cognitive deficits, and led to a mandate at our institution that patients must stop cannabis use before IAT. The Wada test or intracarotid amobarbital test (IAT) is used for language lateralization and to assess memory function in patients with drug-resistant epilepsy who are considering anterior temporal lobectomy with hippocampal resection as treatment [1]. During the test, amobarbital is injected into the internal carotid artery and inactivates brain function in the dependent vascular territory, typically including hippocampal function. It is intended to mimic the effect of removing the epileptogenic temporal lobe and its medial structures, and assesses whether the remaining, contralateral temporal lobe can provide sufficient memory function to compensate for the loss of the ipsilateral hippocampus. If a patient exhibits poor memory performance while testing the presumptively healthy side, an anterior temporal resection with removal of mesial structures cannot be recommended as the results indicate that the patient may have inadequate memory function post-operatively. Conceivably, systemic factors that impair memory, such as medications or drugs like cannabis, can interfere with IAT performance and cause misleading results. Based on our limited experience, we recommend inquiring about cannabis use in all patients scheduled for IAT and to consider delaying the procedure until they demonstrate abstenance from using cannabis products containing THC. This process may spare the patient an invalid test and risks associated with repeat angiography. We have introduced a protocol at our own facility in which patients are screened for use of cannabis, and are informed that they must stop consuming cannabis products for four weeks prior to IAT. On the day of the procedure, the patients are again asked whether they have consumed cannabis and, if they confirm recent use (e.g., within the week preceding IAT), the procedure is canceled and rescheduled. SECTION Funding
Suppose you have an inexpensive plant (process) that you have many copies of, and where each requires feedback control to achieve good closed-loop performance. There is then a feedback controller design challenge for each plant. The traditional approach to such a design is to use linear or nonlinear methods for one copy of the plant and implement the designed controller on each copy of the plant. The hope is that the original design is robust so it performs well on all copies. Here, we assume that the plants can be connected on the internet of things and introduce a novel strategy to design, in a distributed fashion, controllers for each plant where the design of the controller for each plant is informed by the success of the designs of all other controllers. We show that a type of robust controller can be designed over the internet of things. Our methods hold promise in practical commercial applications. The “internet of things” (IoT) is the interaction between devices to sense the environment, interpret the information, and react to real-world events autonomously with or without human intervention over a network (Vermesan and Friess, 2011). It promotes the idea that an increasing number of devices are being connected to the internet which allows more efficient monitoring and control of these devices (Anzelmo et al., 2011). Furthermore, smart systems, like the smart lights studied here, are some of the building blocks for the IoT (Kortuem and Kawsar, 2010). With mobile TCP/IP communication in hand, mobile devices such as those in control systems of automobiles, aircraft, and trains can be connected to the IoT to improve various aspects of travel (Ernst and Uehara, 2002). Automobiles can be connected over the IoT to receive updates to controllers such as ones for climate control or cruise control for optimal performance over all connected vehicles. Also, home appliances, such as refrigerators, ovens, and washing machines, will be able to communicate with each other to reach optimum performance in each home connected to the IoT. 
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. Trajectory optimization for aerial vehicle has been a research focus for many years because it is important to improve the autonomous flight capability. Among so many types of aircraft and tasks, trajectory optimization for unmanned aerial vehicle (UAV) and hypersonic vehicle are paid special attentions. For example, when UAVs are equipped in military, they can execute attack and reconnaissance tasks (Ropero et al., 2019). In civil use, UAVs are often assigned to search and rescue tasks (Juan et al., 2018). As for the hypersonic vehicles, they can make precise strike towards target instantly (Sushnigdha and Joshi, 2018). In each of the above situations, an optimal trajectory is expected to realize a safe and efficient flight. It is well known that a complete flight is composed of several phases, i.e., take-off, climb, cruise, descent and landing, and existing literatures mainly concentrate on generating the trajectory for one single flight phase (especially the cruise phase). The trajectory in other phases is often fixed in advance, which is far from satisfaction in a real world scenario. 
Congenital TORCH infections are a significant cause of epileptic spasms, an infantile epileptic encephalopathy, through disruptions to several pathways in neurodevelopment. Congenital Zika virus has a similar neurotropism to other TORCH agents, and leads to microcephaly, severe neurodevelopmental impairment, and high rates of early onset seizures. Here we report a child with confirmed congenital Zika virus who developed extensor epileptic spasms and hypsarrhythmia associated with a loss of early developmental milestones. Early treatment led to resolution of epileptic spasms and improved developmental trajectory, though the child continues to have ongoing focal seizures and prominent developmental impairment. Congenital Zika virus infection requires close monitoring as early identification of epileptic spasms is likely important in long term developmental outcome. Congenital Zika virus infection is responsible for an epidemic of children with microcephaly and severe neurodevelopmental impairment [1]. Mouse models show that the Zika virus exerts a tropism for neural progenitor cells, and acts through direct neurotoxicity and by disrupting the differentiation, maturation, and migration of developing neurons [2]. This leads to a wide spectrum of abnormal brain development that can involve severe microcephaly, abnormal gyral patterning, widespread abnormalities of cortical organization, and more rarely isolated focal dysplasias. The sequelae of congenital Zika virus infection are broad, and include diffuse hypertonia and hyperreflexia, irritability, and epilepsy [3]. The rates of seizures in congenital Zika virus infection range from 9 to 54%, with seizure subtypes including focal and generalized seizures and more recently descriptions of epileptic spasms in as many as 20% of children [4]. The classification of epilepsy and EEG findings in congenital Zika infection has been limited, though a case series shows interictal abnormalities and abnormal backgrounds occur in more than half of children, while 29% meet criteria for hypsarrhythmia variant [5]. Here we present a child with confirmed congenital Zika infection who develops epileptic spasms to better illustrate the range of cerebral pathology seen in this disease and how that correlates with epilepsy and electrographic findings. SECTION Case report We report this case to expand the etiologic basis for epileptic spasms to include another neurotropic virus, to demonstrate the range of pathology seen in congenital Zika virus infection, and to show that appropriate antiepileptic treatment can resolve background epileptic encephalopathy even in severe cases. This case supports more rigorous monitoring for epileptic spasms in children with congenital Zika virus infection, and illustrates the importance of screening for infectious etiologies in ES.
One persistent archaeological challenge is the generation of systematic documentation for the extant archaeological record at the scale of landscapes. Often our information for landscapes is the result of haphazard and patchy surveys that stem from opportunistic and historic efforts. Consequently, overall knowledge of some regions is the product of ad hoc survey area delineation, degree of accessibility, effective ground visibility, and the fraction of areas that have survived destruction from development. These factors subsequently contribute unknown biases to our understanding of chronology, settlements patterns, interaction, and exchange. Aerial remote sensing offers one potential solution for improving our knowledge of landscapes. With sensors that include LiDAR, remote sensing can identify archaeological features that are otherwise obscured by vegetation. Object-based image analyses (OBIA) of remote sensing data hold particular promise to facilitate regional analyses thorough the automation of archaeological feature recognition. Here, we explore four OBIA algorithms for artificial mound feature detection using LiDAR from Beaufort County, South Carolina: multiresolution segmentation, inverse depression analysis, template matching, and a newly designed algorithm that combines elements of segmentation and template matching. While no single algorithm proved to be consistently superior to the others, a combination of methods is shown to be the most effective for detecting archaeological features. At the time of European arrival into Eastern North America, the archaeological record included thousands of intact earth and shell mound structures (Anderson, 2012; Howe, 2014; Thomas, 1894). Beginning in the 19th century, these deposits became the focus of archaeological research due to their ability to produce artifacts that shed light on cultural affinity and chronology (Lyman et al., 1997; e.g., Claflin, 1931; Fairbanks, 1942; Ford and Willey, 1941; Jones et al., 1933; Moore, 1894a, 1894b, 1899; Moorehead, 1891; Putnam, 1875; Squire and Davis, 1848; Swallow, 1858; Wauchope, 1948; Willey, 1939). Over time, archaeological interest in mounds has grown to include studies of pre-contact technology, diet, social behavior, trade, exchange, interaction, and settlement (e.g., Anderson, 2004; Caldwell, 1952; Calmes, 1967; Claassen, 1986, 1991, 2010; Crusoe and DePratter, 1976; Marquardt, 2010; Matteson, 1960; Russo, 2004, 2006; Thompson et al., 2011; Trinkley, 1985). 
Newly emerging location-based social network (LBSN) services provide us with new platforms to share interests and individual experience based on their activity history. The problems of data sparsity and user distrust in LBSNs create a severe challenge for traditional recommender systems. Moreover, users’ behaviors in LBSNs show an obvious spatio-temporal pattern. Valuable extra information from microblog-based social networks (MBSNs) can be utilized to improve the effectiveness of POI suggestion. In this study, we propose a latent probabilistic generative model called MTAS, which can accurately capture the underlying information in users’ words extracted from both LBSNs and MBSNs by taking into consideration the decision probability, a latent variable indicating a user’s tendency to publish a review in LBSNs or MBSNs. Then, the parameters of the MTAS model can be inferred by the Gibbs sampling method in an effective manner. Based on MTAS, we design an effective framework to fulfill the top-k suggestion. Extensive experiments on two real geo-social networks show that MTAS achieves better performance than existing state-of-the-art methods. Recent years have witnessed a blooming of Web 2.0, positioning systems and wireless communication technologies. Location based social networks (LBSNs), such as Foursquare2  and Yelp3  have become a popular application and facilitated users’ daily life. People can expand circles of friends, share their real experience and post reviews, photos and videos. As a significant tool of LBSNs, suggestion methods learn users’ preferences from their historical records and meta-data such as social relationship and review content, then suggest underlying points-of-interest (POIs) to the specific users. POI suggestion can benefit advertising agencies with an effective way of launching advertisements to the potential consumers, and improve user viscosity to LBSN service providers as well (Zhao et al., 2016a). 
This study focuses on the analysis of a group of decapitated crania, which date to the Epiclassic period (ca. 600–900 CE). The crania were excavated from a small ritual site in the former lake bed of Lake Xaltocan, in the Basin of Mexico. Using non-metric cranial attributes, we undertook an intergroup analysis using an MMD biodistance. To place the Lake Xaltocan remains in a regional and historical biological context, we examined and recorded 13 non-metric, cranial features in 276 skulls from sites throughout central Mexico, including Tlatilco (N1 = 78), Teotihuacan (N2 = 66), Valle de Toluca (N3 = 33), and Xaltocan (N4 = 118). The results indicate that the most closely associated groups are Xaltocan and Valle de Toluca. Nevertheless, biodistance data indicate that all the groups significantly differ from one another, with a level of confidence of α = 6.6 × 10−6. This finding points to the existence of biological discontinuity among all the samples. Our results fall in line with existing proposals asserting that, after the fall of Teotihuacan, an intense population mobility occurred in central Mexico. In the Basin of Mexico, the Epiclassic period (ca. 600–900 CE) was marked by social, political, economic, and cultural change. Archaeological data suggest that most of the regional population during this time was organized around small political centers, which were separated by areas of low population and small hamlets (e.g. Parsons, 1989, 2008). Key population concentrations may have been tied to particular political and economic spheres around (1) Teotihuacan, (2) Cuauhtitlan-Atzcapotzalco, (3) Portezuelo–Cerro de la Estrella–Xico, (4) Cerro de la Mesa Ahumada, and (5) Tula, to the north (Parsons, 1989; García-Chávez, 2004; García-Chávez and Martínez-Yrízar, 2006; Crider et al., 2007). Several scholars have speculated that the political fragmentation in the region led to competition and, perhaps, to conflict (e.g. Kelly, 1978; Diehl and Berlo, 1989). It is likely that these conditions shaped the regional population dynamics (Cowgill, 2013; Beekman, 2015; Beekman and Christensen, 2003, 2011). In our sample, we did not identify any biological ties of the sacrificed victims in Xaltocan to other Mesoamerican populations. They do not seem to have shared any links to earlier populations at Teotihuacan, supporting the notion that the demographic environment was dynamic after the collapse of the larger state. However, the Xaltocan samples also differ from earlier (Tlatilco) and contemporaneous (Valle de Toluca) populations, though they show more affinity to the latter. It is possible that the variation of NMT among the samples we studied reflects the morphological variability that should be expected in human populations. Nevertheless, ethnohistorical and linguistic data, point to a considerable mobility of various ethnic groups in this region such as the Nahuas and the Otopames (see Brumfiel, 2003; Lastra, 2006), which foreseeably could have characterized earlier periods (Beekman and Christensen, 2003). In this regard, the differences we documented shed light on and support the notion that incredible demographic variability occurred in central Mexico over time. Determining the identity of the sacrificed victims will require additional biological data, as well as the integration of other archaeological data (e.g., chemical sourcing of pottery). Nevertheless, these first results establish an important baseline for future research.
Instrumental neutron activation analysis (INAA) of the chemical composition of 93 Preclassic fired-clay figurine fragments from two southern lowland Maya sites, Nixtun-Ch'ich' and Ixlú, was the basis for investigating their circulation. Most artifacts fell into five compositional groups while eight were unassigned. All artifacts but one are thought to have been produced locally. Only five analysed fragments suggest circulation or exchange, including three between Nixtun-Ch'ich' and Ixlú and two from different loci within Nixtun-Ch'ich'. These findings provide little support for the proposition that early Maya figurines were intentionally broken and the fragments exchanged in some sort of ‘enchainment’ activity, as suggested in other parts of the world. Like pottery vessels, fired-clay anthropomorphic figurines are typically recovered in fragments. But figurine breakage has attracted considerable attention from archaeologists, perhaps because their human form prompts unspoken analogies to the violence of dismemberment and decapitation, and interest in the final location of the body parts. It has been proposed that these objects were intentionally broken and the fragments exchanged to cement local and regional relationships (Chapman, 2000; Talalay, 1987). Here we examine the applicability of this proposition to the Preclassic lowland Maya, using instrumental neutron activation analysis (INAA) to characterise the chemical composition of 93 anthropomorphic and zoomorphic figurine fragments excavated from two sites. SECTION Archaeological and geological background 
We report a case of impairment of consciousness (IOC) induced by electrical cortical stimulation (ECS) of homologous regions within the lateral frontal convexities in a patient with medically intractable epilepsy. The patient had mixed features of idiopathic generalized and focal epilepsy. On intracranial EEG recording, interictal and ictal discharges showed a high degree of synchrony across widespread bilateral fronto-parietal areas. We identified regions in the lateral frontal lobes that reliably and produced loss of consciousness by ECS. This was accompanied by evoked EEG activity of admixed frequencies over the fronto-parietal, mesial frontal and temporal regions during stimulation and was not associated with after-discharges. Symptoms were immediately reversible upon cessation of stimulation. This finding suggests that focal cortical stimulation can disrupt widespread networks that underlie consciousness. Individuals with high degrees of speculated thalamo-frontal cortical connectivity might be more susceptible to this effect, and the findings highlight the importance of standardizing the testing of level of consciousness during mapping sessions. Although consciousness is commonly impaired in epileptic seizures, limited literature is available on loss of consciousness induced by electrical cortical stimulation (ECS) in humans undergoing intracranial EEG evaluations for localization of epileptic focus. One theory advocates the presence of consciousness ‘switch’ in subcortical structures. While this model is novel and simplistic, it has its inherent limitations. In this case study, we propose an alternative approach on the entity and discuss the complex circuits underlying it and correlate that with the electrophysiological findings and the pathophysiology of the phenotype of the disease and discuss potential causes for rarity of reports on the subject. This is a case of a patient with drug-resistant epilepsy with mixed features between generalized and focal epilepsy. This was supported by clinical history and suspected to have a focal seizure onset based on the results of a non-invasive pre-surgical evaluation. However there was bihemispheric widespread electrical changes at seizure onset by intracranial EEG (icEEG). During electrical cortical stimulation (ECS) for mapping function, the patient experienced reproducible impairment of consciousness (IOC) within 2 seconds of electrical stimulation over regions in the bilateral frontal convexities. There was immediate recovery of consciousness and amnesia for the events during stimulation following the end of the electrical train. The IOC correlated with emergence of evoked admixed frequencies of EEG activity over areas in the fronto-parietal cortices and in the mesial frontal and temporal regions. We discuss the pathophysiology and the pathways that possibly underlie this phenomenon and rhythms, and we discuss implications and future directions. SECTION Methods This report highlights the importance of testing for impairment of consciousness during sessions of electrical cortical stimulation. It also underlines the possibility that localized stimulation in some cases may produce widespread changes in brain physiology and in consciousness. Future prospective studies may elucidate the occurrence and prognostic value of impairment of consciousness during electrical cortical stimulation for mapping function. SECTION Conflict of interest
The present study explores four different regression formulae for the histological assessment of age at death in ribs and their performance in a prehispanic Classic Maya population sample consisting of 57 individuals from Xcambó, Yucatan, Mexico. Regressions employed include the original age regression formula by Stout and Paine (1992), Cho et al.'s (2002) two formulae for samples of indeterminate ethnicity, as well as a formula published by Valencia et al. (2010) adjusted specifically for populations of Maya descent. In addition to applying these methods, we report histomorphometric variables (total cortical area (TA), cortical area (CA), relative cortical area (CA/TA%), osteon population density (OPD), and osteon cross-sectional area (On.Ar.)) from the 6th rib and compare these variables across groups within our sample defined by macroscopic age and sex, as well as with results reported for modern reference samples used by Cho et al. and Valencia and colleagues. Our study shows mean CA and CA/TA% are relatively high across all ages in the prehispanic Maya sample, especially in females, indicating a high degree of robusticity. OPD is high when compared with samples used by Cho et al., but similar to the modern Maya reference sample. Comparison of histological age at death estimates reveals interesting patterns of deviation; specifically Cho et al.'s formulae both deviate strongly from all other age estimates. Calculated mean net difference between Cho et al.'s and macroscopic age estimates, for example, is nearly 16 years. Both, Stout and Paine, and Valencia et al., result in age reconstructions more similar to macroscopic estimates (mean net difference around 8 years). Since Cho et al.'s formulae are unique in employing On.Ar., CA, and TA, in addition to OPD, OPD-based regression formulae may perform better in archaeological samples. However, some of the deviation observed could result from differences in histomorphometric variables between modern reference and archaeological samples, the outcome of complex biocultural processes. Continued analyses of histomorphological variation between differing reference and archaeological samples will be necessary to improve histological assessments of age at death in archaeological contexts. The estimation of age at death in human remains is one of the most fundamental steps undertaken in archaeological and forensic casework. In archaeological contexts, macroscopic methods are often preferred over microscopic ones, for many reasons. First, invasive techniques are often avoided or not permitted due to ethnical concerns, and/or to preserve sample integrity. Second, labs are often not equipped with microscopes and stations for the preparation and analysis of thin sections. Third, histological assessment takes more time, and requires additional experience. Fourth, histological approaches often focus on skeletal regions indicating pathological changes (Schultz, 2001), while standardized healthy bone sections are necessary for age estimations. Finally, and most importantly within the context of the present study, current literature focusses on the refinement of regression formulae, with little published about their applicability in archaeological contexts (see Pfeiffer and Pinto, 2012). 
Finding a person across a camera network plays an important role in video surveillance. For a real-world person re-identification application, in order to guarantee an optimal time response, it is crucial to find the balance between accuracy and speed. We analyse this trade-off, comparing a classical method, that comprises hand-crafted feature description and metric learning, in particular, LOMO and XQDA, to deep learning based techniques, using image classification networks, ResNet and MobileNets. Additionally, we propose and analyse network distillation as a learning strategy to reduce the computational cost of the deep learning approach at test time. We evaluate both methods on the Market-1501 and DukeMTMC-reID large-scale datasets, showing that distillation helps reducing the computational cost at inference time while even increasing the accuracy performance. Person re-identification refers to the problem of identifying a person of interest across a camera network (Zheng et al., 2017a; Panda et al., 2017). This task is specially important in surveillance applications, since nowadays the security systems in public areas such as airports, train stations or crowded city areas, are continuously improving to ensure the population’s welfare. In big cities, there are extensive networks of cameras in the most sensitive locations. Identifying an individual requires finding it among all the instances that are present on the collection of images captured by the cameras. These images show usually complex crowded scenes, thus increasing even more the computational complexity of the problem. Therefore, the automation of this task that involves large-scale data becomes essential, as otherwise it would be a laborious task to be performed by humans. 
Minimum Time Search (MTS) algorithms help in search missions proposing search trajectories that minimize the target detection time considering the available information about the search scenario. This work proposes a MTS planner based on ant colony optimization that includes communication and collision avoidance constraints. This ensures that the Unmanned Aerial Vehicles (UAVs) are able to complete the optimized search trajectories without risk of collision or loss of communication with the ground control station. This approach is a great advantage nowadays, where UAVs flight regulation is quite strict, often requiring to monitor the state of the UAVs during the whole mission, impeding UAV deployments without continuous communication to the ground control station. The proposed algorithm is tested with several search scenarios and compared against two state of the art techniques based on Cross Entropy Optimization and Genetic Algorithms, which have been adapted to make them consider collision and communication constraints as well. Nowadays, there is a strong research interest in UAVs mission planning due to their advantages in several applications, such as mission planning (Atencia et al., 2018), target tracking (Pulford, 2005), target monitoring (de Moraes and de Freitas, 2019; Khan et al., 2017) or fire fighting (Bilbao et al., 2015). This work focuses on search under uncertainty or Probabilistic Search (PS), more concretely on Minimum Time Search (MTS) algorithms which propose search trajectories that minimize the target detection time. MTS has several applications that vary from searching for survivors after natural disasters (e.g. a fire or an earthquake) to look for military targets. An example is shown in Fig. 1, where several UAVs search for a life boat while the mission is being monitored from the base station located on a ship. MTS algorithms exploit the available information about the target location, which is typically modeled with a probability map that states the prior target presence probability distribution. Fig. 1 depicts a search mission where the prior knowledge of the target (life boat) position is represented by a discretized probability map. In this map, cells with warmer colors indicates higher probability of target presence. In order to reduce the target detection time, cells with higher probability of target presence should be explored as soon as possible by the UAVs. This work proposes a MMAS-based algorithm that minimizes the detection time of a target with unknown location while maintaining a multi-hop connection to the GCS and avoiding collisions between UAVs. To handle the infeasible solutions, communication maintenance and collision avoidance penalty functions are defined and added to the fitness criterion (expected target detection time) following a penalty based method. Besides, the proposed algorithm takes advantage of the possibility offered by the ant colony algorithms of including heuristic information and uses a new heuristic that combines MTS with safety (communication and collision avoidance) information, allowing the algorithm to obtain more appropriate feasible solutions from the first iterations. The performance of the algorithm is analyzed in detail and compared with other MTS algorithms over several prefixed scenarios and over several random scenarios.
The quality of meat products is traditionally assessed by chemical or sensorial analysis, which are time consuming, need specialized technicians and destroy the products. The development of new technologies to monitor meat pieces using non-destructive methods in order to establish their quality is earning importance in the last years. An increasing number of studies have been carried out on meat pieces combining Magnetic Resonance Imaging (MRI), texture descriptors and regression techniques to predict several physico-chemical or sensorial attributes of the meat, mainly different types of pig ham and loins. In spite of the importance of the problem, the conclusions of these works are still preliminary because they only use the most classical texture descriptors and regressors instead of stronger methods, and because the methodology used to measure the performance is optimistic. In this work, we test a wide range of texture analysis techniques and regression methods using a realistic methodology to predict several physico-chemical and sensorial attributes of different meat pieces of Iberian pigs. The texture descriptors include statistical techniques, like Haralick descriptors, local binary patterns, fractal features and frequential descriptors, like Gabor or wavelet features. The regression techniques include linear regressors, neural networks, deep learning, support vector machines, regression trees, ensembles, boosting machines and random forests, among others. We developed experiments using 15 texture feature vectors, 28 regressors over 4 datasets of Iberian pig meat pieces to predict 39 physico-chemical and sensorial attributes, summarizing16,380 experiments. There is not any combination of texture vector and regressor which provides the best result for all attributes tested. Nevertheless, all these experiments provided the following conclusions: (1) the regressor performance, measured using the squared correlation (R2), is from good to excellent (above 0.5625) for 29 out of 39 attributes tested; (2) the WAPE (Weighted Absolute Percent Error) is lower than 2% for 32 out of 37 attributes; (3) the dispersion in computer predictions around the true attributes is lower or similar than the dispersion in the labeling expert’s for the majority of attributes (85%); and (4) differences between predicted and true values are not statistically significant for 29 out of 37 attributes using the Wilcoxon ranksum statistical test. We can conclude that these results provide a high reliability for an automatic system to predict the quality of meat pieces, which may operate on-line in the meat industries in the future. Hams and loins from Iberian pig, which is an autochthonous porcine breed developed traditionally in the SouthWest of Spain, are one of the most valuable meat products in this country. This is mainly ascribed to their exceptional sensorial attributes that depend on characteristics of raw material and processing conditions. Thus, not only characteristics of fresh pieces but also their modifications during the processing are important parameters to control the technological process of dry-cured hams and loins (Pérez-Palacios et al., 2011b). Temperature and relative humidity conditions during the processing lead to dehydration and, hence, to weight loss and a water activity decrease. Meat from Iberian pigs should contain plenty of intramuscular fat, which is an important characteristic, due to its positive influence on quality parameters on the final product, such as marbling, juiciness, odor, and aroma (Ruiz et al., 2002). The determination of salt content is important from a microbiological point of view, but it also influences on the texture and flavor of the final product (Toldrá et al., 1997). Color is also one of the most interesting characteristics of meat products (Resurrección, 2003), and for dry cured meat products it is the most relevant appearance property (Gandemer, 2002). It is also important to study the final sensory quality of Iberian meat products, considering their most distinguished sensorial attributes, such as appearance, odor, taste and flavor (García and Carrapiso, 2001). Scientific studies on these meat products have carried out the sensory analysis objectively, with trained panellists and following standardized conditions. 
The transition from inhumation to cremation is a well-documented phenomenon in Bronze Age Central Europe. However, almost nothing is known about similar transitions taking place in other mortuary practices, such as secondary burials. This study brings new insights into diachronic trends in secondary burials during the Central European Bronze and Iron Age. Diachronic trends in secondary burials are defined here by different kinds of excarnation. The type of excarnation was observed in 23 secondary burials dating to the Early Bronze Age and the turn of the Late Bronze to Early Iron Age at five sites in Moravia (Czech Republic). Osteological and taphonomic assessment of unburned human bones recovered from settlement contexts indicates a changing pattern of secondary burial practice over time. Early Bronze Age human remains bear traces of both passive excarnation by natural agents, such as exposure to carnivores, and excarnation by primary burial. By the Late Bronze Age and Early Iron Age secondary burials show evidence of excarnation with tools. This modification of secondary burial practices, may be connected with a contemporaneous change of primary burial practices from inhumation to cremation. Mortuary practices in Central Europe changed radically during the Bronze Age (Brandt et al., 2014; Harding and Fokkens, 2013; Harding, 2000; Sørensen and Rebay, 2008). By the Early Bronze Age (EBA; 2200–1500 BCE) the dominant ritual is characterized by relatively uniform primary inhumations of complete bodies. During the Middle Bronze Age (1500–1300 BCE) cremation becomes more prevalent (Stuchlík, 1993). This change spread across Continental Europe with the Urnfield culture in Late Bronze and Early Iron Age from 1300 to 700 BCE. The break between the Late Bronze and Early Iron Age (LBA/EIA) in Central Europe corresponds with two phases of the Lusatian culture. During the transition from inhumation to cremation, secondary burials gradually increase with their methods and characteristics changing over time (Rulf, 1996; Stuchlík, 2010). 
Autonomy on rovers is desirable in order to extend the traversed distance, and therefore, maximize the number of places visited during the mission. It depends heavily on the information that is available for the traversed surface on other planet. This information may come from the vehicle’s sensors as well as from orbital images. Besides, future exploration missions may consider the use of reconfigurable rovers, which are able to execute multiple locomotion modes to better adapt to different terrains. With these considerations, a path planning algorithm based on the Fast Marching Method is proposed. Environment information coming from different sources is used on a grid formed by two layers. First, the Global Layer with a low resolution, but high extension is used to compute the overall path connecting the rover and the desired goal, using a cost function that takes advantage of the rover locomotion modes. Second, the Local Layer with higher resolution but limited distance is used where the path is dynamically repaired because of obstacle presence. Finally, we show simulation and field test results based on several reconfigurable and non-reconfigurable rover prototypes and a experimental terrain. Autonomy is an essential capability for rovers to explore the surface of other planets. The distances from Earth entail big latencies in communications between the rover and the terrestrial ground station. As an example, there is a radio transmission delay of several minutes between Earth and Mars (Lester and Thronson, 2011; Lester et al., 2017). Therefore, direct teleoperation arises as a difficult task to be carried out remotely from Earth. Besides, communications with rovers at the red planet generally occur only a few times per Martian sol (solar day) due to the availability of Deep Space Network antennas, conforming a limited time-slot for providing commands and retrieving data from the rover (Bajracharya et al., 2008). These facts are contrary to the necessity of increasing the number of scientifically interesting places visited by rovers. Providing higher autonomy would allow them to traverse longer distances. However, new issues arise since rovers tackle a high uncertainty when they are traveling, i.e., they may encounter unexpected situations, mostly due to terrain shape and/or composition, as well as the existence of stones. These issues affect the traversability for the vehicle. The improper evaluation of the terrain could lead to a fatal situation of the vehicle, compromising the mission as a result. This was the case of the Spirit rover, which got stuck in loose sand, making it impossible to continue driving (Ono et al., 2015) and thus bringing the mission to an end. By using traversability information, autonomy can be improved thanks to the use of path planning algorithms, which allow the vehicle to compute onboard a safe path from one location to another. In this paper a novel path planning has been presented. It works at two scales, global and local, using the so called Multi-layered Grid. The grid is composed of two layers, each of them for different planning purposes. The path connecting the rover initial and goal locations is computed on the Global Layer, according to the information related to the terrain and the rover locomotion modes. Then, the generated global path plan is dynamically repaired using the Local Layer whenever an obstacle is detected by the rover during its traverse.
Several micro-archaeological methods are suggested in this study in order to identify cess deposits. These methods were deployed at a Near Eastern mound (Megiddo, Israel), yet are applicable to any archaeological site anywhere in the world. The study presented here, was performed on a 2–3 mm thick yellowish fibrous material, ca. 40 × 15 cm in size, which was discovered in Area H at Tel Megiddo in relation to a well-built structure dating to the Late Bronze Age IIA (mid-14th century BCE). Area H is located near the remains of a large Late Bronze Age palace, which had been excavated in the early 20th century. In order to reveal the nature of the yellowish fibrous material we carried out infrared spectroscopy, petrographic microscopy and lipid analyses. The results led us to suggest that this substance is related to fecal matter. We therefore analyzed it for pollen and gastrointestinal parasite remains. While the latter were for the most part absent, the palynological investigation provided information about dietary components that are usually under-represented in the reconstruction of vegetative diets, especially beverages and possible use of medicinal plants, consumed by the Megiddo residents, who may had some link to the palace. The paper demonstrates how diverse micro-archaeological analyses complement each other, and when applied in concert yield novel information about the past. On the final day of excavations of the 2012 season at Tel Megiddo (northern Israel, Fig. 1) a yellowish fibrous feature was detected. This 2–3 mm thick feature, ca. 40 × 15 cm in size (Fig. 2), was associated with a well-built structure in Level H-14, which dates, based on ceramic finds, to the Late Bronze Age IIA in the 14th century BCE (for radiocarbon dating, see Boaretto, forthcoming). Area H is located in the northwestern sector of the site (Fig. 3), near the Late Bronze Age palace unearthed in the course of the University of Chicago excavations in the 1930s (Area AA, Fig. 4; Loud, 1948), and in proximity to the gate of the city (Fig. 3). Level H-14 is contemporaneous with the Amarna archives – the diplomatic correspondence of Pharaohs Amenophis III and Amenophis IV (Akhenaten) with Near Eastern kingdoms and Canaanite petty-kings, written in Akkadian cuneiform and found in Egypt in the late 19th century CE. Among the ca. 370 tablets, there are six that were sent by Biridiya, the ruler of Megiddo, presumably one of the inhabitants of the Megiddo palace. 
Fermented and alcoholic beverages played a pivotal role in feastings and social events in past agricultural and urban societies across the globe, but the origins of the sophisticated relevant technologies remain elusive. It has long been speculated that the thirst for beer may have been the stimulus behind cereal domestication, which led to a major social-technological change in human history; but this hypothesis has been highly controversial. We report here of the earliest archaeological evidence for cereal-based beer brewing by a semi-sedentary, foraging people. The current project incorporates experimental study, contextual examination, and use-wear and residue analyses of three stone mortars from a Natufian burial site at Raqefet Cave, Israel (13,700–11,700 cal. BP). The results of the analyses indicate that the Natufians exploited at least seven plant taxa, including wheat or barley, oat, legumes and bast fibers (including flax). They packed plant-foods, including malted wheat/barley, in fiber-made containers and stored them in boulder mortars. They used bedrock mortars for pounding and cooking plant-foods, including brewing wheat/barley-based beer likely served in ritual feasts ca. 13,000 years ago. These innovations predated the appearance of domesticated cereals by several millennia in the Near East. The consumption of fermented and alcoholic beverages is one of the most prevalent human behaviors, but the time and cultural context of its origins remain unclear. Archaeological evidence for alcohol production and use is usually associated with fermenting domesticated species in agricultural societies, such as ancient Egypt, Mesopotamia, China, and South America (Goldstein, 2001; Jennings, et al., 2005; Katz and Voigt, 1986; McGovern, et al., 2004; Samuel, 1996; Wang, et al., 2016). It has long been speculated that humans' thirst for beer may have been the stimulus behind cereal domestication (Braidwood et al., 1953), and some scholars have attributed this invention to the Natufians in the Near East (ca. 15,000–11,500 Cal BP) (see Hayden, et al., 2013). The Natufians were innovative in many material and social realms, and paved the way to the establishment of the first sedentary Neolithic villages at about 11,500 cal BP (Bar-Yosef, 1998). To test the so far unsubstantiated “Natufian beer hypothesis” we examined three stone mortars from the first chamber at Raqefet Cave (13,700–11,700 cal. BP), a site with a long archaeological sequence (Lengyel, 2007), that also served as a Natufian burial place in Mt. Carmel, Israel (Fig. 1A; Fig. S1A) (Nadel, et al., 2013). 
In this article, the results of the archaeological excavations at the Late Mesolithic site of Monte do Carrascal 2 are presented. The site, located inland, southeast from the contemporary Sado shell middens, comprised two hearths with faunal remains and a set of lithic materials that were analysed techno-typologically, as well as in terms of their spatial distribution through GIS tools (K Ripley Function, Kernel Density Estimation and Nearest Neighbour analysis). The study points to a different functionality of this site when compared to most all other Mesolithic sites known in the region, with its uncommon inland location, suggesting that it was possibly a hunting camp. The Late Mesolithic settlement (ca. 6200–5600 cal BC) of western Iberia is best known through the shell middens complexes located in the Tagus (Roche, 1989; Bicho et al., 2017, 2013, 2010; Gonçalves, 2013; Gonçalves et al., 2017) and Sado rivers basins (Arias et al., 2015; Diniz and Arias, 2012) and in the coastal areas surrounding the Cape Saint Vincent (Carvalho and Valente, 2005), as well as some sites located in the Mira river valley and southwest coast (Arnaud, 1994; Lubell et al., 2007). The occupation of these regions during Late Mesolithic, with particular relevance to Sado and Muge valleys, is thought to be related to the coastal and estuarine transformations (Zilhão, 2003; Bicho et al., 2010) resulting from the 8.2 ka cold event (Alley and Agustsdottir, 2005; Alley et al., 1993). Until very recently there was a total lack of knowledge regarding Late Mesolithic sites in this interior region of Alentejo. The increasing number of impact mitigation archaeological excavations in the area has helped to change this general picture, revealing important sites as the ones mentioned above (Carrascal 2, Barranco Horta do Almada 1, Defesa de Cima 2 and Xarez 12) that contribute to re-shape the settlement pattern and land management strategies for Mesolithic populations in South Portugal.
We studied three patients with Unverricht–Lundborg disease for autistic features along with other clinical features associated with progressive myoclonus epilepsy. We diagnosed this disease based on noise and touch sensitive myoclonus, ataxia, cognitive decline, typical EEG features, normal MRI of the brain and applied Children's Global Assessment Scale and Childhood Autism Spectrum Test to these children. The CGAS score was 35 in two and 50 in one of them. CAST scores were above 15 in all of three of them. Autistic features may be an important clinical feature of this disease. History and physical examination for myoclonus should probably be taken in autistic children. Unverricht–Lundborg disease is an autosomal recessive progressive myoclonus epilepsy characterized by onset at the age of 6–15 years, severe incapacitating stimulus-sensitive progressive myoclonus, tonic–clonic seizures, absence seizures and characteristic abnormalities in the electroencephalogram (EEG) [1–3]. Neuropsychiatric disturbances are a recognized feature of Unverricht–Lundborg disease [4]. However, autistic features have not recived much detail in these patients. Autistic features may however be co-associated with epilepsy [5]. Hence, we studied three patients with this disease for autistic features along with other clinical features. SECTION Case series 
The territorial characteristics and environmental factors involved in the selection of a specific site for establishing a settlement are key features in the analysis of hunter-gatherers' knowledge of (and dominance over) their surroundings and in the attempts to understand the survival strategies that they deployed. This paper presents a macrospatial analysis using GIS tools, which provides an objective comparison of territorial variables at several Magdalenian archaeological sites located in the south-eastern Pyrenees (NE Iberia). To establish the settlement patterns, we analyse territorial values: orientation, elevation, slope, sites aspect (caves, rock shelters or open air) and distance from rivers. With the data obtained, we create solar radiation models, construct groups of sites according to visibility, and calculate the displacement costs of mobility. The results suggest a series of different settlement patterns during the Magdalenian. The visibility of rivers from the archaeological sites and potential sunlight are characteristic features throughout the period, but the distance between rivers and settlements decreases diachronically. Comparison of the climate models indicates that settlements in the vicinity of the river were more frequent at times with evidence of low rainfall. Likewise, the costs of displacement from the surrounding territory to the archaeological sites increase; access to the Lower Magdalenian sites is easier, and access to the Upper Magdalenian sites much more difficult. The Magdalenian is one of the most documented and well-known Palaeolithic chrono-cultural phases in western Europe, characterized by the diversity of symbolic art, fauna and lithic remains in the archaeological record (Fullola et al., 2012; Vega et al., 2013). These records have allowed the study of changes in strategies for procuring biotic and abiotic resources and in social and symbolic behaviour, and reflect the variability and development of technical systems of lithic knapping and production of bone tools (Benito-Calvo et al., 2009; Calvo et al., 2009, 2008; Esteve, 2009; Fullola, 2001; Fullola et al., 2006; García-Diez and Vaquero, 2015; Langlais, 2010; Maluquer de Motes, 1983–1984; Mangado et al., 2014, 2013, 2010, 2009; Martínez-Moreno et al., 2007; Mithen, 1988; Montes, 2005; Mora et al., 2011; Morales and Verges, 2014; Peña and Cruz, 2014; Roman, 2016; Sánchez de la Torre, 2015; Sánchez de la Torre and Mangado, 2013; Tejero, 2009; Tejero et al., 2013; Utrilla et al., 2013, 2010; Utrilla and Mazo, 2014; Utrilla and Montes, 2009). These studies have used a variety of approaches, but spatial analyses have not been applied to date in the Magdalenian sites of north-eastern Iberia. The spatial analysis of prehistoric sites provides interesting insights into the study of settlement dynamics and mobility strategies. Moreover, the conservation and excavation of the Magdalenian sites in north-eastern Iberia has allowed paleoenvironmental reconstructions of the southern Pyrenees at the end of the Last Glacial period, coinciding with the deglaciation of the Pyrenees around 20 kyr BP (Alcolea, 2017; Allué, 2009; Allué et al., 2012a, 2012b, 2018; Aura et al., 2011; Bergadà, 1991; Burjachs, 2009; Fullola et al., 2012; Fumanal and Ferrer, 2014; Soler et al., 2009). During the Magdalenian, the climatic sequence underwent a transition between the colder episode GS-2a and the GI-1 interstadial. The interstadial GI-1 began with a rapid increase in temperatures, similar to current temperatures. Favourable temperatures were interrupted by two colder oscillations, the sub-stadials GI-1d and GI-1b (sensu Walker et al., 1999). The warm pulsation after GI-1a oscillation gradually decreased until the beginning of stadial GS-1, with very cold temperatures in a large part of Europe until the beginning of the Holocene. Our results suggest that changes in settlement patterns, resource gathering and the organization of the territory were closely linked to changes in the landscape. Consequently, the availability of resources and the capacity to adopt efficient survival strategies were important factors when choosing a settlement. In the same way, knowledge of the territory and the ability to adapt to these changes would have been essential for hunter-gatherer groups in order to guarantee their survival.
Changes in the importance of agriculture in prehistoric economies are of major interest in a range of contexts worldwide. Measures of site location in relation to agricultural potential are an important tool for identifying relative shifts in the importance of agriculture over time within a given region. Here we examine the application of GIS modeling of agricultural potential based on soil characteristics, topography, and proximity to drainage in the highlands of the Jornada branch of the Mogollon culture of southcentral New Mexico to explore shifts in agricultural reliance over time. We describe the methods, potential limitations, and potential advantages of this approach, as well as preliminary results. Identifying variables that reliably predict agricultural potential is complicated by the limited resolution of available data for the large study area, overlapping legal jurisdictions (state versus federal land), complex topography, as well as by a lack of modern non-mechanized farming data to ground-truth estimates of relative productivity. Nevertheless, the approach allows comparisons for relative changes in settlement placement over time in relation to variables that are likely to impact agricultural productivity. Our results support other evidence of strong agricultural reliance in the pithouse period, substantially greater than in the Archaic; the pueblo period occupation may be slightly more tightly linked to optimal agricultural land, though the latter conclusion is uncertain. The evolution of agricultural life is associated with a group of developments sometimes referred to as “the Neolithic package” in the Old World and reflected in parallel developments in New World Formative societies. Aside from the adoption of agriculture itself, these changes typically include major technological transformations (for instance construction of substantial dwellings and storage facilities and widespread use of pottery and food grinding technologies), and changes in material manifestations of social patterns (particularly aggregation into sedentary village communities of integrated but distinct households). The interaction among these developments is of fundamental interest given their near universality and their role in creating the foundations of subsequent urbanization and complex social formations. However, despite the apparent consistency of such a “package,” the links among its constituents are variable, a factor noted even in early descriptions of Neolithic and Formative patterns (e.g., Childe, 1936; Willey and Phillips, 1958), and the concept of a rigid Neolithic package has been heavily criticized in recent decades (e.g., Jordan and Zvelebil, 2009; Thomas, 1999). 
Cova Bonica has yielded one of the few assemblages of Cardial Neolithic records of directly dated human remains (c. 5470 and 5220 years cal. BC – unmodelled) in the Iberian Peninsula and has provided the first complete genome of an Iberian farmer. A minimum of seven individuals and six age clusters have been ascribed on the basis of the disarticulated human bones. A large number of archaeological artifacts have likewise been identified in the same layer, preserved in a small number of remnants in different areas of the cave. This study presents the results of a multi-proxy archaeological analysis of the spatial distribution, human remains, small and large mammals, palaeobotanical remains, lithics, ceramics and radiocarbon dating, with the aim of reconstructing the cave's history and the context of the layer containing the human remains. The results suggest the cave was used for at least two distinct purposes: one related to its use for funerary practices, as documented by a small group of artifacts (ornamental objects, ceramics, tools), charcoal and small mammals; the other related to its use as a sheep pen as indicated by reworked fumier, the results of a zooarchaeological study and an ovicaprine palaeodemographic profile. The paper concludes that the funerary and ritualistic practices of the Cardial Neolithic in SW Europe are difficult to reconstruct because human remains are often scattered in archaeological layers where other human activities may also have been conducted. For this reason, artifacts associated with human remains do not constitute a solid foundation on which to reconstruct funerary practices. Indeed, only a multi-proxy analysis of the archaeological material is capable of evaluating different geological and/or archaeological processes and their associated activities. The transition from hunter-gatherer to farmer populations meant radical economic and social changes from the ways of nomadism to more sedentary ways of life (Guilaine, 2013). According to palaeogenetic data (Gamba et al., 2014; Olalde et al., 2015; Rivollat et al., 2015), farming appears to have been introduced into central and western Europe around 8000 years ago by new migrant populations, while the archaeological evidence suggests at least two distinct, but well-defined, migration routes: up the Danube valley and along the Mediterranean shoreline (Cruz Berrocal, 2012; Guilaine, 2013; Zilhão, 2001). The Neolithic period in the associated archaeological sites shares close links with the Linearbandkeramik (LBK) culture in central Europe and with the Impressa and Cardial traditions in southern Europe, characterized by the domestication of plants and animals, and the use of a new lithic technology and raw materials, among other features. Scholars have employed a range of approaches in their study of Neolithic phenomena in Europe, including, the determination of the rhythms of this process, radiocarbon dating, stratigraphy, and analyses of the social structure, economy, funerary practices, etc. (Bickle and Whittle, 2013; Binder, 2000; Bogaard, 2004; Cruz Berrocal, 2012; Davison et al., 2006; Gronenborn, 1999; Oross and Bánffy, 2009). 
Taphonomic modifications to Neolithic human skeletal remains from six rock-cut tombs in Malta has provided key information about funerary practices and the local environment. Application of microscopic analysis, computed tomography (CT) scanning, and 3D imaging of the modifications has allowed their comparison with similar examples in modern and archaeological skeletal material. The modifications are interpreted as pupal chambers and feeding damage by dermestid beetles. Based on observation of the behaviour and ecology of dermestid beetles, we suggest several scenarios for funerary practices at the Xemxija tombs which nuance our current understanding of collective burial during the late Neolithic in Malta. Modifications to human skeletal remains from archaeological contexts provide a wealth of information regarding individual health, cultural interactions with the remains of the dead, and the effects of the burial environment (White, 1992; Haglund, 1997a, 1997b; Andrews and Fernandez-Jalvo, 2003; Ortner, 2003; Smith, 2006; Duday, 2009; Robb et al., 2015). The latter two categories of taphonomic markers – cultural and natural modifications to human remains following death and deposition – are sometimes juxtaposed, and there is debate as to whether natural modifications can reveal cultural practices (cf. Knüsel and Robb, 2016, 656). As the emerging field of funerary archaeoentomology is beginning to show, however, insect modifications to human skeletal remains allow us not only to reconstruct palaeoenvironments, but also advance our understanding of funerary practices (Huchet and Greenberg, 2010; Huchet et al., 2013; Huchet, 2014a, 2014b; Matu et al., 2017). 
We propose SECUR-AMA, an Active Malware Analysis (AMA) framework for Android. (AMA) is a technique that aims at acquiring knowledge about target applications by executing actions on the system that trigger responses from the targets. The main strength of this approach is the capability of extracting behaviors that would otherwise remain invisible. A key difference from other analysis techniques is that the triggering actions are not selected randomly or sequentially, but following strategies that aim at maximizing the information acquired about the behavior of the target application. Specifically, we design SECUR-AMA as a framework implementing a stochastic game between two agents: an analyzer and a target application. The strategy of the analyzer consists in a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to efficiently search the state and action spaces taking into account previous interactions in order to obtain more information on the target. The target model instead is created online while playing the game, using the information acquired so far by the analyzer and using it to guide the remainder of the analysis in an iterative process. We conduct an extensive evaluation of SECUR-AMA analyzing about 1200 real Android malware divided into 24 families (classes) from a publicly available dataset, and we compare our approach with multiple state-of-the-art techniques of different types, including passive and active approaches. Results show that SECUR-AMA creates more informative models that allow to reach better classification results for most of the malware families in our dataset. In recent years the increasing reliance on computer systems and the increasing use of Internet, wireless networks, autonomous systems, e.g., cars, boats, as well as the growth of smart and tiny devices as part of the Internet of Things (IoT) resulted in a corresponding increase in the number of cyber-security flaws. In particular, Android is one of the most diffused operating systems employed in smartphones and IoT devices, making it the preferred target for cyber-criminals due to its huge market share (Cheung, 2018). Android malware are then one of the biggest threats in IT security nowadays, with millions of malicious applications released every year. Analyzing such amount of threats have become almost impossible for human security experts, and consequently, tools based on machine learning are fundamental to automate and speed up the process. In this work we aim to extend an analysis technique we proposed in Sartea and Farinelli (2017), by creating a fully fledged automated framework for Android malware analysis that substitutes the manual analysis of an unknown application, i.e., by performing automated test interactions and adapting to what is observed. Broadly, the concept of executing specific actions to perform a better analysis can be linked to the general framework of active learning, and recently there has been a specific interest in applying active learning techniques to malware analysis (Nissim et al., 2014). In that work, authors propose the use of an SVM classifier to select which samples (already analyzed) should be fed to the classifier, so to refine the classification bounds. In this work, our aim is to generate malware models that can be studied by a human security expert or processed by automated techniques (clustering, classification) for comparison. Hence, we focus on the decision making side of the analysis by devising an intelligent strategy for the analyzer action selection. For this reason, SECUR-AMA differs from active learning approaches where the methodology is usually tied to the specific choice of classifier in order to improve the classification bounds, e.g., k-NN and naïve Bayes (Wei et al., 2015), logistic regression (Guo and Schuurmans, 2007), linear regression (Yu et al., 2006), SVM (Tong and Koller, 2002). 
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. As the most fundamental forms of fuzzy reasoning, fuzzy modus ponens (FMP) and fuzzy modus tollens (FMT) can be respectively expressed as follows (Dubois and Prade, 1991; Gottwald, 2001; Wang, 2000): In this paper, we discuss the robustness of interval-valued fuzzy reasoning triple I algorithms based on the normalized Minkowski distance in Hausdorff metrics and provide a numerical example to illustrate the feasibility of the algorithms in practical application. When the parameter m of Schweizer–Sklar operators is different, we can obtain different fuzzy reasoning triple I algorithms models, which provide foundation of practical applications. By analyzing, when m is close to 1, triple I algorithms have good behavior of robustness. Moreover, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis.
In this study, bulk bone collagen carbon (δ13C) and nitrogen (δ15N) isotope data from 49 individuals, recovered from two Medieval burial grounds in Hereford, England, are coupled with incremental dentine data from five individuals with high δ15N bone values who survived into old age, to see whether the high δ15N values were consistent throughout their childhood and adolescence. There are statistically insignificant differences between mean bone δ13C and δ15N values from the two Hereford populations, exhumed at Cathedral Close and St. Guthlac's Priory, despite temporal and demographic differences (St Guthlac's mean: δ13C −19.4 ± 0.5‰ and δ15N 10.9 ± 1.2‰; Hereford Cathedral mean: δ13C −19.6 ± 0.4‰ and δ15N 10.4 ± 0.9‰, 1σ). In comparison to other contemporary urban populations, the Hereford individuals present significantly lower but more variable δ15N values, suggesting a diet low in protein from high trophic level foods such as meat and milk, possibly the result of differing social status or geographic factors. The approximately 23-year long incremental dentine profiles all show considerable fluctuation in stable isotope values during childhood and adolescence for all individuals until around age 20, suggesting possible influence by physiological processes related to growth and development. Coupled with archaeological, historical, and palaeopathological observations, stable isotope analysis is an increasingly valuable tool to examine palaeodietary, geographic, and temporal differences in past human populations (Lee-Thorp, 2008; Müldner and Richards, 2005, 2006, 2007a, 2007b). Typically, such analyses have been undertaken through sampling bone collagen, focusing on the ratios of carbon (13C/12C, δ13C) and nitrogen (15N/14N, δ15N), to estimate the proportion of plant, animal, and fish protein in a population's diet, along with possible indicators of stress or illness, based on several well-established principles (Balasse and Ambrose, 2005; Beaumont and Montgomery, 2016; Cerling, 1993; Lee-Thorp, 2008). As such, the δ15N values of sampled collagen will increase significantly when an individual's diet includes protein from fish, meat or animal secondary products (Schoeninger and DeNiro, 1984). These values may further be lowered by anabolic events, such as growth and pregnancy, and increased by catabolic events, such as malnutrition and illness (Fuller et al., 2004, 2005, 2006; Mekota et al., 2006; Waters-Rist and Katzenberg, 2010). Typically, δ13C values will predominantly reflect the photosynthetic pathway (C3 or C4) of plants at the bottom of the food chain (Farquhar et al., 1989). Increased δ13C values can also indicate the presence of marine protein in a person's diet, due to the heightening effect of carbon reservoirs on the δ13C values of marine organisms, or access to imported plant protein such as millet (Schoeninger and DeNiro, 1984). The bone δ13C values presented in this paper show that there is little variation in the overall bulk bone values between different burial populations in Hereford, portraying high levels of continuity between Early and Late Medieval populations. Furthermore, there is no statistically significant difference between individuals of varying age or sex. The δ13C values are similar to those of other Medieval English populations, suggesting a largely terrestrial diet. In contrast, the Hereford populations carry relatively low δ15N values compared to their Late Medieval counterparts, possibly caused by less access to 15N enriched protein, but lack of geographic and faunal data in proximity to Hereford means they cannot be accurately compared. The incremental profiles present several similar characteristics, such as lower values during childhood, possibly caused by physiological processes related to growth and development, rather than a specific childhood diet or illness. We suggest that the bone collagen δ13C and δ15N values of children and young adults should be considered separately from those of fully developed adults. Furthermore, that the incremental dentine profiles of presumably healthy older adults may be used as intra-population standards with which to compare the bone collagen values of those who died during childhood in order to better identify outliers within that population.
In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection. High resolution (HR) image restoration from corresponding low resolution (LR) data is known as image super resolution (SR). Specifically, if a single image is used for the enhancement, the approach is called single image super resolution (SISR). Inherently, this problem is ill-posed since it is possible to recover various HR outputs for a single LR input. Such inverse problem is usually solved by utilizing the prior knowledge. The prior knowledge can be learned by predicting a pixel value with interpolation methods, e.g. bicubic interpolation (Keys, 1981), edge-guided interpolation (Zhang and Wu, 2006), or adaptive non local sparsity-based modeling (Romano et al., 2014). Another ways to acquire the knowledge is to exploit the internal structure of pixels within the same LR image (Glasner et al., 2009; Freedman and Fattal, 2011; Cui et al., 2014), or learn it from corresponding pairs of LR and HR examples, i.e. example-based algorithms (Chang et al., 2004; Kim and Kwon, 2010; Bevilacqua et al., 2012; Jia et al., 2013; Dong et al., 2016; Kim et al., 2016a, b; Tai et al., 2017; Liu et al., 2017). The first group, known as interpolation-based SISR (Li and Orchard, 2001), often intend to mitigate a down-sampling process only. Also, the interpolation techniques are based on generic smoothness priors and therefore are indiscriminate, as they smooth both edges and object parts, what leads to the blurring effect (Chang et al., 2004). 
We describe a patient with unilateral periventricular nodular heterotopia (PNH) and drug-resistant epilepsy, whose SEEG revealed that seizures were arising from the PNH, with the almost simultaneous involvement of heterotopic neurons (“micronodules”) scattered within the white matter, and subsequently the overlying cortex. Laser ablation of heterotopic nodules and the adjacent white matter rendered the patient seizure free. This case elucidates that “micronodules” scattered in white matter between heterotopic nodules and overlying cortex might be another contributor in complex epileptogenicity of heterotopia. Detecting patient-specific targets in the epileptic network of heterotopia creates the possibility to disrupt the pathological circuit by minimally invasive procedures. Periventricular nodular heterotopia (PNH) is a malformation of neuronal migration characterized by masses of neurons and glial cells with a rudimentary laminar organization located close to the periventricular germinal matrix [1,2]. PNH may be bilateral or unilateral. Genetic factors can play a major role in bilateral cases while acquired factors may be more important in the latter [3]. Focal epilepsy, most commonly drug-resistant, presents in childhood or early adulthood [4]. PNH often has intrinsic epileptogenicity but may not always be primarily involved in the generation of seizures [4–8]. Stereo-EEG (SEEG) studies found diverse, patient-specific networks, with seizure-onset being simultaneous in nodules and overlying or widespread cortical structures, or simultaneous in mesial temporal structures and ipsilateral adjacent heterotopia, or onset in the overlying cortex or the nodules alone [4,5,7,8]. Although an earlier study has characterized PNH as poorly responsive to traditional epilepsy surgery [9], others reported a high rate of seizure-freedom, similar to other lesional epilepsies [5]. Conflicting results were published recently on SEEG-guided thermocoagulation: as a very effective therapeutic approach for drug-resistant epilepsy related to PNH [7] and on contrary, having transitory, mild, or no effect [8]. There are only a few cases describing laser ablation in PNH (summaries and outcomes are shown in Table 1). 
A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC’09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC’09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization. Many real-life problems can be formulated more realistically within multiobjective optimization (MOO) framework since a set solutions, rather than a single solution, exhibiting different forms of concession among multiple and often conflicting objectives is provided as result of the optimization process (Abraham et al., 2005). Such a set of solutions containing different degrees of trade-off among multiple objectives is commonly known as a Pareto-optimal set in which Pareto-optimality is defined in terms of a dominance relation between two solutions as follows: given two solutions s1 and s2, s1≠s2, s1 is said to dominate s2 if s1 is not worse than s2 in all objectives and s1 is strictly better than s2 for at least one objective. For example, for a minimization problem, minf(x)=(f1(x),f2(x),…,fP(x)), x=(x1,x2,…,xn)∈Rn, solution s1∈Rn is said to dominate solution s2∈Rn, denoted as s1⪰s2, if and only if fi(s1)≤fi(s2) for i=1,2,…,j−1,j+1,…,P and fj(s1)<fj(s2) for at least one 1≤j≤P, where P is the number of objectives. 
Many Reinforcement Learning (RL) real-world applications have multi-dimensional action spaces which suffer from the combinatorial explosion of complexity. Then, it may turn infeasible to implement Centralized RL (CRL) systems due to the exponential increasing of dimensionality in both the state space and the action space, and the large number of training trials. In order to address this, this paper proposes to deal with these issues by using Decentralized Reinforcement Learning (DRL) to alleviate the effects of the curse of dimensionality on the action space, and by transferring knowledge to reduce the training episodes so that asymptotic converge can be achieved. Three DRL schemes are compared: DRL with independent learners and no prior-coordination (DRL-Ind); DRL accelerated-coordinated by using the Control Sharing (DRL+CoSh) Knowledge Transfer approach; and a proposed DRL scheme using the CoSh-based variant Nearby Action Sharing to include a measure of the uncertainty into the CoSh procedure (DRL+NeASh). These three schemes are analyzed through an extensive experimental study and validated through two complex real-world problems, namely the inwalk-kicking and the ball-dribbling behaviors, both performed with humanoid biped robots. Obtained results show (empirically): (i) the effectiveness of DRL systems which even without prior-coordination are able to achieve asymptotic convergence throughout indirect coordination; (ii) that by using the proposed knowledge transfer methods, it is possible to reduce the training episodes and to coordinate the DRL process; and (iii) obtained learning times are between 36% and 62% faster than the DRL-Ind schemes in the case studies. Reinforcement Learning (RL) is increasingly being used to learn complex behaviors in robotics. Two of the main challenges to be solved for modeling RL systems acting in the real-world are: (i) the high dimensionality of the state and action spaces, and (ii) the large number of training trials required to learn most of complex behaviors. Many real-world applications have multi-dimensional action spaces (e.g., multiple actuators or effectors). In those cases, RL suffers from the combinatorial explosion of complexity which occurs when a single or centralized RL (CRL) scheme is used. It may turn infeasible to implement CRL systems in terms of computational resources or learning time due to the exponential increasing of dimensionality in both the state space and the action space as in Martín and de Lope Asiaín (2007) and Leottau et al. (2018). Instead, the use of Decentralized Reinforcement Learning (DRL) helps to alleviate this problem as it has been empirically evidenced by Buşoniu et al. (2006) and Leottau et al. (2017, 2018). In DRL, a problem is decomposed into several sub-problems, whose resources are managed separately while working toward a common goal, i.e. learning and performing a behavior. In the case of multidimensional action spaces, a sub-problem corresponds to control one particular variable. For instance, in mobile robotics, a common high-level motion command is the requested velocity vector (e.g., [vx,vy,vθ] for an omni-directional robot). Then, if each speed component of this vector is handled individually, a distributed control scheme can be applied. This paper presented a Decentralized Reinforcement Learning (DRL) architecture to alleviate the effects of the curse of dimensionality and the large number of training trials required to learn tasks in which multi-dimensional action spaces are involved. Three DRL schemes are considered and tested: DRL-Ind, implemented with independent learners and no prior-coordination; DRL+CoSh, accelerated-coordinated by using the Control Sharing (CoSh) knowledge transfer approach, which is extended from the single-agent case to the DRL proposed architecture; and DRL+NeASh, a knowledge transfer approach proposed for including a measure of uncertainty to the CoSh procedure.
Humans have evolved a distinctive relationship with “objects” (tools and technology), which has strongly influenced their anatomical and cognitive capacities. The human hand is functionally specialized for manipulation and, in terms of cognition, tools are generally integrated into the body scheme when handled. Stone tools can supply information on the evolution of this cognitive reciprocal relationship. Despite the many studies on stone tool morphology, information on hand-tool system is scanty. In this preliminary survey, we measure hand-tool distances in three lithic instruments of different size (cleaver, handaxe, convergent sidescraper), in order to investigate basic patterns associated with their handling patterns. Tool size does influence the distance from the wrist and the aperture of the hand. The associated grasping differences depend more on the tool length than on the hand morphology or dimension. Nonetheless, hand-tool metrics covariation patterns are different according to the different tool types, suggesting specific factors associated with their respective haptic experience. Females display, on average, more variability than males when handling the sidescraper, but not for the power-gripped cleaver and handaxe. We propose a new method to analyze hand-tool metrics according to the haptic interaction. These kinds of studies provide basic mandatory information which can be used to develop proper ergonomic and cognitive perspectives in tool extension and cognitive archaeology. Primates have a particular ability in object manipulation when compared with other mammals. Although there is evidence of object-assisted behaviors in many primate species (Goodall, 1964; Van Lawick-Goodall, 1971; Luncz et al., 2016), only humans evolved a specialized tool-dependant culture (Plummer, 2004). The importance of the human-tool relationship is the main reason why technology is studied in many different fields ranging from biomechanics to rehabilitation medicine or paleoanthropology. 
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. With the rapidly increasing requirement for “high-speed and high-density” railway transportation, the railway industry has to pursue the intelligence of railway transportation system. Railway intelligent transportation system is a new generation system that is integrated by advanced intelligent technology, communication technology and information processing technology to fulfill the aims of guaranteeing safety, improving transportation efficiency, enhancing management and high service quality at lower cost (Li et al., 2003; Ning et al., 2006). High-speed railway is a mode of transport with large capacity and high safety, which is mainly used as inter-city routes with heavy passenger transport (Wong et al., 2002). Thus, effectively managing and operating the high-speed railways then becomes an important issue for the railway operators. In this paper, we propose a stochastic integer programming model for passenger capacity problem under random passenger demands and some strategies for optimizing train timetabling problem in the high-speed rail network by using network optimization techniques. Then rewrite it as an integer programming model when assumes that there are finite number scenarios. The model incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. And it needs data from train service planning and online scheduling system to provide available transport capacities as well as all associated prices and costs. Historical data is gathered and maintained over time to permit forecasting the variation of passenger demands. In addition, we provide a tabu-based algorithm and strategies in order to obtain approximate optimal solutions and near-optimal timetable for the problem in a short time.
Reinforcement Learning (RL) is an artificial intelligence technique used to solve Markov and semi-Markov decision processes. Actor critics form a major class of RL algorithms that suffer from a critical deficiency, which is that the values of the so-called actor in these algorithms can become very large causing computer overflow. In practice, hence, one has to artificially constrain these values, via a projection, and at times further use temperature-reduction tuning parameters in the popular Boltzmann action-selection schemes to make the algorithm deliver acceptable results. This artificial bounding and temperature reduction, however, do not allow for full exploration of the state space, which often leads to sub-optimal solutions on large-scale problems. We propose a new actor–critic algorithm in which (i) the actor’s values remain bounded without any projection and (ii) no temperature-reduction tuning parameter is needed. The algorithm also represents a significant improvement over a recent version in the literature, where although the values remain bounded they usually become very large in magnitude, necessitating the use of a temperature-reduction parameter. Our new algorithm is tested on an important problem in an area of management science known as airline revenue management, where the state-space is very large. The algorithm delivers encouraging computational behavior, outperforming a well-known industrial heuristic called EMSR-b on industrial data. Reinforcement Learning or RL (see Bertsekas and Tsitsiklis (1996), Sutton and Barto (1998), Gosavi (2014b) and Szepesvári (2010)) is used to solve problems in which an agent selects optimal actions in an unknown stochastic environment via repeated trials and errors. In every trial, the agent gathers feedback from the environment and uses the feedback to update its knowledge base. Typically, after a large number of trials and errors in its interactions, the agent learns to select an optimal action in every state. The Markov Decision Process or Problem (MDP) and the semi-MDP (SMDP) (Bertsekas and Tsitsiklis, 1996) have been extensively used as the underlying models in the above-described RL domain. Essentially, in the MDP model, the underlying system dynamics and behavior are governed by Markov chains. Further, in an MDP, the time taken in a one-step transition from any state to any other is assumed to be the same. The SMDP is a more general model in which this transition time is assumed to be a random variable whose distribution is known. 
Autosomal dominant lateral temporal epilepsy (ADLTE) is a genetic focal epilepsy syndrome characterized by focal seizures with dominant auditory symptomatology. We present a case report of an 18-year-old patient with acute onset of seizures associated with epilepsy. Based on the clinical course of the disease and the results of the investigation, the diagnosis of ADLTE with a proven mutation in the RELN gene, which is considered causative, was subsequently confirmed. The aim of this study was to use 3 Tesla (3 T) magnetic resonance imaging (MRI) and advanced neuroimaging methods in a patient with a confirmed diagnosis of ADTLE. 3 T MRI brain scan and advanced neuroimaging methods were used in the standard protocols to analyzse voxel-based MRI, cortical thickness, and functional connectivity. Morphometric MRI analysis (blurred grey-white matter junctions, voxel-based morphometry, and cortical thickness analysis) did not provide any informative results. The functional connectivity analysis revealed higher local synchrony in the patient in the left temporal (middle temporal gyrus), left frontal (supplementary motor area, superior frontal gyrus), and left parietal (gyrus angularis, gyrus supramarginalis) regions and the cingulate (middle cingulate gyrus) as compared to healthy controls. Evidence of multiple areas of functional connectivity supports the theory of epileptogenic networks in ADTLE. Further studies are needed to elucidate this theory. Autosomal dominant lateral temporal epilepsy (ADLTE), also known as autosomal dominant partial epilepsy with auditory features (ADPEAF), is a genetic focal epilepsy syndrome. It is characterized by focal seizures with or without a loss of consciousness, inconstantly with secondary generalization. Focal seizures are mainly characterized by auditory symptoms. Auditory auras are the most common symptom, and occur in isolation or precede some kind of receptive aphasia. Other symptoms following the auditory phenomena include vertigo, paroxysmal headache, déjà-vu, and epigastric discomfort [2]. Sensory symptomatology (e.g., visual, olfactory) and autonomic motor symptomatology are less common. Neurological findings and the mental status of patients are normal. The manifestation of the syndrome occurs between the ages of four and 50 years, with the maximal occurrence in the adolescent period [3]. Structural examinations of the brain (CT, MRI) at standard resolutions most often return normal findings. Routine and sleep electroencephalography (EEG) may be normal, but findings of focal/slow wave abnormality in the temporal areas are not uncommon, occurring in approximately 20% of patients [2,3]. The disease heredity is autosomal dominant with varying penetration (about 70%) [1]. The diagnosis is based on personal and family history, seizure semiology, and normal MRI brain scan. Approximately 33% of patients show a pathogenic variant in the LGI1 gene [2]. In a smaller percentage of ADLTE cases, mutation in the reelin (RELN) gene is shown in heterozygous form [2]. The RELN gene is primarily expressed in brain tissue. The protein product of the RELN gene is called reelin. Reelin regulates the correct formation of laminated structures during embryonic development and postnatally modulates dendritic growth and synaptic plasticity [2]. Homozygous variants of the RELN mutation cause lissencephaly with cerebellar hypoplasia, severe neuronal migration defects, delayed cognitive development, and epileptic seizures [5]. Heterozygous mutation of the RELN mutation can cause small changes in the cortex corresponding to neuronal migration disorders [2]. The prognosis of the disease is benign and, in most cases, there is a very good response to treatment with properly selected anti-seizure drugs (valproate, phenytoin, and carbamazepine are recommended). SECTION Case report 
Laugerie-Haute is one of the key sequences of the European Upper Paleolithic. However, the absolute chronology of the sequence is not well-established, and recently the integrity of the layers has been called into question. In this paper, we present new radiocarbon dating results for the entire sedimentary sequence at Laugerie-Haute Ouest, which contains important “Aurignacian V” and Solutrean assemblages. Our results show that the entire Laugerie-Haute Ouest sequence was deposited in a period of 8500–10,000 years, dating between 28 and 19 cal. kBP. Understanding change in the archaeological record requires control of chronology. Here we present a high-resolution chronology for one of the key sequences for the European Upper Paleolithic, the large rockshelter site of Laugerie-Haute (Périgord, Dordogne, Southwest France). The two sedimentary sequences at Laugerie-Haute-Est and -Ouest together cover the main phases of the Upper Paleolithic of Western Europe. The archaeological significance of the site is recognized in its status as a national monument and a UNESCO World Heritage site. Despite the status of the Laugerie-Haute site, its long research history and its much discussed Solutrean levels, the chronology of the sedimentary sequence of Laugerie-Haute is not firmly established. In this paper, we present a radiocarbon-based chronology for the complete sedimentary sequence of Laugerie-Haute Ouest. SECTION The site Our study obtained 38 point provenienced samples (unheated, compact long bone fragments, mostly from reindeer) from Laugerie-Haute Ouest, one of the key sequences for the European Upper Paleolithic, and especially for the Solutrean technocomplex. The samples and the resulting dates cover the complete sequence at this site. Our results show that the entire LHO sequence was deposited in a period of 8500–10,000 years, dating between 28 cal. kBP and 19 cal. kBP. The Solutrean levels represent a short time period of perhaps only one millennium. The Aurignacian V, one of the rare occurrences in Southwest France, dates to the early phase in comparison to the Iberian time range of this phenomenon. The results conform with stratigraphical integrity of Laugerie-Haute Ouest, support the chronological framework for the Upper Paleolithic and provide a basis for establishing a climatic framework of the Laugerie-Haute Ouest sequence.
This article presents the second study of ochres associated with the Lower Magdalenian (18.7 cal kya) “Red Lady” human burial in El Mirón Cave (Cantabria, Spain). In the first study (Seva Román et al., 2015), we determined that the burial deposit contained iron oxides and idiomorphic hematite that were not from sources near the site, but possibly from Monte Buciero, some 27 km to the north on the present Atlantic shore in Santoña. We have now analyzed sediments both from the burial and from samples taken during prospection on Monte Buciero, along with ochres from deposits above the burial layer, from the face of a large limestone block immediately adjacent to the burial, and from an area of the cave wall close to it in the NE corner of the cave vestibule that bears the engraving of a horse. As before, the analyses used were binocular microscopy, Raman spectroscopy, X-ray diffraction (XRD), scanning electron microscopy (SEM-EDX) and inductively coupled plasma mass spectrometry (ICP-MS). We were thus able to determine that the sources of the ochres in the burial deposit and on the block are the same—Monte Buciero. However we found substantial differences between the ochre on the block and the ochre underlying the horse panel image on the cave wall, which very likely also dates to sometime in the Magdalenian. The instrumental analysis of ochres has provided important information on the various uses of these mineral oxides in prehistory (e.g., García Borja et al., 2004; Bersani and Lottici, 2016; Eiselt et al., 2011; Marshall et al., 2005; Mooney et al., 2003; Popelka et al., 2007, 2008, 2016; Rifkin et al., 2016; Ferrier et al., 2017), including Upper Paleolithic cave painting (Clottes et al., 1990a, 1990b; Hernanz et al., 2010, 2012; Hernanz, 2015; Iriarte et al., 2009, 2017; Seva Román et al., 2015), and possible use by Neanderthals (Zilhão, 2001; Zilhão et al., 2010; D'Errico et al., 1998, 2010; Hovers et al., 2003; Roebroeks et al., 2012). The arguably symbolic use of red ochre has been documented from archeological sites in Africa, the Near East and Europe dating as early as the Acheulean and Middle Paleolithic/Middle Stone Age, and may be a fundamentally common characteristic of genus Homo (e.g., Wreschner, 1980; Watts, 2002; Hovers et al., 2003). Based on these results we can make the following conclusions: The ochre used in the Red Lady burial was brought from Monte Buciero, in Santoña, about 27 km from El Mirón Cave. In order to paint the eastern face of the block in the area contiguous with the burial, people utilized the same ochre as in the burial, but with the addition of microfragments of bone together presumably with some (non-detected) agglutinant, such as animal or vegetal fat. The ochre utilized in the pigment on the cave wall adjacent to the horse image was made of goethite which was heated and pounded, thus producing a redder tone and an extremely fine granulometry. Triturated bone was not utilized in this paint. It is an iron oxide that is very abundant in the vicinity of the cave in the upper Asón and tributary Carranza valleys, which we also prospected and sampled. The ochres utilized in the upper strata (501 y 503), are of a completely different nature and are geologically more similar to the iron oxide outcrops in the above-mentioned valleys, with the appearance of siderite. In its elaboration, as in its granulometry and composition, the paint on the block is completely different from the pigment adjacent to the engraved horse image on the cave wall.
Currently, much effort is directed towards the development of new cell sources for clinical therapy using cell fate conversion by small molecules. Direct lineage reprogramming to a progenitor state has been reported in terminally differentiated rodent hepatocytes, yet remains a challenge in human hepatocytes. Human hepatocytes were isolated from healthy and diseased donor livers and reprogrammed into progenitor cells by 2 small molecules, A83-01 and CHIR99021 (AC), in the presence of EGF and HGF. The stemness properties of human chemically derived hepatic progenitors (hCdHs) were tested by standard in vitro and in vivo assays and transcriptome profiling. We developed a robust culture system for generating hCdHs with therapeutic potential. The use of HGF proved to be an essential determinant of the fate conversion process. Based on functional evidence, activation of the HGF/MET signal transduction system collaborated with A83-01 and CHIR99021 to allow a rapid expansion of progenitor cells through the activation of the ERK pathway. hCdHs expressed hepatic progenitor markers and could self-renew for at least 10 passages while retaining a normal karyotype and potential to differentiate into functional hepatocytes and biliary epithelial cells in vitro. Gene expression profiling using RNAseq confirmed the transcriptional reprogramming of hCdHs towards a progenitor state and the suppression of mature hepatocyte transcripts. Upon intrasplenic transplantation in several models of therapeutic liver repopulation, hCdHs effectively repopulated the damaged parenchyma. Our study is the first report of successful reprogramming of human hepatocytes to a population of proliferating bipotent cells with regenerative potential. hCdHs may provide a novel tool that permits expansion and genetic manipulation of patient-specific progenitors to study regeneration and the repair of diseased livers. Currently, liver transplantation represents the only approved standard of care for patients with end-stage liver diseases.1 Experimental studies in rodents and clinical trials of hepatocyte transplantation have shown that direct infusion of mature hepatocytes may serve as an alternative to whole organ replacement in some cases. However, hepatocyte transplantation only results in a partial and relatively short-term correction of liver dysfunction, and has been hampered by numerous issues related to the shortage of donor tissue, limited numbers of cells suitable for transplantation, and a low efficiency of engraftment in the abnormal microenvironment of diseased livers.2–4 In addition, human hepatocytes are difficult to maintain and expand in vitro because of the lack of adequate environmental signals. Typically, mature hepatocytes have low proliferative potential and easily become apoptotic in culture which reduces their therapeutic value. 
Phenotypic and functional natural killer (NK)-cell alterations are well described in chronic hepatitis B virus (cHBV) infection. However, it is largely unknown whether these alterations result from general effects on the overall NK-cell population or the emergence of distinct NK-cell subsets. Human cytomegalovirus (HCMV) is common in cHBV and is associated with the emergence of memory-like NK cells. We aimed to assess the impact of these cells on cHBV infection. To assess the impact of memory-like NK cells on phenotypic and functional alterations in cHBV infection, we performed in-depth analyses of circulating NK cells in 52 patients with cHBV, 45 with chronic hepatitis C virus infection and 50 healthy donors, with respect to their HCMV serostatus. In patients with cHBV/HCMV+, FcεRIγ- memory-like NK cells were present in higher frequencies and with higher prevalence than in healthy donors with HCMV+. This pronounced HCMV-associated memory-like NK-cell expansion could be identified as key determinant of the NK-cell response in cHBV infection. Furthermore, we observed that memory-like NK cells consist of epigenetically distinct subsets and exhibit key metabolic characteristics of long-living cells. Despite ongoing chronic infection, the phenotype of memory-like NK cells was conserved in patients with cHBV/HCMV+. Functional characteristics of memory-like NK cells also remained largely unaffected by cHBV infection with the exception of an increased degranulation capacity in response to CD16 stimulation that was, however, detectable in both memory-like and conventional NK cells. The emergence of HCMV-associated memory-like NK cells shapes the overall NK-cell response in cHBV infection and contributes to a general shift towards CD16-mediated effector functions. Therefore, HCMV coinfection needs to be considered in the design of immunotherapeutic approaches that target NK cells in cHBV. Hepatitis B virus (HBV) is a non-cytopathic DNA virus that triggers immune-mediated liver pathology. It is estimated that 257 million people worldwide are suffering from chronic HBV (cHBV) infection and are therefore at high risk of developing progressive liver disease. The capacity of the immune system to control HBV infection provides a rationale for immunotherapeutic approaches. Direct and indirect roles of natural killer (NK) cells in mediating anti-HBV immunity have been described.1,2 For example, in a hydrodynamic injection model of acute HBV infection, a direct antiviral effect of NK cells has been reported.3 In that model, NK cells can also indirectly support HBV clearance by positively affecting HBV-specific T-cell responses via interferon γ (IFNγ) secretion.4 However, in cHBV infection NK cells exhibit an impaired IFNγ production, consequently leading to reduced non-cytolytic antiviral potential and diminished support of T-cell responses.5,6 A landmark study has also shown that despite their reduced cytokine production, NK cells obtained from patients with cHBV displayed a conserved cytotoxic function. This phenomenon has been termed functional dichotomy.5 
Acute liver failure is a rapidly progressive deterioration of hepatic function resulting in high mortality and morbidity. Metabolic enzymes can translocate to the nucleus to regulate histone acetylation and gene expression. Levels and activities of pyruvate dehydrogenase complex (PDHC) and lactate dehydrogenase (LDH) were evaluated in nuclear fractions of livers of mice exposed to various hepatotoxins including CD95-antibody, α-amanitin, and acetaminophen. Whole-genome gene expression profiling by RNA-seq was performed in livers of mice with acute liver failure and analyzed by gene ontology enrichment analysis. Cell viability was evaluated in cell lines knocked-down for PDHA1 or LDH-A and in cells incubated with the LDH inhibitor galloflavin after treatment with CD95-antibody. We evaluated whether the histone acetyltransferase inhibitor garcinol or galloflavin could reduce liver damage in mice with acute liver failure. Levels and activities of PDHC and LDH were increased in nuclear fractions of livers of mice with acute liver failure. The increase of nuclear PDHC and LDH was associated with increased concentrations of acetyl-CoA and lactate in nuclear fractions, and histone H3 hyper-acetylation. Gene expression in livers of mice with acute liver failure suggested that increased histone H3 acetylation induces the expression of genes related to damage response. Reduced histone acetylation by the histone acetyltransferase inhibitor garcinol decreased liver damage and improved survival in mice with acute liver failure. Knock-down of PDHC or LDH improved viability in cells exposed to a pro-apoptotic stimulus. Treatment with the LDH inhibitor galloflavin that was also found to inhibit PDHC, reduced hepatic necrosis, apoptosis, and expression of pro-inflammatory cytokines in mice with acute liver failure. Mice treated with galloflavin also showed a dose-response increase in survival. PDHC and LDH translocate to the nucleus, leading to increased nuclear concentrations of acetyl-CoA and lactate. This results in histone H3 hyper-acetylation and expression of damage response genes. Inhibition of PDHC and LDH reduces liver damage and improves survival in mice with acute liver failure. Thus, PDHC and LDH are targets for therapy of acute liver failure. Acute liver failure is a rapidly progressive, life-threatening deterioration of liver function that results in altered neurological state and coagulopathy. The only treatment that improves the outcome of acute liver failure is emergency liver transplantation. Because liver transplantation has limitations related to invasiveness, limited donor availability and high mortality, therapies for acute liver failure are urgently needed. 
The variety of alterations found in hepatocellular carcinoma (HCC) makes the identification of functionally relevant genes and their combinatorial actions in tumorigenesis challenging. Deregulation of receptor tyrosine kinases (RTKs) is frequent in HCC, yet little is known about the molecular events that cooperate with RTKs and whether these cooperative events play an active role at the root of liver tumorigenesis. A forward genetic screen was performed using Sleeping Beauty transposon insertional mutagenesis to accelerate liver tumour formation in a genetic context in which subtly increased MET RTK levels predispose mice to tumorigenesis. Systematic sequencing of tumours identified common transposon insertion sites, thus uncovering putative RTK cooperators for liver cancer. Bioinformatic analyses were applied to transposon outcomes and human HCC datasets. In vitro and in vivo (through xenografts) functional screens were performed to assess the relevance of distinct cooperative modes to the tumorigenic properties conferred by RTKs. We identified 275 genes, most of which are altered in patients with HCC. Unexpectedly, these genes are not restricted to a small set of pathway/cellular processes, but cover a large spectrum of cellular functions, including signalling, metabolism, chromatin remodelling, mRNA degradation, proteasome, ubiquitination, cell cycle regulation, and chromatid segregation. We validated 15 tumour suppressor candidates, as shRNA-mediated targeting confers tumorigenicity to RTK-sensitized cells, but not to cells with basal RTK levels. This demonstrates that the context of enhanced RTK levels is essential for their action in tumour initiation. Our study identifies unanticipated genetic interactions underlying gene cooperativity with RTKs in HCC. Moreover, these results show how subtly increased levels of wild-type RTKs provide a tumour permissive cellular environment allowing a large spectrum of deregulated mechanisms to initiate liver cancer. Hepatocellular carcinoma (HCC) is among the most aggressive cancers, with an increasing incidence, and few therapeutic options.1 The exceptional investments on -omics studies over the last decade have unveiled not only an impressive list of alterations, but also a high degree of molecular heterogeneity between patients with HCC.2,3 The uniqueness of HCC in its alterations and heterogeneity may explain how treatments effective in other cancers have largely failed when applied to HCC.4 Such context challenges the interpretation of -omics data, with the necessity to: i) determine which of these alterations are functionally relevant for tumorigenic properties, ii) distinguish sets of alterations with a tumour-boosting efficiency linked to specific patient subtypes or genetic contexts, and iii) elucidate how different combinatorial alterations can lead to equivalent vs. divergent fitness outcomes in cancer cells. The identification of functionally relevant signals, and of functional synergistic interactions between co-occurring events, is further complicated by the fact that some signals, although rarely mutated in HCC, are frequently activated in a high proportion of patients and are considered key regulators of tumorigenesis. This is the case, for example, for some receptor tyrosine kinase (RTK) pathway genes.4,5 The implication of deregulated RTK signalling in HCC is well established and RTK targeting agents are actively explored for combined therapies.6 
Hepatocellular carcinoma (HCC) risk varies dramatically in patients with cirrhosis according to well-described, readily available predictors. We aimed to develop simple models estimating HCC risk in patients with alcohol-related liver disease (ALD)-cirrhosis or non-alcoholic fatty liver disease (NAFLD)-cirrhosis and calculate the net benefit that would be derived by implementing HCC surveillance strategies based on HCC risk as predicted by our models. We identified 7,068 patients with NAFLD-cirrhosis and 16,175 with ALD-cirrhosis who received care in the Veterans Affairs (VA) healthcare system in 2012. We retrospectively followed them for the development of incident HCC until January 2018. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at entry into the cohort in 2012. We plotted decision curves of net benefit against HCC screening thresholds. We identified 1,278 incident cases of HCC during a mean follow-up period of 3.7 years. Mean annualized HCC incidence was 1.56% in NAFLD-cirrhosis and 1.44% in ALD-cirrhosis. The final models estimating HCC were developed separately for NAFLD-cirrhosis and ALD-cirrhosis and included 7 predictors: age, gender, diabetes, body mass index, platelet count, serum albumin and aspartate aminotransferase to √alanine aminotransferase ratio. The models exhibited very good measures of discrimination and calibration and an area under the receiver operating characteristic curve of 0.75 for NAFLD-cirrhosis and 0.76 for ALD-cirrhosis. Decision curves showed higher standardized net benefit of risk-based screening using our prediction models compared to the screen-all approach. We developed simple models estimating HCC risk in patients with NAFLD-cirrhosis or ALD-cirrhosis, which are available as web-based tools (www.hccrisk.com). Risk stratification can be used to inform risk-based HCC surveillance strategies in individual patients or healthcare systems or to identify high-risk patients for clinical trials. Annual HCC risk varies greatly in patients with cirrhosis ranging from as little as <0.2% to as high as >5%. Although this variability is well recognized, few models exist to estimate HCC risk in patients with cirrhosis and none are commonly used. Liver societies recommend the same screening strategy (abdominal ultrasonography every 6 months with or without concomitant serum alpha-fetoprotein [AFP]) irrespective of HCC risk.1–3 Studies show poor compliance with these screening recommendations.4,5 Stratification of HCC risk in patients with cirrhosis into low (e.g. <1% per year), medium (e.g. 1–3% per year) and high (e.g. >3% per year) would enable optimization and individualization of outreach efforts and screening strategies in patients with cirrhosis. It would also enable identification of high-risk patients for clinical trials of HCC screening. 
Although CD8+T cell exhaustion hampers viral control during chronic HBV infection, the pool of CD8+T cells is phenotypically and functionally heterogeneous. Therefore, a specific subpopulation of CD8+T cells should be further investigated. This study aims to dissect a subset of CD8+T cells expressing C-X-C motif chemokine receptor 5 (CXCR5) in chronic HBV infection. The frequency of CXCR5+CD8+T cells and the levels of C-X-C motif chemokine ligand 13 (CXCL13), a chemokine of CXCR5, were measured in patients with chronic HBV infection. C57BL/6, interleukin (IL)-21 receptor- or B cell-deficient mice were hydrodynamically injected with pAAV-HBV1.2 plasmids. Phenotype and functions of peripheral and intrahepatic CXCR5+ and CXCR5−CD8+T cells were assessed. CXCR5+CD8+T cells were partially exhausted but possessed a stronger antiviral ability than the CXCR5− subset in patients with chronic HBV infection; moreover, CXCR5+CD8+T cells were associated with a favorable treatment response in patients with chronic hepatitis B (CHB). High levels of CXCL13 from patients with CHB facilitated the recruitment of intrahepatic CXCR5+CD8+T cells, and this subpopulation produced high levels of HBV-specific interferon (IFN)-γ and IL-21. Notably, PD1 (programmed death 1) blockade and exogenous IL-21 enhanced the production of IFN-γ. More strikingly, mice injected with CXCR5+CD8+T cells showed remarkably decreased expression of HBsAg. Additionally, an impaired production of HBV-specific IFN-γ from intrahepatic CXCR5+CD8+T cells was observed in IL-21 receptor- or B cell-deficient mice. CXCL13 promotes the recruitment of CXCR5+CD8+T cells to the liver, and this subpopulation improves viral control in chronic HBV infection. The identification of this unique subpopulation may contribute to a better understanding of CD8+T cell functions and provide a potential immunotherapeutic target in chronic HBV infection. CD8+T cell responses are crucial to prevent and control HBV infection, but their activity is thought to be abolished during persistent HBV infection. Thus, functional defects in CD8+T cell responses, termed exhaustion, are considered the major factor driving the chronicity of HBV infection.1 However, it is unclear how exhausted CD8+T cells are able to mediate the sustained inhibition of HBV replication in some patients with chronic HBV infection (CHB). Accumulated data have established that the pool of exhausted CD8+T cells consists of phenotypically and functionally distinct subpopulations.2,3 Although CD8+T cells are poorly functional in chronic viral infections, they are not functionally inert.4–6 Therefore, a distinct subpopulation of CD8+T cells in chronic HBV infection should be further identified. 
Sofosbuvir, an NS5B inhibitor, combined with velpatasvir, an NS5A inhibitor (SOF/VEL), produces high sustained virologic response rates 12 weeks after treatment (SVR12) in patients with genotype 1–6 HCV infection, and has no anticipated clinically relevant drug-drug interactions with immunosuppressants. This study evaluated the safety and efficacy of SOF/VEL in adults with recurrent chronic genotype 1–4 HCV infection after liver transplant. Patients received SOF/VEL 400/100 mg daily for 12 weeks. Patients could be treatment experienced or treatment naïve with no cirrhosis or with compensated cirrhosis. The primary endpoints were SVR12 and discontinuations due to adverse events. A total of 79 patients were enrolled and treated in this study (37 [47%] had genotype 1, 3 [4%] genotype 2, 35 [44%] genotype 3, and 4 [5%] genotype 4 HCV). Of these, 81% were male, 82% were white, 18% had compensated cirrhosis, and 59% were treatment experienced. The most commonly used immunosuppressants were tacrolimus (71%), mycophenolic acid (24%), cyclosporine (14%), and azathioprine (11%). Median (range) time from liver transplantation was 7.5 (0.3, 23.9) years. The SVR12 rate was 96%. By genotype, SVR12 rates were 95% (genotype 1), 100% (genotype 2), 97% (genotype 3), and 100% (genotype 4). Two patients experienced virologic relapse: one with genotype 1a infection was non-cirrhotic and treatment naïve, and one with genotype 3 infection was non-cirrhotic and treatment experienced. One patient discontinued SOF/VEL due to hyperglycemia. No serious or severe adverse events were deemed SOF/VEL-related by the investigator, and no liver transplant rejection episodes or deaths occurred during the study period. Treatment with SOF/VEL for 12 weeks was highly effective and well tolerated in genotype 1–4 HCV-infected liver transplant recipients with and without cirrhosis. Sofosbuvir/velpatasvir is a combination of two drugs in one tablet that is approved for the treatment of patients with chronic hepatitis C virus (HCV) infection. When patients with chronic HCV infection receive a liver transplant, the HCV infection usually recurs, and damages the transplanted liver. This study tested the effects of 12 weeks of sofosbuvir/velpatasvir treatment in patients who had HCV recurrence after a liver transplant. Three months following the end of treatment, 96% of patients were cured of HCV infection. Among HCV-infected liver transplant recipients, HCV recurrence emerges in nearly all patients.1 Within five years post-transplant, cirrhosis related to HCV ensues in approximately 30% of patients with recurrent, chronic HCV infection and is associated with increased graft loss rates and death.2–5 In the setting of post-transplant immunosuppression, the rate of hepatic fibrosis is accelerated in HCV-infected patients compared to the pre-transplant period.6 The single-tablet regimen of SOF/VEL was well tolerated and highly effective in genotype 1–4 HCV-infected liver transplant recipients. SOF/VEL is a pangenotypic, ribavirin-free, simple, and well tolerated treatment option for HCV-infected liver transplant recipients. SECTION Financial support
Treatment allocation in patients with hepatocellular carcinoma (HCC) on a background of Child-Pugh B (CP-B) cirrhosis is controversial. Liver resection has been proposed in small series with acceptable outcomes, but data are limited. The aim of this study was to evaluate the outcomes of patients undergoing liver resection for HCC in CP-B cirrhosis, focusing on the surgical risks and survival. Patients were retrospectively pooled from 14 international referral centers from 2002 to 2017. Postoperative and oncological outcomes were investigated. Prediction models for surgical risks, disease-free survival and overall survival were constructed. A total of 253 patients were included, of whom 57.3% of patients had a preoperative platelet count <100,000/mm3, 43.5% had preoperative ascites, and 56.9% had portal hypertension. A minor hepatectomy was most commonly performed (84.6%) and 122 (48.2%) were operated on by minimally invasive surgery (MIS). Ninety-day mortality was 4.3% with 6 patients (2.3%) dying from liver failure. One hundred and eight patients (42.7%) experienced complications, of which the most common was ascites (37.5%). Patients undergoing major hepatectomies had higher 90-day mortality (10.3% vs. 3.3%; p = 0.04) and morbidity rates (69.2% vs. 37.9%; p <0.001). Patients undergoing an open hepatectomy had higher morbidity (52.7% vs. 31.9%; p = 0.001) than those undergoing MIS. A prediction model for surgical risk was constructed (https://childb.shinyapps.io/morbidity/). The 5-year overall survival rate was 47%, and 56.9% of patients experienced recurrence. Prediction models for overall survival (https://childb.shinyapps.io/survival/) and disease-free survival (https://childb.shinyapps.io/DFsurvival/) were constructed. Liver resection should be considered for patients with HCC and CP-B cirrhosis after careful selection according to patient characteristics, tumor pattern and liver function, while aiming to minimize surgical stress. An estimation of the surgical risk and survival advantage may be helpful in treatment allocation, eventually improving postoperative morbidity and achieving safe oncological outcomes. Hepatocellular carcinoma (HCC) is the most common primary liver tumor and the third leading cause of cancer-related deaths worldwide.1,2 When feasible, curative options such as liver transplantation (LT) and resection represent the treatment of choice as they offer long-term survival.3,4 HCC occurs primarily in patients with underlying liver disease, negatively affecting prognosis and increasing the complexity of treatment;5,6 liver cirrhosis, in fact, is an independent prognostic factor for both short and long-term outcomes, and the assessment of liver function remains critical in the management of patients with HCC as selected treatments may induce collateral liver damage, eventually leading to decompensation.7 Child-Pugh classification has been proposed as a scoring system to grade liver function and is currently adopted by most of the available guidelines on HCC treatment.7–13 The clinical scenario of a patient with HCC in the setting of Child-Pugh B cirrhosis frequently represents a delicate challenge. When patients fulfil the criteria for LT, the benefits of this option should be balanced with the waiting list and the scarcity of donors. When LT is not an option as primary treatment, non-curative treatments should be considered in the context of survival. In either scenario, liver resection represents a valid choice that could be safely considered in the appropriate setting. An estimation of the surgical risk and of oncological survival may be useful in deciding whether to undertake one or other treatment while avoiding unpredictable outcomes; accurate patient selection according to preoperative baseline characteristics, tumor burden and liver functional tests, as well as minimization of the surgical stress, could lead to improvements in postoperative morbidity and lead to safe oncological outcomes. SECTION Abbreviations
Osteoporotic fractures are a major cause of morbidity and reduced quality of life in patients with primary sclerosing cholangitis (PSC), a progressive bile duct disease of unknown origin. Although it is generally assumed that this pathology is a consequence of impaired calcium homeostasis and malabsorption, the cellular and molecular causes of PSC-associated osteoporosis are unknown. We determined bone mineral density by dual-X-ray absorptiometry and assessed bone microstructure by high-resolution peripheral quantitative computed tomography in patients with PSC. Laboratory markers of liver and bone metabolism were measured, and liver stiffness was assessed by FibroScan. We determined the frequency of Th17 cells by the ex vivo stimulation of peripheral blood mononuclear cells in a subgroup of 40 patients with PSC. To investigate the potential involvement of IL-17 in PSC-associated bone loss, we analyzed the skeletal phenotype of mice lacking Abcb4 and/or Il-17. Unlike in patients with primary biliary cholangitis, bone loss in patients with PSC was not associated with disease duration or liver fibrosis. However, we observed a significant negative correlation between the bone resorption biomarker deoxypyridinoline and bone mineral density in the PSC cohort, indicating increased bone resorption. Importantly, the frequency of Th17 cells in peripheral blood was positively correlated with the urinary deoxypyridinoline level and negatively correlated with bone mass. We observed that Abcb4-deficient mice displayed a low-bone-mass phenotype, which was corrected by an additional Il-17 deficiency or anti-IL-17 treatment, whereas the liver pathology was unaffected. Our findings demonstrate that an increased frequency of Th17 cells is associated with bone resorption in PSC. Whether antibody-based IL-17 blockade is beneficial against bone loss in patients with PSC should be addressed in future studies. Primary sclerosing cholangitis (PSC) is a severe idiopathic disease characterized by the progressive fibrosis of intrahepatic and extrahepatic bile ducts with a median age at onset of between 30–40 years.1–3 In contrast to primary biliary cholangitis (PBC), which predominantly affects middle-aged or elderly women,2 approximately 60% of PSC patients additionally develop colitis. Genetic variations associated with PSC can involve genes related to the immune system (human leukocyte antigen) and pathways that mediate inflammation. These associations are clearly different from those of ulcerative colitis.3 In addition to genetic risk factors, environmental influences also play important roles in the development of biliary inflammation and fibrosis.4 In this context, changes in microbial flora in the gut and subsequent activation of the immune system may be an important driving factor.5 In fact, recent studies have shown a clear difference in the gut microbiome between patients with PSC and healthy controls.6 We have previously shown that the bile fluid of patients with PSC is frequently colonized with different pathogens. Moreover, patients with PSC displayed a higher Th17 cell frequency after the stimulation of peripheral blood mononuclear cells with these pathogens, suggesting an important role of Th17 cells in the pathogenesis of PSC.7 However, the impact of microbial alterations on immune dysregulation and the perpetuation of biliary inflammation remain to be determined. 
In phase III studies, the fixed dose combination of sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) administered for 12 weeks led to a sustained virologic response at 12 weeks (SVR12) in 96% of NS5A inhibitor-experienced patients, and an SVR12 rate of 98% in DAA-experienced patients who had not previously received an NS5A inhibitor. Herein, we evaluate the relationship between the presence of detectable resistance-associated substitutions (RASs) at baseline and treatment outcome, and whether RASs were selected for in cases of virologic failure. NS3, NS5A, and NS5B deep sequencing analyses were performed at baseline for all patients and at the time of virologic failure. Results are reported using a 15% cut-off. A total of 82.7% of NS5A inhibitor-experienced patients (205/248) had baseline NS3 and/or NS5A RASs; 79% had baseline NS5A RASs. SVR12 rates were similar in patients with or without NS3 and/or NS5A RASs, and with or without VOX- or VEL-specific RASs. RASs at NS5A position Y93 were present in 37.3% of patients and 95% achieved SVR12. All patients with ≥2 NS5A RASs achieved SVR12. Baseline NS3 and/or NS5A RASs were present in 46.6% (83/178) of non-NS5A inhibitor DAA-experienced patients, all of whom achieved SVR12. All patients with baseline NS5B nucleoside inhibitor RASs, including two patients with S282T, achieved SVR12. Treatment-selected resistance was seen in one of seven patients who relapsed. Baseline RASs had no impact on virologic response in DAA-experienced patients following treatment with SOF/VEL/VOX for 12 weeks. Selection of viral resistance with virologic relapse was uncommon. Chronic hepatitis C virus infection (HCV) is a global health problem causing death and morbidity.1 Recent studies have shown that the global prevalence of HCV is estimated to be 1% in 2015, corresponding to 71.1 million individuals with chronic HCV infection.2,3 The disease burden of HCV infection is due to progression of chronic liver disease, which can lead to cirrhosis, liver failure, hepatocellular carcinoma (HCC), and death. Globally, 27% of all cases of cirrhosis and 25% of all HCC is attributable to HCV infection.4 With the introduction of direct-acting antiviral agents (DAAs), highly effective regimens with sustained virologic response (SVR) rates of >90% are now available for most patients with HCV infection.5–7 Despite high SVR rates following DAA treatment in both clinical trials and “real-world” cohorts, there is a growing population of patients who fail DAA-based therapies and have limited approved retreatment options.8–13 These patients, who represent the majority of recent treatment failures, are of particular concern because the resistance-associated substitutions (RASs) that are selected by NS5A inhibitors maintain viral fitness long after the end of the failed treatment.14,15 It has been shown that patients with genotype 1 (GT1) HCV infection who failed 8 or 12 weeks of treatment with the fixed dose combination of ledipasvir/sofosbuvir (LDV/SOF) and were retreated with LDV/SOF for 24 weeks had an SVR rate of 71%.16 Patients who failed treatment with SOF and VEL, and were retreated with SOF/VEL+ribavirin for 24 weeks had an overall SVR rate of 91%, with an SVR rate of 76% for a subset of patients with GT3 HCV infection.17 
The effect of hepatocellular carcinoma (HCC) on the response to interferon-free direct-acting antiviral (DAA) therapy in patients with chronic hepatitis C (CHC) infection remains unclear. Using a systematic review and meta-analysis approach, we aimed to investigate the effect of DAA therapy on sustained virologic response (SVR) among patients with CHC and either active, inactive or no HCC. PubMed, Embase, Web of Science, and the Cochrane Central Register of Controlled Trials were searched from 1/1/2013 to 9/24/2018. The pooled SVR rates were computed using DerSimonian-Laird random-effects models. We included 49 studies from 15 countries, comprised of 3,341 patients with HCC and 35,701 without HCC. Overall, the pooled SVR was lower in patients with HCC than in those without HCC (89.6%, 95% CI 86.8–92.1%, I2 = 79.1% vs. 93.3%, 95% CI 91.9–94.7%, I2 = 95.0%, p = 0.0012), translating to a 4.8% (95% CI 0.2–7.4%) SVR reduction by meta-regression analysis. The largest SVR reduction (18.8%) occurred in patients with active/residual HCC vs. inactive/ablated HCC (SVR 73.1% vs. 92.6%, p = 0.002). Meanwhile, patients with HCC who received a prior liver transplant had higher SVR rates than those who did not (p <0.001). Regarding specific DAA regimens, patients with HCC treated with ledipasvir/sofosbuvir had lower SVR rates than patients without HCC (92.6%, n = 884 vs. 97.8%, n = 13,141, p = 0.026), but heterogeneity was high (I2 = 84.7%, p <0.001). The SVR rate was similar in patients with/without HCC who were treated with ombitasvir/paritaprevir/ritonavir ± dasabuvir (n = 101) (97.2% vs. 94.8%, p = 0.79), or daclatasvir/asunaprevir (91.7% vs. 89.8%, p = 0.66). Overall, SVR rates were lower in patients with HCC, especially with active HCC, compared to those without HCC, though heterogeneity was high. Continued efforts are needed to aggressively screen, diagnose, and treat HCC to ensure higher CHC cure rates. Chronic hepatitis C virus (HCV) infection affected an estimated 71.1 million patients worldwide in 2015 and is a leading cause of liver cirrhosis and hepatocellular carcinoma (HCC).1 Among patients who have undergone treatment with curative intent for HCC, early hepatic decompensation and HCC recurrence were the major drivers of mortality.2 In recent studies of chronic hepatitis B-related HCC, antiviral therapy was shown to significantly reduce overall long-term mortality even in patients with very advanced HCC or decompensated cirrhosis, including those who were only receiving palliative treatment for HCC.3–6 Prior to the advent of interferon (IFN)-free direct-acting antiviral (DAA) therapy, patients with HCV-related HCC were often excluded from anti-HCV therapy as they tended to be older and had multiple non-liver and liver comorbidities, many of which rendered them unsuitable candidates for IFN-based therapy. Since 2014, many of these patients with HCC became treatment candidates for their chronic hepatitis C (CHC), despite the presence of advanced liver disease and comorbidities, as DAA therapy is not only highly efficacious but well tolerated.7 Individual real-world studies to date have included patients with HCC from both the East and West, and some have reported significantly lower cure rates.8–14 However, most studies had small sample sizes and heterogeneous patient demographic and clinical characteristics. 
An optimal allocation system for scarce resources should simultaneously ensure maximal utility, but also equity. The most frequent principles for allocation policies in liver transplantation are therefore criteria that rely on pre-transplant survival (sickest first policy), post-transplant survival (utility), or on their combination (benefit). However, large differences exist between centers and countries for ethical and legislative reasons. The aim of this study was to report the current worldwide practice of liver graft allocation and discuss respective advantages and disadvantages. Countries around the world that perform 95 or more deceased donor liver transplantations per year were analyzed for donation and allocation policies, as well as recipient characteristics. Most countries use the model for end-stage liver disease (MELD) score, or variations of it, for organ allocation, while some countries opt for center-based allocation systems based on their specific requirements, and some countries combine both a MELD and center-based approach. Both the MELD and center-specific allocation systems have inherent limitations. For example, most countries or allocation systems address the limitations of the MELD system by adding extra points to recipient’s laboratory scores based on clinical information. It is also clear from this study that cancer, as an indication for liver transplantation, requires special attention. The sickest first policy is the most reasonable basis for the allocation of liver grafts. While MELD is currently the standard for this model, many adjustments were implemented in most countries. A future globally applicable strategy should combine donor and recipient factors, predicting probability of death on the waiting list, post-transplant survival and morbidity, and perhaps costs. Liver transplantation (LT) has been undoubtedly one of the most successful procedures developed in the late 20th century, and as a consequence allocation of scarce liver grafts has caused many controversies (Figs. 1, 2).1 In the early stages of the procedure, from the 1980s until the mid-1990s, liver grafts were prioritized in the USA based on the degree of sickness and localization of the patients in the hospital.2 For example, candidates admitted to an intensive care unit (ICU) received the highest priority, ahead of patients hospitalized in a non-ICU setting and outpatients, somewhat independently of their accumulated waiting time.3 This policy carried the obvious risk of spoiling the system by forcing competing centers to keep the candidates on the ICU in order to get priority, when an organ became available. Next to the location of the patients, listing time was an important variable; patients listed early in a compensated stage of liver disease could gain much priority.4 As a consequence, a minimal listing criterion was introduced based on the Child-Turcotte-Pugh (CTP) score with a minimum of 7 out of 15 points to qualify for listing.5 The introduction of this additional criterion, however, did not reduce the number of listed candidates because waiting time remained the most important recipient variable for organ allocation, until Freeman et al. reported a lack of correlation between waiting time and waiting list mortality.6 This led to a change in the paradigm of organ allocation as waiting time ceased to be a key criterion.7 
Liver transplantation (LT) is the most effective treatment for patients with acute liver failure (ALF) though with the limitations of surgical risks and the need for life-long immunosuppression. Transplantation of microencapsulated human hepatocytes in alginate is an attractive option over whole liver replacement. The safety and efficacy of hepatocyte microbeads transplanted intraperitoneally has been shown in animal models. We report our experience of the use of this therapy in children with ALF on named patient basis under a MHRA Specials Manufacture License. Clinical grade human hepatocyte microbeads (HMBs) and empty microbeads (EMBs) were tested in immunocompetent healthy rats. Subsequently, eight children with ALF, who were awaiting a suitable allograft for LT, received intraperitoneal transplantation of HMBs. We monitored complications of the procedure, assessed the host immune response and residual function of the retrieved HMBs either after spontaneous native liver regeneration or at the time of LT. Intraperitoneal transplantation of HMBs in healthy rats was safe with preserved synthetic and detoxification functions without the use of immunosuppression. Subsequently, eight children with ALF received HMBs (4 neonatal haemochromatosis, 2 viral infections and 2 children with unknown cause at time of infusion), median age of 14.5 days, range 1 day - 6 years. The procedure was well tolerated without complications. Of the 8 children, 4 avoided LT while 3 were successfully bridged to LT following the intervention. HMBs retrieved after infusions (at the time of LT) were structurally intact, free of host cell adherence and contained viable hepatocytes with preserved functions. The results demonstrate the feasibility and safety of a HMB infusion in children with ALF.  
/Aims: HCV reinfection following successful treatment can compromise treatment outcome. This systematic review assessed the rate of HCV reinfection following treatment among people with recent drug use and those receiving opioid agonist therapy (OAT). Bibliographic databases and conference abstracts were searched for studies assessing post-treatment HCV reinfection rate among people with recent drug use (injecting or non-injecting) or those receiving OAT. Meta-analysis was used to cumulate reinfection rates and meta-regression to explore heterogeneity. Thirty-six studies were included (person-years follow-up=6,311). The overall rate of HCV reinfection was 5.9/100 person-years (95%CI: 4.1-8.5) among people with recent drug use (injecting or non-injecting), 6.2/100 person-years (95%CI: 4.3-9.0) among people recently injecting drugs, and 3.8/100 person-years (95%CI: 2.5-5.8) among those receiving OAT. Reinfection rates were comparable following interferon-based (5.4/100 person-years; 95%CI: 3.1-9.5), and direct-acting antiviral therapy (3.9/100 person-years; 95%CI: 2.5-5.9). In stratified analysis, reinfection rate was 1.4/100 person-years (95%CI: 0.8-2.6) among people receiving OAT with no recent drug use, 5.9/100 person-years (95%CI: 4.0-8.6) among people receiving OAT with recent drug use, and 6.6/100 person-years (95%CI: 3.4-12.7) among people with recent drug use, not receiving OAT. In meta-regression analysis, longer follow-up was associated with lower reinfection rate [adjusted Rate Ratio (aRR) per year increase in mean/median follow-up: 0.77, 95%CI: 0.69-0.86]. Compared with people receiving OAT with no recent drug use, those with recent drug use, receiving OAT (aRR: 3.50, 95%CI: 1.62-7.53), and those with recent drug use, not receiving OAT (aRR: 3.96, 95%CI: 1.82-8.59) had higher reinfection rates. HCV reinfection risk following treatment increased among people with recent drug use compared to those receiving OAT. Lower rates in studies with longer follow-up suggested higher reinfection risk early post-treatment.  
The therapeutic outcomes of surgical resection (SR) or radiofrequency ablation (RFA) for perivascular hepatocellular carcinoma (HCC) have not been compared. The aim of this study was to compare SR with RFA as first-line treatment in patients with perivascular HCC and to evaluate the long-term outcomes of both therapies. This retrospective study was approved by the institutional review board. The requirement for informed consent was waived. Between January 2006 and December 2010, a total of 283 consecutive patients with small perivascular HCCs (≤3 cm, Barcelona Clinic Liver Cancer stage 0 or A) underwent SR (n = 182) or RFA (n = 101) as a first-line treatment. The progression-free survival (PFS) and overall survival (OS) rates were compared by propensity score matching. Subgroup analysis of these outcomes was conducted according to the type of hepatic vessels. The median follow-up was 7.8 years. Matching yielded 62 pairs of patients. In the two matched groups, the PFS rates at 5 and 10 years were 58.0% and 17.8%, respectively, in the SR group, and 25.4% and 14.1%, respectively, in the RFA group (p <0.001). The corresponding OS rates at 5 and 10 years were 93.5% and 91.9% in the SR group and 82.3% and 74.1% in the RFA group, respectively (p <0.001). In contrast to those in patients with perivenous HCCs, subgroup analysis indicated that extrahepatic recurrence and OS were significantly different according to the treatment modality in patients with periportal HCCs (p = 0.004 and p <0.001, respectively). In patients with small perivascular HCCs, SR provided better long-term tumor control and OS than RFA, particularly for periportal tumors. Recent clinical guidelines for the management of hepatocellular carcinoma (HCC) have indicated that liver transplantation, surgical resection (SR), and radiofrequency ablation (RFA) are curative treatment modalities for very early or early stage HCC.1,2 Although the best therapy is liver transplantation, owing to the scarcity of donor organs, high cost, and longer waiting period of transplantation, SR and RFA have been mainstays for the curative treatment of HCC in clinical practice.3 Many studies compared the therapeutic efficacy of these two therapies, and most of these studies demonstrated that the efficacy of RFA and SR in terms of survival outcomes was similar for a single small HCC of ≤3 cm.4–7 
MUC13 is reportedly overexpressed in human malignancies. However, the clinicopathological and biological significance of MUC13 in human intrahepatic cholangiocarcinoma (iCCA) remain unclear. The aim of this study was to define the role of MUC13 in the progression of iCCA. Expression levels of MUC13 in human iCCA samples were evaluated by immunohistochemistry, western blot, and real-time PCR. In vitro and in vivo experiments were used to assess the effect of MUC13 on iCCA cell growth and metastasis. Crosstalk between MUC13 and EGFR/PI3K/AKT signaling was analyzed by molecular methods. The upstream regulatory effects of MUC13 were evaluated by Luciferase and DNA methylation assays. MUC13 was overexpressed in human iCCA specimens and iCCA cells. MUC13 overexpression positively correlated with clinicopathological characteristics of iCCA, such as vascular invasion and lymph node metastasis, and was independently associated with poor survival. Results from loss-of-function and gain-of-function experiments suggested that knockdown of MUC13 attenuated, while overexpression of MUC13 enhanced, the proliferation, motility, and invasiveness of iCCA cells in vitro and in vivo. Mechanistically, we found that the phosphatidylinositol 3-kinase-AKT signal pathway and its downstream effectors, such as tissue inhibitor of metalloproteinases 1 and matrix metallopeptidase 9, were required for MUC13-mediated tumor metastasis of iCCA. MUC13 interacted with EGFR and subsequently activated the EGFR/PI3K/AKT signal pathway by promoting EGFR dimerization and preventing EGFR internalization. We also found that MUC13 was directly regulated by miR-212-3p, whose downregulation was related to aberrant CpG hypermethylation in the promoter area. These findings suggest that aberrant hypermethylation-induced downregulation of miR-212-3p results in overexpression of MUC13 in iCCA, leading to metastasis via activation of the EGFR/PI3K/AKT signaling pathway.  
Hepatitis C virus (HCV) infection contributes to the development of autoimmune disorders such as cryoglobulinaemia vasculitis (CV). However, it remains unclear why only some individuals with HCV develop HCV-associated CV (HCV-CV). HCV-CV is characterized by the expansion of anergic CD19+CD27+CD21low/− atypical memory B cells (AtMs). Herein, we report the mechanisms by which AtMs participate in HCV-associated autoimmunity. The phenotype and function of peripheral AtMs were studied by multicolour flow cytometry and co-culture assays with effector T cells and regulatory T cells in 20 patients with HCV-CV, 10 chronically HCV-infected patients without CV and 8 healthy donors. We performed gene expression profile analysis of AtMs stimulated or not by TLR9. Immunoglobulin gene repertoire and antibody reactivity profiles of AtM-expressing IgM antibodies were analysed following single B cell FACS sorting and expression-cloning of monoclonal antibodies. The Tbet+CD11c+CD27+CD21− AtM population is expanded in patients with HCV-CV compared to HCV controls without CV. TLR9 activation of AtMs induces a specific transcriptional signature centred on TNFα overexpression, and an enhanced secretion of TNFα and rheumatoid factor-type IgMs in patients with HCV-CV. AtMs stimulated through TLR9 promote type 1 effector T cell activation and reduce the proliferation of CD4+CD25hiCD127−/lowFoxP3+ regulatory T cells. AtM expansions display intraclonal diversity with immunoglobulin features of antigen-driven maturation. AtM-derived IgM monoclonal antibodies do not react against ubiquitous autoantigens or HCV antigens including NS3 and E2 proteins. Rather, AtM-derived antibodies possess rheumatoid factor activity and target unique epitopes on the human IgG-Fc region. Our data strongly suggest a central role for TLR9 activation of AtMs in driving HCV-CV autoimmunity through rheumatoid factor production and type 1 T cell responses. Chronic HCV infection is associated with extrahepatic complications that are largely immunologically driven. Among those, cryoglobulinaemia and its clinical sequelae hold the strongest association. Cryoglobulins are readily detectable in 40–60% of HCV-infected patients,1–3 whereas cryoglobulinaemia vasculitis (CV) develops in only 5–10% of the cases.4,5 The presence of autoantibodies and T cells in vascular infiltrates as well as the observation that specific HLA alleles confer susceptibility to CV in HCV-infected patients support the autoimmune nature of this virus-associated pathology.6,7 CV pathophysiology depends on the interaction between HCV and lymphocytes that directly modulate B- and T cell function, which ultimately leads to the polyclonal activation and expansion of B cells producing rheumatoid factors (RFs).1,8 We previously reported abnormal immune responses mediated by T cells in patients with HCV-associated cryoglobulinaemia vasculitis (HCV-CV), with a quantitative defect in regulatory T cells (Tregs),6 and a Th1 polarization.9,10 HCV infection has been associated with lymphoproliferations, which likely result from an indirect process following the chronic antigenic stimulation of a limited pool of pre-existing autoreactive B cells. It has been proposed that persistently high levels of HCV-containing immune complexes stimulate the proliferation of RF-bearing B cells, but the precise antigen(s) and stimulatory mechanisms have remained elusive. Our group and others previously identified a clonal expansion of CD27+IgM+CD21low/− memory B cells, referred to as activated or atypical memory B cells (AtMs), in HCV-CV.11,12 AtMs are clonal or clonally related, and mainly express the VH1-69 IgH gene, which is also highly prevalent in HCV-associated lymphoproliferations.11–13 These clonal cells express reduced levels of the complement receptor 2, CD21, which mirrors an anergic state.14,15 AtMs are prone to undergoing apoptosis.11,13 They do not proliferate upon BCR stimulation but respond to the TLR9 agonist CpG by expressing activator and proliferative markers.11,13 Anergy is a well-known regulatory mechanism for maintaining immune tolerance of autoreactive cells.14,15 Indeed, AtMs in HCV-CV produced somatically mutated RF autoantibodies,16 and are not removed from the B cell repertoire. However, it remains unclear why only some HCV-infected individuals develop CV, and why anergic mechanisms fail to prevent the development of non-Hodgkin’s lymphoma in some patients with HCV-CV. 
Mitochondrial dysfunction and subsequent metabolic deregulation are commonly observed in cancers including hepatocellular carcinoma (HCC). When mitochondrial function is impaired, reductive glutamine metabolism is a major cellular carbon source for de novo lipogenesis to support cancer cell growth. The underlying regulators of reductively metabolized glutamine in mitochondrial dysfunction are not completely understood in tumorigenesis including in HCC. We systematically investigated the role of oxoglutarate dehydrogenase-like (OGDHL), one of the rate-limiting components of the key mitochondrial multi-enzyme OGDH complex (OGDHC), in the regulation of lipid metabolism in hepatoma cells and explored the underlying molecular mechanisms. Lower expression of OGDHL was associated with advanced tumor stage, significantly worse survival and more frequent tumor recurrence in three independent cohorts totaling 681 postoperative HCC patients. Promoter hypermethylation and DNA copy deletion of OGDHL were independently correlated with reduced OGDHL expression in HCC specimens. Additionally, OGDHL overexpression significantly inhibited the growth of hepatoma cells as mouse xenografts while knockdown of OGDHL promoted proliferation in hepatoma cells. Mechanistically, OGDHL downregulation upregulated the α-ketoglutarate (αKG):citrate ratio by reducing OGDHC activity, which subsequently drove reductive carboxylation (RC) of glutamine-derived αKG for lipogenesis via retrograde TCA cycling in hepatoma cells. Notably, silencing of OGDHL activated the mTORC1 signaling pathway in an α-KG-dependent manner, which in turn transcriptionally induced expression of SCD1 and FASN, thus, enhancing de novo lipogenesis. Meanwhile, metabolic reprogramming in OGDHL-negative hepatoma cells provided an abundant supply of NADPH and GSH to support the cellular antioxidant system. The reduction of reductive glutamine metabolism through OGDHL overexpression or through use of glutaminase inhibitors sensitized tumor cells to sorafenib, a molecular-targeted therapy for HCC. Our findings established that silencing of OGDHL contributed to HCC development and survival by regulating glutamine metabolic pathways, and suggest OGDHL as a promising prognostic biomarker and therapeutic target for HCC. Hepatocellular carcinoma (HCC) is one of the most common malignancies worldwide, which has increased morbidity and mortality of patients with liver disorders [1]. Therefore, novel causative molecular biomarkers need to be identified to improve clinical decision-making and therapy. Cancer cells undergo huge metabolic alterations to compete for nutrient-limited resources with surrounding normal cells in order to maintain rapid growth; therefore; understanding these changes is fundamental for identifying potential biomarkers and therapeutic targets in HCC [2]. 
Patients with decompensated cirrhosis on the waiting list for liver transplantation (LT) commonly develop complications that may preclude them from reaching LT. Circulatory dysfunction leading to effective arterial hypovolemia and activation of vasoconstrictor systems is a key factor in the pathophysiology of complications of cirrhosis. The aim of this study was to investigate whether treatment with midodrine, an alpha-adrenergic vasoconstrictor, together with intravenous albumin improves circulatory dysfunction and prevents complications of cirrhosis in patients awaiting LT. A multicenter, randomized, double-blind, placebo-controlled trial (NCT00839358) was conducted, including 196 consecutive patients with cirrhosis and ascites awaiting LT. Patients were randomly assigned to receive midodrine (15–30 mg/day) and albumin (40 g/15 days) or matching placebos for one year, until LT or drop-off from inclusion on the waiting list. The primary endpoint was incidence of any complication (renal failure, hyponatremia, infections, hepatic encephalopathy or gastrointestinal bleeding). Secondary endpoints were mortality, activity of endogenous vasoconstrictor systems and plasma cytokine levels. There were no significant differences between both groups in the probability of developing complications of cirrhosis during follow-up (p = 0.402) or one-year mortality (p = 0.527). Treatment with midodrine and albumin was associated with a slight but significant decrease in plasma renin activity and aldosterone compared to placebo (renin −4.3 vs. 0.1 ng/ml.h, p < 0.001; aldosterone −38 vs. 6 ng/dl, p = 0.02, at week 48 vs. baseline). Plasma norepinephrine only decreased slightly at week 4. Neither arterial pressure nor plasma cytokine levels changed significantly. In patients with cirrhosis awaiting LT, treatment with midodrine and albumin, at the doses used in this study, slightly suppressed the activity of vasoconstrictor systems, but did not prevent complications of cirrhosis or improve survival. The natural history of decompensated cirrhosis is characterized by the development of recurrent complications of the disease, leading to frequent hospital admissions, impaired quality of life and increased mortality.1–5 Liver transplantation (LT) is the definitive treatment for patients with end-stage liver disease. Patients with decompensated cirrhosis achieve good outcomes after LT with three-year survival above 80%.5–8 However, one of the major concerns is the development of complications while on the waiting list, which may be a cause of death or may delay or preclude patients with decompensated cirrhosis from receiving LT.9–12 
The development of hepatic models capable of long-term expansion with competent liver functionality is technically challenging in a personalized setting. Stem cell-based organoid technologies can provide an alternative source of patient-derived primary hepatocytes. However, self-renewing and functionally competent human pluripotent stem cell (PSC)-derived hepatic organoids have not been developed. We developed a novel method to efficiently and reproducibly generate functionally mature human hepatic organoids derived from PSCs, including human embryonic stem cells and induced PSCs. The maturity of the organoids was validated by a detailed transcriptome analysis and functional performance assays. The organoids were applied to screening platforms for the prediction of toxicity and the evaluation of drugs that target hepatic steatosis through real-time monitoring of cellular bioenergetics and high-content analyses. Our organoids were morphologically indistinguishable from adult liver tissue-derived epithelial organoids and exhibited self-renewal. With further maturation, their molecular features approximated those of liver tissue, although these features were lacking in 2D differentiated hepatocytes. Our organoids preserved mature liver properties, including serum protein production, drug metabolism and detoxifying functions, active mitochondrial bioenergetics, and regenerative and inflammatory responses. The organoids exhibited significant toxic responses to clinically relevant concentrations of drugs that had been withdrawn from the market due to hepatotoxicity and recapitulated human disease phenotypes such as hepatic steatosis. Our organoids exhibit self-renewal (expandable and further able to differentiate) while maintaining their mature hepatic characteristics over long-term culture. These organoids may provide a versatile and valuable platform for physiologically and pathologically relevant hepatic models in the context of personalized medicine. Human cell-based and personalized in vitro liver models are urgently needed for drug efficacy and toxicity tests in pre-clinical drug development. Although the liver is a representative organ with a native regenerative potential in vivo, primary human hepatocytes (PHHs), which are considered the gold standard for evaluating hepatic metabolism, are limited by their loss of proliferative capacity and long-term functionality in vitro.1 To overcome the limitations of PHHs, various approaches have been developed, including genetic modification,2 3D culture combined with tissue engineering technologies,1,3–7 and defined medium compositions.8,9 However, the development of alternative and sustainable cell sources to recapitulate the function of native liver remains challenging. 
The effectiveness of direct-acting antivirals (DAAs) against hepatitis C virus (HCV), following successful treatment of early hepatocellular carcinoma (HCC), has been studied extensively. However, the benefit in terms of overall survival (OS) remains to be conclusively demonstrated. The aim of this study was to assess the impact of DAAs on OS, HCC recurrence, and hepatic decompensation. We prospectively enrolled 163 consecutive patients with HCV-related cirrhosis and a first diagnosis of early Barcelona Clinic Liver Cancer stage 0/A HCC, who had achieved a complete radiologic response after curative resection or ablation and were subsequently treated with DAAs. DAA-untreated patients from the ITA.LI.CA. cohort (n = 328) served as controls. After propensity score matching, outcomes of 102 DAA-treated (DAA group) and 102 DAA-untreated patients (No DAA group) were compared. In the DAA group, 7/102 patients (6.9%) died, HCC recurred in 28/102 patients (27.5%) and hepatic decompensation occurred in 6/102 patients (5.9%), after a mean follow-up of 21.4 months. OS was significantly higher in the DAA group compared to the No DAA group (hazard ratio [HR] 0.39; 95% CI 0.17–0.91; p = 0.03). HCC recurrence was not significantly different between the DAA and No DAA groups (HR 0.70; 95% CI 0.44–1.13; p = 0.15). A significant reduction in the rate of hepatic decompensation was observed in the DAA group compared with the No DAA group (HR 0.32; 95% CI 0.13–0.84; p = 0.02). In the DAA group, sustained virologic response was a significant predictor of OS (HR 0.02; 95% CI 0.00–0.19; p <0.001), HCC recurrence (HR 0.25; 95% CI 0.11–0.57; p <0.001) and hepatic decompensation (HR 0.12; 95% CI 0.02–0.38; p = 0.02). In patients with HCV-related cirrhosis who had been successfully treated for early HCC, DAAs significantly improved OS compared with No DAA treatment. Hepatocellular carcinoma (HCC) is the third leading cause of cancer-related death globally, and the leading cause of mortality in cirrhotic patients, with hepatitis C virus (HCV) being the major risk factor in the Western world and Japan.1 Orthotopic liver transplantation (OLT) is the definitive treatment for HCC and cirrhotic liver, but this approach cannot be offered to all patients due to limited graft availability and rigorous selection criteria.2 Alternative curative treatment options for patients with compensated cirrhosis are surgical resection and loco-regional ablation of early HCC (i.e. Barcelona Clinic Liver Cancer [BCLC] stage 0/A).2 
C-C motif chemokine receptor 2 (CCR2) has been recognized as a promising target for the treatment of liver fibrosis. PC3-secreted microprotein (PSMP)/microseminoprotein (MSMP) is a novel chemotactic cytokine and its receptor is CCR2. In the present study we investigated the expression and role of PSMP in liver fibrosis/cirrhosis. PSMP expression was studied in patients with fibrosis/cirrhosis and in 3 murine models of liver fibrosis, including mice treated with carbon tetrachloride (CCl4), bile-duct ligation, or a 5-diethoxycarbonyl-1,4-dihydrocollidine diet. The role of PSMP was evaluated in Psmp-/- mice and after treatment with a PSMP antibody in wild-type mice. The direct effects of PSMP on macrophages and hepatic stellate cells were studied in vitro. In this study, we found that PSMP was highly expressed in fibrotic/cirrhotic tissues from patients with different etiologies of liver disease and in the 3 experimental mouse models of fibrosis. Damage-associated molecular pattern molecules HMGB-1 and IL-33 induced hepatocytes to produce PSMP. PSMP deficiency resulted in a marked amelioration of hepatic injury and fibrosis. In CCl4-induced hepatic injury, the infiltration of macrophages and CCR2+ monocytes into the liver was significantly decreased in Psmp-/- mice. Consistent with the decreased levels of intrahepatic macrophages, proinflammatory cytokines were significantly reduced. Moreover, adeno-associated virus-8 vectors successfully overexpressing human PSMP in Psmp-/- mouse livers could reverse the attenuation of liver injury and fibrosis induced by CCl4 in a CCR2-dependent manner. Treatment with a specific PSMP-neutralizing antibody, 3D5, prevented liver injury and fibrosis induced by CCl4 in mice. At the cellular level, PSMP directly promoted M1 polarization of macrophages and activation of LX-2 cells. PSMP enhances liver fibrosis through its receptor, CCR2. PSMP is a potentially attractive therapeutic target for the treatment of patients with liver fibrosis. Liver fibrosis, a wound-healing response to chronic liver injury, is characterized by excessive deposition of extracellular matrix (ECM) in the liver and is triggered by a variety of causes, including hepatitis virus infection, alcohol abuse, cholestasis, autoimmune, drug/toxin and non-alcoholic steatohepatitis (NASH), which eventually lead to loss of liver function and disruption of the liver structure.1,2 The initiation of fibrosis crucially depends on an inflammatory phase in which liver resident macrophages, Kupffer cells, are activated and release transforming growth factor-β (TGF-β), as well as other proinflammatory cytokines that activate hepatic stellate cells (HSCs).3–7 HSCs are responsible for producing most of the ECM and play a central role in liver fibrogenesis.8,9 HSCs are quiescent and located in the space between hepatocytes and sinusoidal endothelium (space of Disse) as retinoid storage cells.10 Upon liver injury, HSCs, the major collagen-synthesizing cells in the liver, are activated and transdifferentiate into myofibroblast-like cells, which show enhanced proliferation, chemotaxis, survival and collagen production.8,9,11,12 HSC activation is driven by multiple mediators, such as chemokines, reactive oxygen species, growth factors, matrix stiffness, matricellular proteins and damage-associated molecular patterns (DAMPs).5,8,13 Currently, there are no approved drugs that can effectively reverse liver fibrosis, further highlighting the urgent clinical need for novel antifibrotic therapies.14 
Embedded into a complex signaling network that coordinates glucose uptake, usage and production, the nuclear bile acid receptor FXR is expressed in several glucose-processing organs including the liver. Hepatic gluconeogenesis is controlled through allosteric regulation of gluconeogenic enzymes and by glucagon/cAMP-dependent transcriptional regulatory pathways. We aimed to elucidate the role of FXR in the regulation of fasting hepatic gluconeogenesis. The role of FXR in hepatic gluconeogenesis was assessed in vivo and in mouse primary hepatocytes. Gene expression patterns in response to glucagon and FXR agonists were characterized by quantitative reverse transcription PCR and microarray analysis. FXR phosphorylation by protein kinase A was determined by mass spectrometry. The interaction of FOXA2 with FXR was identified by cistromic approaches and in vitro protein-protein interaction assays. The functional impact of the crosstalk between FXR, the PKA and FOXA2 signaling pathways was assessed by site-directed mutagenesis, transactivation assays and restoration of FXR expression in FXR-deficient hepatocytes in which gene expression and glucose production were assessed. FXR positively regulates hepatic glucose production through two regulatory arms, the first one involving protein kinase A-mediated phosphorylation of FXR, which allowed for the synergistic activation of gluconeogenic genes by glucagon, agonist-activated FXR and CREB. The second arm involves the inhibition of FXR’s ability to induce the anti-gluconeogenic nuclear receptor SHP by the glucagon-activated FOXA2 transcription factor, which physically interacts with FXR. Additionally, knockdown of Foxa2 did not alter glucagon-induced and FXR agonist enhanced expression of gluconeogenic genes, suggesting that the PKA and FOXA2 pathways regulate distinct subsets of FXR responsive genes. Thus, hepatic glucose production is regulated during physiological fasting by FXR, which integrates the glucagon/cAMP signal and the FOXA2 signal, by being post-translationally modified, and by engaging in protein-protein interactions, respectively. Glucose supply to tissues is maintained through a complex regulatory network mostly driven by the pancreatic hormones insulin and glucagon which control glucose use, storage and synthesis. Through glycogenolysis and gluconeogenesis, the liver contributes to ∼70–80% of glucose production during an overnight fast,1 the remaining 30% coming from intestinal and kidney gluconeogenesis in physiological conditions.2,3 
Protease inhibitors (PIs) are of central importance in the treatment of patients with chronic hepatitis C virus (HCV) infection. HCV NS3 protease (NS3P) position 80 displays polymorphisms associated with resistance to the PI simeprevir for HCV genotype 1a. We investigated the effects of position-80-substitutions on fitness and PI-resistance for HCV genotypes 1-6, and analyzed evolutionary mechanisms underlying viral escape mediated by pre-existing Q80K. The fitness of infectious NS3P recombinants of HCV genotypes 1-6, with engineered position-80-substitutions, was studied by comparison of viral spread kinetics in Huh-7.5 cells in culture. Median effective concentration (EC50) and fold resistance for PIs simeprevir, asunaprevir, paritaprevir, grazoprevir, glecaprevir and voxilaprevir were determined in short-term treatment assays. Viral escape was studied by long-term treatment of genotype 1a recombinants with simeprevir, grazoprevir, glecaprevir and voxilaprevir and of genotype 3a recombinants with glecaprevir and voxilaprevir, next generation sequencing, NS3P substitution linkage and haplotype analysis. Among tested PIs, only glecaprevir and voxilaprevir showed pan-genotypic activity against the original genotype 1-6 culture viruses. Variants with position-80-substitutions were all viable, but fitness depended on the specific substitution and the HCV isolate. Q80K conferred resistance to simeprevir across genotypes but had only minor effects on the activity of the remaining PIs. For genotype 1a, pre-existing Q80K mediated accelerated escape from simeprevir, grazoprevir and to a lesser extent glecaprevir, but not voxilaprevir. For genotype 3a, Q80K mediated accelerated escape from glecaprevir and voxilaprevir. Escape was mediated by rapid and genotype-, PI- and PI-concentration-dependent co-selection of clinically relevant resistance associated substitutions. Position-80-substitutions had relatively low fitness cost and the potential to promote HCV escape from clinically relevant PIs in vitro, despite having a minor impact on results in classical short-term resistance assays. Regimens based on direct-acting antivirals (DAAs) have revolutionized treatment of patients with chronic infection with hepatitis C virus (HCV), which globally has been estimated to cause 70–150 million chronic infections and at least 400,000 deaths annually.1,2 Approved DAAs target the HCV protease (NS3P), NS5A and NS5B.3,4 Protease inhibitors (PIs), available since 2011, constitute an important component of DAA-based combination therapies.3–5 While the initially developed PIs telaprevir and boceprevir have been discontinued, simeprevir might be used for treatment of patients infected with genotype 1, and grazoprevir or paritaprevir for genotypes 1 and 4.6,7 Asunaprevir, approved in Asia and the Middle East, is used only for subtype 1b.3 The novel PIs glecaprevir and voxilaprevir are recommended for genotypes 1-6.6,7 
Acute-on-chronic liver failure (ACLF) is a syndrome of systemic inflammation and organ failures. Obesity, also characterized by chronic inflammation, is a risk factor among patients with cirrhosis for decompensation, infection, and mortality. Our aim was to test the hypothesis that obesity predisposes patients with decompensated cirrhosis to the development of ACLF. We examined the United Network for Organ Sharing (UNOS) database, from 2005–2016, characterizing patients at wait-listing as non-obese (body mass index [BMI] <30), obese class I-II (BMI 30–39.9) and obese class III (BMI ≥40). ACLF was determined based on the CANONIC study definition. We used Cox proportional hazards regression to assess the association between obesity and ACLF development at liver transplantation (LT). We confirmed our findings using the Nationwide Inpatient Sample (NIS), years 2009–2013, using validated diagnostic coding algorithms to identify obesity, hepatic decompensation and ACLF. Logistic regression evaluated the association between obesity and ACLF occurrence. Among 387,884 patient records of decompensated cirrhosis, 116,704 (30.1%) were identified as having ACLF in both databases. Multivariable modeling from the UNOS database revealed class III obesity to be an independent risk factor for ACLF at LT (hazard ratio 1.24; 95% CI 1.09–1.41; p <0.001). This finding was confirmed using the NIS (odds ratio 1.30; 95% CI 1.25–1.35; p <0.001). Regarding specific organ failures, analysis of both registries demonstrated patients with class I-II and class III obesity had a greater prevalence of renal failure. Class III obesity is a newly identified risk factor for ACLF development in patients with decompensated cirrhosis. Obese patients have a particularly high prevalence of renal failure as a component of ACLF. These findings have important implications regarding stratifying risk and preventing the occurrence of ACLF. Acute-on-chronic liver failure (ACLF) is a syndrome that occurs in patients with cirrhosis, characterized by acute hepatic decompensation, organ system failure, and 28-day mortality of greater than 15%.1 The pathophysiology of ACLF has not been fully elucidated, but appears to be a consequence of a dysregulated inflammatory response, resulting in rapidly evolving organ failure and mortality.2–6 The reported prevalence of ACLF among those hospitalized with decompensated cirrhosis approaches 30%1 and associated healthcare costs of ACLF are as high as $1.7 billion in the US.7 Considering the high prevalence of this condition, along with the associated mortality and healthcare burden, identifying modifiable risk factors for ACLF is of high importance. 
Alcoholic liver disease (ALD) is a major cause of morbidity and mortality worldwide. However, the cellular defense mechanisms underlying ALD are not well understood. Recent studies highlighted the involvement of chaperone-mediated autophagy (CMA) in regulating hepatic lipid metabolism. Sorting nexin (SNX)-10 has a regulatory function in endolysosomal trafficking and stabilisation. Here, we investigated the roles of SNX10 in CMA activation and in the pathogenesis of alcohol-induced liver injury and steatosis. Snx10 knockout (Snx10 KO) mice and their wild-type (WT) littermates fed either the Lieber–DeCarli liquid alcohol diet or a control liquid diet, and primary cultured WT and Snx10 KO hepatocytes stimulated with ethanol, were used as in vivo and in vitro ALD models, respectively. Activation of CMA, liver injury parameters, inflammatory cytokines, oxidative stress and lipid metabolism were measured. Compared with WT littermates, Snx10 KO mice exhibited a significant amelioration in ethanol-induced liver injury and hepatic steatosis. Both in vivo and in vitro studies showed that SNX10 deficiency upregulated lysosome-associated membrane protein type 2A (LAMP-2A) expression and CMA activation, which could be reversed by SNX10 overexpression in vitro. LAMP-2A interference confirmed that the upregulation of Nrf2 and AMPK signalling pathways induced by SNX10 deficiency relied on CMA activation. Pull-down assays revealed an interaction between SNX10 and cathepsin A (CTSA), a key enzyme involved in LAMP-2A degradation. Deficiency in SNX10 inhibited CTSA maturation and increased the stability of LAMP-2A, resulting in an increase in CMA activity. SNX10 controls CMA activity by mediating CTSA maturation, and, thus, has an essential role in alcohol-induced liver injury and steatosis. Our results provide evidence for SNX10 as a potential promising therapeutic target for preventing or ameliorating liver injury in ALD. Alcoholic liver disease (ALD) is a spectrum of disorders ranging from reversible steatosis to life-threatening and irreversible cirrhosis, and, in some patients, hepatocellular cancer.1 Alcohol exposure induces oxidative stress and mitochondrial dysfunction, which lead to hepatocyte apoptosis, necrosis, and necroptosis, as well as hepatic inflammation. In addition, ethanol consumption disrupts lipid metabolism and subsequently results in the accumulation of excessive triglycerides in the liver, further aggravating liver injury.2 Even though much research has focussed on understanding the pathological features of ALD, the cellular mechanisms that defend against the detrimental effects of alcohol remain unclear. Although therapeutic strategies, including alcohol abstinence, corticosteroids, biologics such as anti-tumour necrosis factor (TNF)-α, and liver transplantation have been used, these might not be always either practical or sufficient.3,4 Thus, there is an urgent need to develop specific therapies for ALD. 
Tenofovir disoproxil fumarate (TDF) monotherapy has displayed non-inferior efficacy to TDF plus entecavir (ETV) combination therapy in patients with hepatitis B virus (HBV) resistant to ETV and/or adefovir (ADV). Nonetheless, the virologic response rate was suboptimal in patients receiving up to 144 weeks of TDF monotherapy. We aimed to assess the efficacy and safety of TDF monotherapy given for up to 240 weeks. One trial enrolled patients with ETV resistance without ADV resistance (n = 90), and another trial included patients with ADV resistance (n = 102). Most patients (91.2%) also had lamivudine resistance. Patients were randomized 1:1 to receive TDF monotherapy or TDF + ETV combination therapy for 48 weeks, and then TDF monotherapy until week 240. We compared efficacy between the studies and safety in the pooled population at 240 weeks. At week 240, the proportion of patients with serum HBV DNA <15 IU/ml was not significantly different between the ETV and ADV resistance groups in the full analysis set (84.4% vs. 73.5%; p = 0.07), which was significantly different by on-treatment analysis (92.7% vs. 79.8%; p = 0.02). Virologic blips associated with poor medication adherence occurred in 7 patients throughout the 240 weeks. None developed additional HBV resistance mutations. Among the 170 HBV e antigen (HBeAg)-positive patients at baseline, 12 (7.1%) achieved HBeAg seroconversion at week 240. None achieved HBV surface antigen seroclearance. Significant decreases from baseline were observed at week 240 in the estimated glomerular filtration rate (−3.21 ml/min/1.73 m2 by the CKD-EPI equation, p <0.001) and bone mineral density (g/cm2) at the femur (−2.48%, p <0.001). Up to 240 weeks of TDF monotherapy provided an increasing virologic response rate in heavily pretreated patients with HBV resistant to ETV and/or ADV. However, it was associated with poor serological responses and decreasing renal function and bone mineral density. (ClinicalTrials.gov No, NCT01639066 and NCT01639092). Persistently high serum hepatitis B virus (HBV) DNA levels are an independent risk factor for disease progression to cirrhosis and hepatocellular carcinoma (HCC) in patients with chronic hepatitis B (CHB).1–3 Multiple studies have shown that long-term treatment with nucleos(t)ide analogue (NUC) therapy reduces the risk of mortality and HCC through inhibition of HBV replication.3–6 Nevertheless, functional cure of chronic HBV infection (HBV surface antigen [HBsAg] seroclearance) is very rarely achievable, necessitating almost life-long NUC therapy in most patients.7,8 
Hepatocyte polarity is essential for the development of bile canaliculi and for safely transporting bile and waste products from the liver. Functional studies of autologous mutated proteins in the context of the polarized hepatocyte have been challenging because of the lack of appropriate cell models. The aims of this study were to obtain a patient-specific hepatocyte model that recapitulated hepatocyte polarity and to employ this model to study endogenous mutant proteins in liver diseases that involve hepatocyte polarity. Urine cell-derived pluripotent stem cells, taken from a patient with a homozygous mutation in ATP7B and a patient with a heterozygous mutation, were differentiated towards hepatocyte-like cells (hiHeps). HiHeps were also derived from a patient with MEDNIK syndrome. Polarized hiHeps that formed in vivo-like bile canaliculi could be generated from embryonic and patient urine cell-derived pluripotent stem cells. HiHeps recapitulated polarized protein trafficking processes, exemplified by the Cu2+-induced redistribution of the copper transporter protein ATP7B to the bile canalicular domain. We demonstrated that, in contrast to the current dogma, the most frequent yet enigmatic Wilson disease-causing ATP7B-H1069Q mutation per se did not preclude trafficking of ATP7B to the trans-Golgi Network. Instead, it prevented its Cu2+-induced polarized redistribution to the bile canalicular domain, which could not be reversed by pharmacological folding chaperones. Finally, we demonstrate that hiHeps from a patient with MEDNIK syndrome, suffering from liver copper overload of unclear etiology, showed no defect in the Cu2+-induced redistribution of ATP7B to the bile canaliculi. Functional cell polarity can be achieved in patient pluripotent stem cell-derived hiHeps, enabling, for the first time, the study of the endogenous mutant proteins, patient-specific pathogenesis and drug responses for diseases where hepatocyte polarity is a key factor. Hepatocytes are polarized cells, exemplified by the segregation of their plasma membranes into basolateral/sinusoidal and apical/canalicular domains.1,2 Hepatocyte polarity is essential for many hepatocyte-specific functions.1 Not surprisingly therefore, loss of hepatocyte polarity is correlated with liver diseases.2 In inherited liver diseases, mutations in specific genes can cause a defect in the targeting, expression and/or function of proteins that display a steady-state residence at either sinusoidal or canalicular surface domains.2 
Non-alcoholic fatty liver disease (NAFLD) is the leading cause of chronic liver disease in adults and children. Along with obesity, diabetes and insulin resistance, genetic factors strongly impact on NAFLD development and progression. Dysregulated bile acid metabolism and the fibroblast growth factor 19 (FGF19) pathway play a pivotal role in NAFLD pathogenesis. However, the mechanism through which the FGF19 receptor system is associated with liver damage in NAFLD remains to be defined. We evaluated the impact of the rs17618244 G>A β-Klotho (KLB) variant on liver damage in 249 pediatric patients with biopsy-proven NAFLD and the association of this variant with the expression of hepatic and soluble KLB. In vitro models were established to investigate the role of the KLB mutant. The KLB rs17618244 variant was associated with an increased risk of ballooning and lobular inflammation. KLB plasma levels were lower in carriers of the rs17618244 minor A allele and were associated with lobular inflammation, ballooning and fibrosis. In HepG2 and Huh7 hepatoma cell lines, exposure to free fatty acids caused a severe reduction of intracellular and secreted KLB. Finally, KLB downregulation obtained by the expression of a KLB mutant in HepG2 and Huh7 cells induced intracellular lipid accumulation and upregulation of p62, ACOX1, ACSL1, IL-1β and TNF-α gene expression. In conclusion, we showed an association between the rs17618244 KLB variant, which leads to reduced KLB expression, and the severity of NAFLD in pediatric patients. We can speculate that the KLB protein may exert a protective role against lipotoxicity and inflammation in hepatocytes. Non-alcoholic fatty liver disease (NAFLD) has become the leading cause of liver damage worldwide.1 The histologic spectrum of NAFLD ranges from simple steatosis to non-alcoholic steatohepatitis (NASH), fibrosis, and eventually progress to cirrhosis and hepatocellular carcinoma (HCC).2,3 NAFLD is a multifactorial disease where environmental factors, such as an excessive caloric intake and a sedentary lifestyle, and genetic factors interact with each other, triggering the metabolic and hepatic events that lead to liver fat accumulation and progressive liver disease.4 In recent years, genome-wide association studies (GWAS) have identified inherited variants in genes involved in hepatic fat uptake, synthesis, storage and mobilization of triglycerides which have been associated with a higher risk of NAFLD in adults.5–7 Most of these genetic variants, such as the rs738409 C>G in patatin-like phospholipase domain–containing 3 (PNPLA3) gene and the rs58542926 C>T in transmembrane 6 superfamily member 2 (TM6SF2) gene, also increase the risk of NAFLD in pediatric patients.8–10 Due to their effect size these polymorphisms explain the genetic susceptibility to NAFLD development and progression in most individuals.11 However, other genetic variants may contribute to determine the motley pattern of histologic features associated with NAFLD and to explain the missing heritability.12 
Immunosuppressed patients with chronic hepatitis E virus infection (cHEV), who are ineligible or have failed current treatment with off-label ribavirin, are a potential target population for T cell-based therapy. T cell responses are important for viral control. Herein, we aimed to identify human leukocyte antigen (HLA)-A2 restricted HEV-specific CD8+ T cell epitopes and T cell receptors (TCR) targeting these epitopes, as the basis for a redirected TCR treatment approach for patients with cHEV. HEV genotype 3 overlapping peptide pools were used to screen HEV-specific CD8+ T cell immune responses in HLA-A2+ patients with acute HEV infection and healthy donors, by intracellular cytokine staining. CD8+ T cells targeting the identified epitopes were sorted for sequencing of the TCR repertoires by next generation sequencing. Messenger RNA encoding these TCRs were introduced into lymphocytes of healthy donors and patients with cHEV through TCR redirection. TCR-engineered lymphocytes were evaluated for Dextramer®-binding capacity, target sensitivity and cytotoxicity against peptide-loaded T2 cells. HEV-specific responses were observed across open reading frame (ORF)1 and ORF2 of the HEV genome in patients with acute resolving HEV infection. HLA-A2-restricted HEV-specific CD8+ T cell epitopes targeting the HEV RNA helicase and RNA-dependent RNA polymerase were selected for functional studies. Introduction of HEV-specific TCRs into lymphocytes of immunocompetent donors and patients with chronic hepatitis E enabled the lymphocytes to bind HEV Dextramers, secrete multiple cytokines and exert cytotoxicity in a target-specific manner. We identified TCRs that target HEV-specific CD8+ T cell epitopes, and characterized their immune properties, which may have clinical potential in future T cell-based therapy. Hepatitis E virus (HEV) infection is the most common cause of acute viral hepatitis in the global population. HEV is an emerging public health risk found in both the developed and developing countries.1,2 There are several genotypes of HEV but only one serotype. Genotypes 1 and 2 only infect humans, and are distributed mainly in endemic regions where poor hygiene is the main cause of transmission via the fecal-oral route. Genotypes 3 and 4 are zoonotic; their hosts include both humans and animals, and are found predominantly in urbanized countries. Genotype 3 virus is transmitted by consumption of raw or improperly cooked meat or viscera of infected animals,2 or through transfusion of contaminated blood products.3 Usually, patients with acute hepatitis E spontaneously clear the virus. However, immunosuppressed patients, such as solid organ transplant patients, may develop chronic hepatitis E in around 50% of cases.4,5 Reversion or reduction of the immunosuppressive status may induce spontaneous viral clearance.4–6 If this is not effective or possible, treatment choices are limited. There is no approved therapy for chronic hepatitis E, but interferon-α or ribavirin are off-label treatment options. Unfortunately, interferon-α is contraindicated in patients receiving lung, heart or kidney transplants as it can trigger graft rejection.6 Therefore, for these patients, ribavirin is their only option. However, side effects of ribavirin such as hemolytic anemia limit its use. Ribavirin is effective in approximately 80% of patients who are able to tolerate its use.7 For those who fail treatment, chronic hepatitis E can progress to liver cirrhosis.6,8 Additionally, a recent study has shown that ribavirin induces mutations in HEV, 9 leading to enhanced viral replication and eventually drug failure.10 
A total of 15% of patients with idiopathic non-cirrhotic portal hypertension (INCPH) are women of childbearing age. We aimed to determine maternal and fetal outcome of pregnancies occurring in women with INCPH. We retrospectively analyzed the charts of women with INCPH followed in the centers of the VALDIG network, having had ≥1 pregnancy during the follow-up of their liver disease. Data are represented as median (interquartile range). A total of 24 pregnancies occurred in 16 women within 24 (5–66) months after INCPH diagnosis. Four women had associated partial portal vein thrombosis before pregnancy. At conception, 2 out of the 16 women had detectable ascites and others were asymptomatic. Out of these 24 pregnancies, there were four miscarriages, one ectopic pregnancy, and one medical termination of pregnancy at 20 weeks of gestation. Out of the 18 other pregnancies reaching 20 weeks of gestation (in 14 patients), there were nine preterm and nine term deliveries. All infants were healthy at delivery, but one died at day 1 of unknown cause and one at day 22 of infectious meningitis; both were preterm. Concerning mothers, two had worsening of ascites, two had variceal bleeding despite non-selective betablockers during pregnancy and one developed a main portal vein thrombosis in early postpartum. Genital bleeding occurred in three patients, including two receiving anticoagulation. All 16 women were alive and asymptomatic after a median follow-up of 27 (9–93) months after last delivery. The overall outcome of women with INCPH who become pregnant is favorable despite a significant incidence of complications related to portal hypertension. Fetal outcome is favorable in most pregnancies reaching 20 weeks of gestation. Idiopathic portal hypertension, non-cirrhotic portal fibrosis and idiopathic non-cirrhotic portal hypertension (INCPH) indicate the same clinical entity.1 These terms, thereafter referred to as INCPH, designate a heterogeneous group of liver diseases causing portal hypertension and characterized by the absence of cirrhotic modification of the liver parenchyma and the patency of the portal and hepatic veins. Liver histological lesions found in patients with INCPH include obliterative portal venopathy, hepatoportal sclerosis, nodular regenerative hyperplasia and incomplete septal cirrhosis. The main complications of INCPH include the development of portal vein thrombosis and of gastrointestinal bleeding related to portal hypertension.2 Although called idiopathic, INCPH has been associated with various conditions including thrombophilia, hematologic malignancies, human immunodeficiency virus infection, genetic and immunological disorders.2 
All known hepatitis B virus (HBV) genotypes occur in humans and hominoid Old World non-human primates (NHPs). The divergent woolly monkey HBV (WMHBV) forms another orthohepadnavirus species. The evolutionary origins of HBV are unclear. We analysed sera from 124 Brazilian monkeys collected during 2012–2016 for hepadnaviruses using molecular and serological tools, and conducted evolutionary analyses. We identified a novel orthohepadnavirus species in capuchin monkeys (capuchin monkey hepatitis B virus [CMHBV]). We found CMHBV-specific antibodies in five animals and high CMHBV concentrations in one animal. Non-inflammatory, probably chronic infection was consistent with an intact preCore domain, low genetic variability, core deletions in deep sequencing, and no elevated liver enzymes. Cross-reactivity of antisera against surface antigens suggested antigenic relatedness of HBV, CMHBV, and WMHBV. Infection-determining CMHBV surface peptides bound to the human HBV receptor (human sodium taurocholate co-transporting polypeptide), but preferentially interacted with the capuchin monkey receptor homologue. CMHBV and WMHBV pseudotypes infected human hepatoma cells via the human sodium taurocholate co-transporting polypeptide, and were poorly neutralised by HBV vaccine-derived antibodies, suggesting that cross-species infections may be possible. Ancestral state reconstructions and sequence distance comparisons associated HBV with humans, whereas primate hepadnaviruses as a whole were projected to NHP ancestors. Co-phylogenetic analyses yielded evidence for co-speciation of hepadnaviruses and New World NHP. Bayesian hypothesis testing yielded strong support for an association of the HBV stem lineage with hominoid ancestors. Neither CMHBV nor WMHBV was likely the ancestor of the divergent human HBV genotypes F/H found in American natives. Our data suggest ancestral co-speciation of hepadnaviruses and NHP, and an Old World origin of the divergent HBV genotypes F/H. The identification of a novel primate hepadnavirus offers new perspectives for urgently needed animal models of chronic hepatitis B. The hepatitis B virus (HBV) is one of the most important human pathogens, causing at least 680,000 deaths each year globally caused by chronic infection resulting in liver cirrhosis and hepatocellular carcinoma.1 HBV is the prototype species of the genus Orthohepadnavirus in the family Hepadnaviridae. In humans, HBV comprises 10 genotypes named A–J.2 Additional HBV genotypes infect Old World non-human primates (NHPs), including chimpanzees, gorillas, orangutans, and gibbons.3 Infection of humans with HBV genotypes from NHPs has not been described yet. By contrast, NHPs can carry human HBV genotypes and HBV genotypes from other NHP species, illustrating the potential of primate HBV to cross the species barrier.3 Different from other major blood-borne viruses, such as HIV, there is no evidence for an evolutionary origin of human HBV from viruses carried by Old World NHPs.4 Similarly, the evolutionary origins of the divergent human HBV genotypes F and H associated with American natives inhabiting Alaska and Latin America are unknown.5 
The prevalence of anti-hepatitis C virus antibody in Punjab, India is 3.6%, with 728,000 people estimated to have viremic chronic hepatitis C (CHC). The Mukh-Mantri Punjab Hepatitis C Relief Fund, launched on 18th June 2016, provides no-cost generic direct-acting antivirals (DAAs) with sofosbuvir + ledipasvir ± ribavirin or sofosbuvir + daclatasvir ± ribavirin with the goal of eliminating CHC from Punjab. We assessed the safety and efficacy of decentralized treatment of CHC in a public health care setting. Primary care providers from 3 university and 22 district hospitals were trained to provide algorithm-based DAA treatment and supervised by telehealth clinics conducted fortnightly. The diagnosis of cirrhosis was based on clinical and radiological evidence, including aspartate aminotransferase-to-platelet ratio index (APRI ≥2.0) and FIB-4 score (>3.25), or on liver stiffness measurement ≥12.5 kPa on Fibroscan®. We enrolled 48,088 individuals with CHC (63.8% male; mean age 42.1 years; 80.5% rural; 14.8% compensated cirrhosis; 69.9% genotype [GT] 3) between 18th June 2016 to 31st July 2018. While 36,250 (75.4%) patients completed treatment, 5,497 (11.4%) had treatment interruptions and 6,341 (13.2%) patients are currently ongoing treatment. Sustained virological response at 12 weeks after treatment completion (SVR12) was achieved in 91.6% of patients per protocol, 67.6% in intention-to-treat (ITT) analysis, where all interruptions were treated as failures, and 91.2% in a modified ITT analysis where all patients with successful SVR12 in the interruptions arm were included as cured. SVR12 rates in patients with and without cirrhosis and GT3 versus non-GT3 were comparable. The SVR12 rate was 84.4% in patients who had treatment interruptions. Decentralized care of patients with CHC using generic all-oral DAA regimens is safe and effective regardless of genotype or presence of cirrhosis. Of the 28 million people living in Punjab (2011 Census), 3.6% are estimated to be positive for anti-HCV antibody and 2.6% (728,000) are estimated to test positive for HCV RNA.1,2 A majority of these people may progress to cirrhosis and its complications including variceal bleeding, liver failure and hepatocellular carcinoma (HCC), or death.3,4 Successful treatment of chronic hepatitis C (CHC) is associated with 62–84% reduction in all-cause mortality, 68–79% reduction in risk of HCC and 90% reduction in need for liver transplantation. Treating CHC saves lives, is cost-effective and cost-saving in the long-term by reducing the overall cost of public health expenditure for treatment of liver disease.5 With the advent of generic direct-acting antivirals (DAAs), the treatment is less expensive and equally effective (cure rates >90%).6–8 The Mukh-Mantri Punjab Hepatitis C Relief Fund program, provides free medical treatment for people with CHC with the goal of eliminating hepatitis C from Punjab.9 The Punjab Model is an innovative interactive model of decentralized services that uses telementoring and algorithm-based treatment with generic drugs. The Punjab Model trains and supports primary care providers (PCPs) to learn about emerging treatment options, adverse effects and treatment adherence, so that they can manage CHC using the existing health care infrastructure.10,11 This study highlights the effectiveness of a public health strategy using decentralized care to empower primary healthcare providers to combat and eliminate HCV from the state of Punjab in India. The results demonstrated cure rates of nearly 92% regardless of genotypes, degree of liver fibrosis or high-risk subgroups. The feasibility, safety and efficacy of decentralized care and treatment of CHC demonstrated in the Punjab Model, have led to the beginning of the NVHCP in India. SECTION Financial support
Unlike other hepatitis viruses that have infected primates for millions of years, hepatitis A virus (HAV) likely entered human populations only 10–12 thousand years ago after jumping from a rodent host. The phylogeny of modern hepatoviruses that infect rodents and bats suggest that multiple similar host shifts have occurred in the past. The factors determining such shifts are unknown, but the capacity to overcome innate antiviral responses in a foreign species is likely key. We assessed the capacity of diverse hepatovirus 3ABC proteases to cleave mitochondrial antiviral signaling protein (MAVS) and disrupt antiviral signaling in HEK293 and human hepatocyte-derived cell lines. We also applied maximum-likelihood and Bayesian algorithms to identify sites of diversifying selection in MAVS orthologs from 75 chiropteran, rodent and primate species. 3ABC proteases from bat, but not rodent hepatoviruses efficiently cleaved human MAVS at Glu463/Gly464, disrupting virus activation of the interferon-β promoter, whereas human HAV 3ABC cleaved at Gln427/Val428. In contrast, MAVS orthologs from rodents and bats were resistant to cleavage by 3ABC proteases of cognate hepatoviruses and in several cases human HAV. A search for diversifying selection among MAVS orthologs from all 3 orders revealed 90 of ∼540 residues to be under positive selection, including residues in chiropteran MAVS that align with the site of cleavage of human MAVS by bat 3ABC proteases. 3ABC protease cleavage of MAVS is a conserved attribute of hepatoviruses, acting broadly across different mammalian species and associated with evidence of diversifying selection at cleavage sites in rodent and bat MAVS orthologs. The capacity of hepatoviruses to disrupt MAVS-mediated innate immune responses has shaped evolution of both hepatoviruses and their hosts, and facilitates cross-species transmission of hepatitis A. Despite effective vaccines, hepatitis A virus (HAV) remains a common cause of acute viral hepatitis in many regions of the world.1 The continued presence of this hepatotropic virus is dependent upon unbroken chains of fecal-oral transmission, since persistent infection is rare-to-nonexistent and chronic HAV shedders are unknown. Accordingly, HAV disappears at times from small, isolated, human populations, only to return with a vengeance when re-introduced.2,3 Thus, unlike hepatitis B virus (HBV) and hepatitis C virus (HCV) that cause long-term persistant infections and co-evolved with humans and other primate species over millions of years,4–6 HAV likely became established among human populations only when groups living together became large enough to sustain chains of transmission 10–12 thousand years ago. Multiple, distantly related hepatoviruses have recently been discovered among bats, rodents and other small mammals.7,8 These viruses are hepatotropic, and although distantly related phylogenetically, share antigenic determinants with human HAV. Their phylogeny provides evidence for multiple past host species shifts across different mammalian orders, and ancestral reconstructions suggest a rodent origin for human HAV.7 
Despite direct-acting antivirals being highly effective at eradicating hepatitis C virus infection, their impact on the development of hepatocellular carcinoma (HCC) remains controversial. We analyzed the clinical and radiological outcome of cirrhotic patients treated with interferon-free regimens to estimate the risk of developing HCC. This was a retrospective multicenter study focusing on cirrhotic patients treated with direct-acting antivirals until December 2016. Clinical and radiologic characteristics were collected before the start of antiviral therapy, at follow-up and at HCC development. Diagnosis of HCC was centrally validated and its incidence was expressed as HCC/100 person-years. A total of 1,123 patients were included (60.6% males, 83.8% Child-Pugh A) and 95.2% achieved a sustained virologic response. Median time of follow-up was 19.6 months. Seventy-two patients developed HCC within a median of 10.3 months after starting antiviral treatment. HCC incidence was 3.73 HCC/100 person-years (95% CI 2.96–4.70). Baseline liver function, alcohol intake and hepatic decompensation were associated with a higher risk of HCC. The relative risk was significantly increased in patients with non-characterized nodules at baseline 2.83 (95% CI 1.55–5.16) vs. absence of non-characterized nodules. When excluding these patients, the risk remained increased. These data expose a clear-cut time association between interferon-free treatment and HCC. The mechanisms involved in the increased risk of HCC emergence in the short term require further investigation. Elimination of hepatitis C virus (HCV) infection is now a feasible goal through the availability of oral direct-acting antivirals (DAAs).1 Treatment is safe and it achieves a high rate of sustained virologic response (SVR) both in cirrhotic and non-cirrhotic patients.1 Progressive liver function impairment is halted in patients that achieve SVR and long-term follow-up studies should define the impact of successful treatment on the incidence of hepatocellular carcinoma (HCC). Malignant transformation occurs years before clinical recognition and thus, even if the viral infection is solved, the progression of transformed clones may still take place and ultimately, result in clinically detectable HCC.2–5 Hence, the elimination of viral infection should prevent new oncogenic events and HCC incidence is likely to decrease after several years. However, the current controversial issue is not whether HCC incidence will vanish in the long-term, but whether there is a time-associated peak in HCC incidence after DAA therapy.6 Such a peak would mimic the increased HCC recurrence associated with DAAs that was reported some time ago and is still a matter of debate.7–12 The suggested mechanism for an increased recurrence risk was an imbalance in immune surveillance related to a sudden change in hepatic inflammation, which may also occur in patients without a prior diagnosis of HCC.11,13–16 The absence of randomized controlled trials prevents a direct comparison between treated and untreated patients. Thus, any insight into this issue should come from careful description of HCC development in large cohorts of patients treated with DAA.11 Comparison should be attempted with existing data of almost contemporary cohorts of untreated patients. Any comparisons must consider that impaired liver function does not impede DAA therapy, as was the case for interferon-based regimens. In that sense, the prospective HALT-C17 and EPIC trials18 reported the HCC risk in 2 well-followed homogeneous cohorts of untreated HCV-viremic patients with compensated cirrhosis, in which HCC development was the main endpoint. The prospective CIRVIR study in France has also provided the follow-up data in a population of HCV cirrhotic patients undergoing HCC screening by ultrasound (US).19,20 Hence, they offer valuable comparative information. Unfortunately, none of the registration studies assessing the efficacy and safety of DAAs included prospective US examination to detect HCC and follow-up was too short in some studies.11 Therefore, they are not useful to attempt any comparison. 
Selection criteria for hepatectomy in patients with cirrhosis are controversial. In this study we aimed to build prognostic models of symptomatic post-hepatectomy liver failure (PHLF) in patients with cirrhosis. This was a cohort study of patients with histologically proven cirrhosis undergoing hepatectomy in 6 French tertiary care hepato-biliary-pancreatic centres. The primary endpoint was symptomatic (grade B or C) PHLF, according to the International Study Group of Liver Surgery’s definition. Twenty-six preoperative and 5 intraoperative variables were considered. An ordered ordinal logistic regression model with proportional odds ratio was used with 3 classes: O/A (No PHLF or grade A PHLF), B (grade B PHLF) and C (grade C PHLF). Of the 343 patients included, the main indication was hepatocellular carcinoma (88%). Laparoscopic liver resection was performed in 112 patients. Three-month mortality was 5.25%. The observed grades of PHLF were: 0/A: 61%, B: 28%, C: 11%. Based on the results of univariate analyses, 3 preoperative variables (platelet count, liver remnant volume ratio and intent-to-treat laparoscopy) were retained in a preoperative model and 2 intraoperative variables (per protocol laparoscopy and intraoperative blood loss) were added to the latter in a postoperative model. The preoperative model estimated the probabilities of PHLF grades with acceptable discrimination (area under the receiver-operating characteristic curve [AUC] 0.73, B/C vs. 0/A; AUC 0.75, C vs. 0/A/B) and the performance of the postoperative model was even better (AUC 0.77, B/C vs. 0/A; AUC 0.81, C vs. 0/A/B; p <0.001). By accurately predicting the risk of symptomatic PHLF in patients with cirrhosis, the preoperative model should be useful at the selection stage. Prediction can be adjusted at the end of surgery by also considering blood loss and conversion to laparotomy in a postoperative model, which might influence postoperative management. The safety of elective hepatectomies in cirrhotic patients has increased significantly during the last decades but mortality of such procedures is still estimated between 3 and 14%.1 Post-hepatectomy liver failure (PHLF) is the most worrisome complication, with a reported mortality as high as 50%.2,3 Moreover, it is the leading cause of prolonged hospitalization, increased costs, and poor long-term outcomes in patients undergoing this surgical procedure. 
Cytokine-induced killer (CIK) cell-based immunotherapy is effective as an adjuvant therapy in early stage hepatocellular carcinoma (HCC) but lacks efficacy in advanced HCC. We aimed to investigate immune suppressor mechanisms in HCC, focusing on the role of myeloid-derived suppressor cells (MDSCs) in response to CIK therapy. MDSCs were quantified by flow cytometry and quantitative real-time PCR. Cytokines were detected by cytokine array. A lactate dehydrogenase cytotoxicity assay was performed in the presence or absence of MDSCs to study CIK function against HCC cells in vitro. An FDA-approved PDE5 inhibitor, tadalafil, was used to target MDSCs in vitro and in vivo. Two different murine HCC cell lines were tested in subcutaneous and orthotopic tumor models in C57BL/6 and BALB/c mice. The antitumor effects of human CIKs and MDSCs were also tested in vitro. Adoptive cell transfer of CIKs into tumor-bearing mice induced inflammatory mediators (e.g., CX3CL1, IL-13) in the tumor microenvironment and an increase of tumor-infiltrating MDSCs, leading to impaired antitumor activity in 2 different HCC models. MDSCs efficiently suppressed the cytotoxic activity of CIKs in vitro. In contrast, treatment with a PDE5 inhibitor reversed the MDSC suppressor function via ARG1 and iNOS blockade and systemic treatment with a PDE5 inhibitor prevented MDSC accumulation in the tumor microenvironment upon CIK cell therapy and increased its antitumor efficacy. Similar results were observed when human CIKs were tested in vitro in the presence of CD14+HLA-DR−/low MDSCs. Treatment of MDSCs with a PDE5 inhibitor suppressed MDSC suppressor function and enhanced CIK activity against human HCC cell lines in vitro. Our results suggest that targeting MDSCs is an efficient strategy to enhance the antitumor efficacy of CIKs for the treatment of patients with HCC. Hepatocellular carcinoma (HCC) is a common and fatal cancer, with an increasing incidence worldwide.1 Though many curative therapies have been developed, the overall response to these therapies is inadequate and the long-term prognosis of patients with HCC remains poor because of its high recurrence rates.2 A lot of data have shown that tumor progression is correlated with the accumulation of myeloid-derived suppressor cells (MDSCs) which induce local and possibly systemic immunosuppression.3 Moreover, a greater prevalence of MDSCs has been correlated with early recurrence and was shown to be a predictor of poor prognosis in patients with HCC who underwent curative resection,4 radiotherapy,5 and hepatic arterial infusion chemotherapy.6 MDSCs have been shown to suppress CD8+7–9 and CD4+ T10 cells as well as natural killer (NK)11 cells through diverse direct or indirect mechanisms.12 
As many as 70% of individuals with chronic hepatitis C (CHC) are managed solely in primary care. The aims of this study were to determine the prevalence of elevated liver stiffness measurement (LSM) in a cohort of community managed patients with CHC and to evaluate predictors of advanced liver disease and liver-related events. A prospective cohort of adult patients with CHC were recruited from 21 primary care practices throughout Victoria, Australia. Inclusion criteria included the presence of CHC for >6 months, no recent (<18 months) specialist input and no history of hepatocellular carcinoma. Clinical assessment, LSM and phlebotomy were carried out in primary care. A hospital cohort was recruited for comparison. Participants were followed longitudinally and monitored for liver-related events. Over 26 months, 780 community patients were recruited and included in the analysis. The median LSM was 6.9 kPa in the community, with 16.5% of patients at risk of advanced fibrosis (LSM ≥12.5 kPa); of these 8.5% had no laboratory features of advanced liver disease. The proportion at risk of cirrhosis was no different between the community and hospital cohorts (p = 0.169). At-risk alcohol consumption, advancing age, elevated body mass index and alanine aminotransferase were independent predictors of elevated LSM. Over a median follow-up of 15.2 months, liver-related events occurred in 9.3% of those with an LSM ≥12.5 kPa. An LSM of 24 kPa had the highest predictive power for liver-related events (hazard ratio 152; p <0.001). The prevalence of advanced fibrosis, as determined by LSM, in primary care managed CHC is significant and comparable to a hospital cohort. Furthermore, this study supports the use of LSM as a community screening tool in a CHC population and indicates a possible role in predicting liver-related events. Chronic hepatitis C (CHC) is a major public health issue with an estimated global prevalence greater than 71 million cases and over 1.75 million individuals infected annually.1 Together with chronic hepatitis B (CHB), CHC is accountable for over 1.34 million deaths worldwide annually, being the second highest cause of death from communicable diseases and ranked seventh for all-cause mortality.2 The rapidly increasing rates of hepatocellular carcinoma (HCC) in the developed world have largely been attributed to increased rates of CHC infection since 1975.3 Furthermore, early identification of cirrhosis is associated with improved patient outcomes and HCC survival.4–6 
The ubiquitin ligase F-box and WD repeat domain-containing 7 (FBXW7) is recognized as a tumor suppressor in many cancer types due to its ability to promote the degradation of numerous oncogenic target proteins. Herein, we aimed to elucidate its role in intrahepatic cholangiocarcinoma (iCCA). Herein, we first confirmed that FBXW7 gene expression was reduced in human iCCA specimens. To identify the molecular mechanisms by which FBXW7 dysfunction promotes cholangiocarcinogenesis, we generated a mouse model by hydrodynamic tail vein injection of Fbxw7ΔF, a dominant negative form of Fbxw7, either alone or in association with an activated/myristylated form of AKT (myr-AKT). We then confirmed the role of c-MYC in human iCCA cell lines and its relationship to FBXW7 expression in human iCCA specimens. FBXW7 mRNA expression is almost ubiquitously downregulated in human iCCA specimens. While forced overexpression of Fbxw7ΔF alone did not induce any appreciable abnormality in the mouse liver, co-expression with AKT triggered cholangiocarcinogenesis and mice had to be euthanized by 15 weeks post-injection. At the molecular level, a strong induction of Fbxw7 canonical targets, including Yap, Notch2, and c-Myc oncoproteins, was detected. However, only c-MYC was consistently confirmed as a FBXW7 target in human CCA cell lines. Most importantly, selected ablation of c-Myc completely impaired iCCA formation in AKT/Fbxw7ΔF mice, whereas deletion of either Yap or Notch2 only delayed tumorigenesis in the same model. In human iCCA specimens, an inverse correlation between the expression levels of FBXW7 and c-MYC transcriptional activity was observed. Downregulation of FBXW7 is ubiquitous in human iCCA and cooperates with AKT to induce cholangiocarcinogenesis in mice via c-Myc-dependent mechanisms. Targeting c-MYC might represent an innovative therapy against iCCA exhibiting low FBXW7 expression. Cholangiocarcinoma (CCA) is the second most common primary liver cancer.1,2 Depending on the anatomical site, CCA is classified into intrahepatic (iCCA) and extrahepatic cholangiocarcinoma (eCCA).3 iCCA incidence has been rising over the last decade, while that of eCCA slightly decreased.4 For iCCA detected at early stage, curative surgical resection is the optimal treatment strategy.5,6 However, less than one-third of patients achieves negative tumor margins, and recurrence rates are high.5,7 Furthermore, in patients not meeting the narrow criteria for surgical treatment, therapeutic options are limited.8 Therefore, substantial efforts should be devoted to unravelling the molecular mechanisms of iCCA development and progression. This would lead to novel and more effective therapeutic strategies against this pernicious disease. 
Since the first account of the myth of Prometheus, the amazing regenerative capacity of the liver has fascinated researchers because of its enormous medical potential. Liver regeneration is promoted by multiple types of liver cells, including hepatocytes and liver non-parenchymal cells (NPCs), through complex intercellular signaling. However, the mechanism of liver organogenesis, especially the role of adult hepatocytes at ectopic sites, remains unknown. In this study, we demonstrate that hepatocytes alone spurred liver organogenesis to form an organ-sized complex 3D liver that exhibited native liver architecture and functions in the kidneys of mice. Isolated hepatocytes were transplanted under the kidney capsule of monocrotaline (MCT) and partial hepatectomy (PHx)-treated mice. To determine the origin of NPCs in neo-livers, hepatocytes were transplanted into MCT/PHx-treated green fluorescent protein transgenic mice or wild-type mice transplanted with bone marrow cells isolated from green fluorescent protein-mice. Hepatocytes engrafted at the subrenal space of mice underwent continuous growth in response to a chronic hepatic injury in the native liver. More than 1.5 years later, whole organ-sized liver tissues with greater mass than those of the injured native liver had formed. Most remarkably, we revealed that at least three types of NPCs with similar phenotypic features to the liver NPCs were recruited from the host tissues including bone marrow. The neo-livers in the kidney exhibited liver-specific functions and architectures, including sinusoidal vascular systems, zonal heterogeneity, and emergence of bile duct cells. Furthermore, the neo-livers successfully rescued the mice with lethal liver injury. Our data clearly show that adult hepatocytes play a leading role as organizer cells in liver organogenesis at ectopic sites via NPC recruitment. The liver is a complex organ that plays central roles in metabolism, protein/enzyme synthesis, and blood detoxification, and is the sole solid organ in the body that possesses high regenerative capability. Adult hepatocytes have great growth potential in vivo, similar to that of adult stem cells.1 Following the loss of liver mass, compensatory regenerative responses are accelerated until the original liver mass is restored, and this process is promoted by cytokines, growth factors, and hormones secreted from hepatocytes and liver non-parenchymal cells (NPCs), and other organs via complex interactions.2,3 Transplantation of a small number of hepatocytes to the liver has been shown to lead to complete or near-complete repopulation of the diseased livers of animal models such as urokinase-type plasminogen activator (uPA)-transgenic (Tg) or fumarylacetoacetate hydrolase (Fah)-deficient mice,4–6 ultimately reversing the lethality of these mouse models. 
Absence or low anti-HBV surface antibody (anti-HBs) is associated with an increased risk of HBV reactivation in patients with lymphoma and resolved HBV infection receiving rituximab-containing chemotherapy. Quantification of anti-HBV core antibody (anti-HBc) is a new marker associated with the natural history and treatment response of chronic HBV infection. This study investigated whether baseline anti-HBc and anti-HBs levels may better predict HBV reactivation. We prospectively measured the HBV DNA levels of patients with lymphoma and resolved HBV infection receiving rituximab–cyclophosphamide, hydroxydaunorubicin, vincristine, and prednisolone-based chemotherapy and started an antiviral therapy upon HBV reactivation, defined as a greater than 10-fold increase in HBV DNA compared with previous nadir levels. Anti-HBs and anti-HBc were quantified by a double-sandwich assay. Receiver-operating-characteristic-curve analysis was used to determine the optimal baseline anti-HBc/anti-HBs levels for predicting HBV reactivation. HBV reactivation occurred in 24 of the 197 patients enrolled, with an incidence of 11.6/100 person-years. For the 192 patients with enough serum samples for analysis, low anti-HBs (<56.48 mIU/ml) and high anti-HBc (≥6.41 IU/ml) at baseline were significantly associated with high risk of HBV reactivation (hazard ratio [HR] 8.48 and 4.52, respectively; p <0.01). The multivariate analysis indicated that (1) patients with both high anti-HBc and low anti-HBs at baseline (36 of 192 patients) had an HR of 17.29 for HBV reactivation (95% CI 3.92–76.30; p <0.001), and (2) HBV reactivation may be associated with inferior overall survival (HR 2.41; 95% CI 1.15–5.05; p = 0.02). Baseline anti-HBc/anti-HBs levels may predict HBV reactivation in these patients with lymphoma and help optimize prophylactic antiviral therapy for high-risk patients. HBV reactivation is a common complication that may lead to a life-threatening liver decompensation in patients with chronic HBV infection (HBsAg positive) who receive cytotoxic or immunosuppressive therapy. Prophylactic antiviral therapy is recommended for this patient population by guidelines of HBV management.1–4 For patients with resolved HBV infection (HBsAg negative and anti-HBV core antibody [anti-HBc] positive), HBV reactivation is best characterized in patients with lymphoma who are treated with B cell-depleting agents, such as rituximab, and the incidence of HBV reactivation ranged from about 10% to 30%.5–9 
Induction of cross-reactive antibodies targeting conserved epitopes of the envelope proteins E1E2 is a key requirement for an hepatitis C virus vaccine. Conserved epitopes like the viral CD81-binding site are targeted by rare broadly neutralizing antibodies. However, these viral segments are occluded by variable regions and glycans. We aimed to identify antigens exposing conserved epitopes and to characterize their immunogenicity. We created hepatitis C virus variants with mutated glycosylation sites and/or hypervariable region 1 (HVR1). Exposure of the CD81 binding site and conserved epitopes was quantified by soluble CD81 and antibody interaction and neutralization assays. E2 or E1-E2 heterodimers with mutations causing epitope exposure were used to immunize mice. Vaccine-induced antibodies were examined and compared with patient-derived antibodies. Mutant viruses bound soluble CD81 and antibodies targeting the CD81 binding site with enhanced efficacy. Mice immunized with E2 or E1E2 heterodimers incorporating these modifications mounted strong, cross-binding, and non-interfering antibodies. E2-induced antibodies neutralized the autologous virus but they were not cross-neutralizing. Viruses lacking the HVR1 and selected glycosylation sites expose the CD81 binding site and cross-neutralization antibody epitopes. Recombinant E2 proteins carrying these modifications induce strong cross-binding but not cross-neutralizing antibodies. Hepatitis C virus (HCV) is a global health burden affecting approximately 71 million people worldwide.1 Infection often leads to chronic hepatitis, with the subsequent risk of liver cirrhosis and hepatocellular carcinoma. Persistent HCV infection is now curable with the introduction of direct-acting antivirals. However, a prophylactic HCV vaccine is not available. Since viral re-infection is possible and as many HCV infected individuals are not diagnosed, a vaccine against HCV would facilitate global HCV eradication programs. 
Biliary atresia (BA) results from a neonatal inflammatory and fibrosing obstruction of bile ducts of unknown etiology. Although the innate immune system has been linked to the virally induced mechanism of disease, the role of inflammasome-mediated epithelial injury remains largely undefined. Here, we hypothesized that disruption of the inflammasome suppresses the neonatal proinflammatory response and prevents experimental BA. We determined the expression of key inflammasome-related genes in livers from infants at diagnosis of BA and in extrahepatic bile ducts (EHBDs) of neonatal mice after infection with rotavirus (RRV) immediately after birth. Then, we determined the impact of the wholesale inactivation of the genes encoding IL-1R1 (Il1r1−/−), NLRP3 (Nlrp3−/−) or caspase-1 (Casp1−/−) on epithelial injury and bile duct obstruction. IL1R1, NLRP3 and CASP1 mRNA increased significantly in human livers at the time of diagnosis, and in EHBDs of RRV-infected mice. In Il1r1−/− mice, the epithelial injury of EHBDs induced by RRV was suppressed, with dendritic cells unable to activate natural killer cells. A similar protection was observed in Nlrp3−/− mice, with decreased injury and inflammation of livers and EHBDs. Long-term survival was also improved. In contrast, the inactivation of the Casp1 gene had no impact on tissue injury, and all mice died. Tissue analyses in Il1r1−/− and Nlrp3−/− mice showed decreased populations of dendritic cells and natural killer cells and suppressed expression of type-1 cytokines and chemokines. Genes of the inflammasome are overexpressed at diagnosis of BA in humans and in the BA mouse model. In the experimental model, the targeted loss of IL-1R1 or NLRP3, but not of caspase-1, protected neonatal mice against RRV-induced bile duct obstruction. Biliary atresia (BA) results from a rapidly progressing inflammation and obstruction of extrahepatic bile ducts (EHBDs) in early infancy and is the most common indication for pediatric liver transplantation.1–3 The etiology of BA includes environmental triggers in the genetically susceptible host,4,5 followed by an over-activation of the neonatal immune response in the liver and EHBD.6,7 We and others previously reported several cellular (dendritic cells [DCs], natural killer [NK] cells and CD8+ T cells) and molecular (IL-8, IL-15, IFN-γ, TNFα) effectors of bile duct epithelial injury.8–16 Despite this progress, very little is known about how molecular sensors and related circuits regulate the hepatobiliary injury and duct obstruction in BA. 
Hepatobiliary magnetic resonance imaging (MRI) provides additional information beyond the size and number of tumours, and may have prognostic implications. We examined whether pretransplant radiological features on MRI could be used to stratify the risk of tumour recurrence after liver transplantation (LT) for hepatocellular carcinoma (HCC). A total of 100 patients who had received a liver transplant and who had undergone preoperative gadoxetic acid-enhanced MRI, including the hepatobiliary phase (HBP), were reviewed for tumour size, number, and morphological type (e.g. nodular, nodular with perinodular extension, or confluent multinodular), satellite nodules, non-smooth tumour margins, peritumoural enhancement in arterial phase, peritumoural hypointensity on HBP, and apparent diffusion coefficients. The primary endpoint was time to recurrence. In a multivariable adjusted model, the presence of satellite nodules [hazard ratio (HR) 3.07; 95% confidence interval (CI) 1.14–8.24] and peritumoural hypointensity on HBP (HR 4.53; 95% CI 1.52–13.4) were identified as independent factors associated with tumour recurrence. Having either of these radiological findings was associated with a higher tumour recurrence rate (72.5% vs. 15.4% at three years, p <0.001). When patients were stratified according to the Milan criteria, the presence of these two high-risk radiological findings was associated with a higher tumour recurrence rate in both patients transplanted within the Milan criteria (66.7% vs. 11.6% at three years, p <0.001, n = 68) and those who were transplanted outside the Milan criteria (75.5% vs. 28.6% at three years, p <0.001, n = 32). Radiological features on preoperative hepatobiliary MRI can stratify the risk of tumour recurrence in patients who were transplanted either within or outside the Milan criteria. Therefore, hepatobiliary MRI can be a useful way to select potential candidates for LT. Liver transplantation (LT) is regarded as the best option for radical treatment in patients with very early (stage 0) or early (stage A) hepatocellular carcinoma (HCC), according to the Barcelona Clinic Liver Cancer guideline.1–3 Careful patient selection is important in reducing the tumour recurrence rate and maximising the effectiveness of LT for HCC because the availability of deceased and living donor organs is limited. Thus, the Milan criteria are used as the gold standard to increase the post-transplant survival rate of patients with HCC, with a five-year survival rate of 70% after LT and a recurrence rate of <20%.1,2,4 
Multifocal tumors, developed either from intrahepatic metastasis (IM) or multicentric occurrence (MO), is a distinct feature of hepatocellular carcinoma (HCC). Immunogenomic characterization of multifocal HCC is important for understanding immune escape in different lesions and developing immunotherapy. We combined whole exome/transcriptome sequencing, multiplex immunostaining, immunopeptidomes, T-cell receptor (TCR) sequencing and bioinformatic analyses of 47 tumors from 15 HCC patients with multifocal lesions. IM and MO demonstrated distinct clonal architecture, mutational spectrum and genetic susceptibility. The immune microenvironment also displayed spatiotemporal heterogeneity, such as less T cell and more M2 macrophage infiltration in IM and higher expression of inhibitory immune checkpoints in MO. Similar to mutational profiles, shared neoantigens and TCR repertoires among tumors from the same patients were abundant in IM but scarce in MO. Combining neoantigen prediction and immunopeptidomes identified T-cell specific neoepitopes and achieved a high verification rate in vitro. Immunoediting mainly occurred in MO but not IM, due to the relatively low immune infiltration. HLA LOH, identified in 17% of multifocal HCC, hampered the ability of MHC to present neoantigens, especially in IM. An integrated analysis of Immunoscore, Immunoediting, TCR clonality and HLA LOH of each tumor could stratify patients into two groups with high or low risk of recurrence (P=0.038). Our study comprehensively characterized the genetic structure, neoepitope landscape, T cell profile and immunoediting status that collectively shape tumor evolution, which may optimize personalized immunotherapies for multifocal HCC. Hepatocellular carcinoma (HCC) is the fourth leading cause of cancer-related death and ranks sixth in incidence globally, with 0.25–1 million cases annually [1]. Most often, HCC occurs in the background of chronic hepatitis or liver cirrhosis. Surgery is potentially curative but only amenable for early stage patients. Recent breakthroughs in systemic and immune therapies have shown clinical benefits in HCC[2]. However, improvements in patient outcomes are modest and long-term survival remains poor. One major challenge is that 41%-75% of HCC patients are initially diagnosed as multifocal tumors, which will increase the difficulty of clinical management and lead to poor prognosis [3]. 
Lysosomal acid lipase deficiency (LAL-D) is an autosomal recessive condition that may present in a mild form (cholesteryl ester storage disease [CESD]), which mimics non-alcoholic fatty liver disease (NAFLD). It has been suggested that CESD may affect 1 in 40,000 and is under-diagnosed in NAFLD clinics. Therefore, we aimed to estimate the prevalence of LAL-D using analysis of genetic variation in LIPA. MEDLINE and EMBASE were systematically searched for previously reported disease variants and prevalence estimates. Previous prevalence estimates were meta-analysed. Disease variants in LIPA were annotated with allele frequencies from gnomAD and combined with unreported major functional variants found in humans. Pooled ethnicity-specific prevalences for LAL-D and CESD were calculated using the Hardy-Weinberg equation. Meta-analysis of existing genetic studies estimated the prevalence of LAL-D as 1 per 160,000 (95% CI 1 per 65,025–761,652) using the allele frequency of c.894G>A in LIPA. A total of 98 previously reported disease variants in LIPA were identified, of which 32/98 were present in gnomAD, giving a prevalence of 1 per 307,482 (95% CI 257,672–366,865). Wolman disease was associated with more loss-of-function variants than CESD. When this was combined with 22 previously unreported major functional variants in LIPA identified in humans, the pooled prevalence of LAL-D was 1 per 177,452 (95% CI 149,467–210,683) with a carrier frequency of 1 per 421. The prevalence is lowest in those of East Asian, South Asian, and Finnish ancestry. Using 120 disease variants in LIPA, these data can reassure clinicians that LAL-D is an ultra-rare disorder. Given the therapeutic capability of sebelipase alpha, investigation for LAL-D might be included in second-line metabolic screening in NAFLD. Lysosomal acid lipase deficiency (LAL-D) is an autosomal recessive disorder caused by mutations in LIPA that manifest as a spectrum of liver disease and dyslipidaemia.1,2 It is regarded as a rare disorder however recognition of more mild forms of the condition have led to the suggestion that it may represent a significant proportion of patients presenting with non-alcoholic fatty liver disease (NAFLD).3,4 Emerging data have also reported reduced lysosomal acid lipase (LAL) activity in association with more advanced NAFLD.5–8 Clarity on the prevalence of LAL-D, and its distribution across ethnicities, is needed to determine whether LAL activity testing should be a routine part of NAFLD clinics. 
In 2015, the World Health Organization (WHO) issued guidelines for the management of chronic hepatitis B (CHB) in low- and middle-income countries, but little is known about the applicability of the WHO treatment criteria in sub-Saharan Africa. The aim of this study was to evaluate the diagnostic performance of the WHO guidelines in a large CHB cohort in Ethiopia. Treatment-naïve adults who attended a public CHB clinic in Addis Ababa were included in this analysis. All patients underwent a standardized evaluation at recruitment, including blood tests and transient elastography (Fibroscan®). A Fibroscan result >7.9 kPa was used to define significant fibrosis and >9.9 kPa to define cirrhosis. Treatment eligibility was assessed using the most recent guidelines from the European Association for the Study of the Liver (EASL) as the ‘gold standard’. Out of 1,190 patients with CHB, 300 (25.2%) were eligible for treatment based on the EASL 2017 guidelines and 182 (15.3%) based on the WHO 2015 guidelines. The sensitivity and specificity of the WHO criteria were 49.0 and 96.1%, respectively. Most patients (94 of 182; 51.6%) who fulfilled the WHO criteria had decompensated cirrhosis and might have a dismal prognosis even with therapy. Only 41 of 115 patients (35.7%) with compensated cirrhosis, who are likely to benefit the most from therapy, were eligible for treatment based on the WHO criteria. The WHO guidelines for CHB failed to detect half of the patients in need of treatment in Ethiopia, implying the need for a revision of the WHO treatment criteria. Chronic infection with hepatitis B virus (HBV) continues to be a significant health problem globally. Worldwide, around 2 billion people have evidence of past or present infection with HBV and an estimated 257 million are chronically infected.1,2 Almost half of the world’s population resides in areas of high HBV endemicity, with the highest prevalence in Africa and East Asia. In sub-Saharan Africa, 5–10% of the adult population is living with chronic hepatitis B (CHB).3 Annually, an estimated 887,000 deaths occur as a result of CHB, mainly due to its late complications viz cirrhosis and hepatocellular carcinoma (HCC).1 Between 1990 and 2013 the number of HBV-related deaths due to liver cirrhosis and/or HCC increased by 33% globally.4 
Non-invasive imaging is crucial for the early diagnosis and successful treatment of hepatocellular carcinoma (HCC). Terminology and criteria for interpreting and reporting imaging results must be standardized to optimize diagnosis. The aim of this study was to prospectively compare the diagnostic accuracy of the American Association for the Study of Liver Diseases (AASLD) and the 2014 version of Liver Imaging Reporting and Data System (LI-RADS®) criteria for the non-invasive diagnosis of small HCC, and to evaluate the diagnostic value of ancillary features used in the LI-RADS criteria. Between April 2009 and April 2012, patients with cirrhosis and one to three 10–30 mm nodules were enrolled and underwent computed tomography (CT) and magnetic resonance (MR) imaging. The diagnostic accuracy of both the AASLD and the LI-RADS criteria were determined based on their sensitivity, specificity, positive (PPV) and negative predictive values (NPV). A total of 595 nodules were included (559 [341 HCC, 61%] with MR imaging and 529 [332 HCC, 63%] with CT). Overall, no (0%) LR-1 and LR-2, 44 (33%) and 47 (41%) LR-3, 50 (53%) and 54 (55%) LR-4, 244 (94%) and 222 (91%) LR-5 and 4 (67%) and 9 (82%) LR-5V were HCC on MR imaging and CT, respectively. The sensitivity, specificity, PPV/NPV of the AASLD score was 72.5%, 87.6%, 90.2%, and 66.9% for MR imaging, and 71.4%, 77.7%, 84.3%, 61.7% for CT, respectively. For the combination of LR-5V and LR-5 nodules these measures were 72.5%, 89.9%, 91.9% and 67.5% on MRI and 66.9%, 88.3%, 90.9% and 63.3% on CT, respectively. For the combination of LR-5V, LR-5 and LR-4 nodules they were 87.1%, 69.1%, 81.6% and 77.3% on MRI and 85.8%, 66%, 81% on 73.5% on CT, respectively. The 2014 version of the LI-RADS is no more accurate than the AASLD score for the non-invasive diagnosis of small HCC in high-risk patients, but it provides important and complementary information on the probability of having HCC in high-risk patients, allowing for possible changes in the management of these patients. Hepatocellular carcinoma (HCC) is the fifth most frequent cancer and the second leading cause of death from cancer worldwide, occurring on cirrhosis in over 90% of cases.1,2 Early detection is the only hope for effective and curative treatment of patients with HCC, emphasizing the crucial role of screening strategies in the monitoring of high-risk patients. Computed tomography (CT) and magnetic resonance (MR) imaging play a key role in the diagnostic strategy of HCC, and an imaged-based diagnosis of HCC is accepted by several guidelines in Europe, North America and Asia, based on the unique vascular profile of this tumor.2–6 In fact, in nodules measuring >10 mm, the combination of hyperenhancement during the arterial phase followed by washout during the portal venous and/or delayed phases has been shown to have a specificity for HCC of nearly 100%, but the sensitivity and specificity is influenced by the size of the tumors.7,8 
Hepatocellular carcinoma (HCC) is a frequent complication of liver disease. When feasible, hepatic resection is the first-choice therapy. However, tumor recurrence complicates at least 2/3 hepatic resections at 5 years. Early recurrences are mainly tumor or treatment-related, but predictors of late recurrences are undefined. We aimed to evaluate the factors related to HCC recurrence after curative resection, with liver and spleen stiffness measurement (LSM and SSM) as markers of severity and duration of the underlying liver disease. We enrolled patients with chronic liver disease and primary HCC suitable for hepatic resection. We followed up patients for at least 30 months or until HCC recurrence. We performed uni- and multivariate analyses to evaluate the predictive role of tumor characteristics, laboratory data, LSM and SSM for both early and late recurrence of HCC. We prospectively enrolled 175 patients. Early HCC recurrence at multivariate analysis was associated with viral etiology, HCC grading (3 or 4), resection margins <1 cm and being beyond the Milan criteria. HCC late recurrence at univariate analysis was associated with esophageal varices (hazard ratio [HR] 3.321, 95% CI 1.564–7.053), spleen length (HR 3.123, 95% CI 1.377–7.081), platelet/spleen length ratio if <909 (HR 2.170, 95% CI 1.026–4.587), LSM (HR 1.036, 95% CI 1.005–1.067), SSM (HR 1.046, 95% CI 1.020–1.073). HCC late recurrence at multivariate analysis was independently associated only with SSM (HR 1.046, CI 1.020–1.073). Late HCC recurrence-free survival was significantly different according to the SSM cut-off of 70 kPa (p = 0.0002). SSM seems to be the only predictor of late HCC recurrence, since it is directly correlated with the degree of liver disease and portal hypertension, both of which are involved in carcinogenesis. Hepatocellular carcinoma (HCC) is a frequent complication in patients with chronic liver diseases, and one of the most common malignancies worldwide.1,2 Liver resection is the first option for the treatment of patients with small solitary tumors and preserved liver function.1,2 Tumor recurrence complicates 70% of cases of hepatic resection at 5 years, and is the expression of both intrahepatic metastasis (mainly stated as early recurrence) and the development of de novo tumors (late recurrence).3–8 
Baveno VI and expanded Baveno VI criteria can avoid the need for esophagogastroduodenoscopy (EGD) to screen for varices needing treatment (VNT) in a substantial proportion of compensated patients with viral and/or alcoholic cirrhosis. This multicenter, cross-sectional study aims to validate these criteria in patients with compensated cirrhosis due to non-alcoholic fatty liver disease (NAFLD), accounting for possible differences in liver stiffness measurement (LSM) values between M and XL probes. We assessed 790 patients with NAFLD-related compensated cirrhosis who had EGD within six months of a reliable LSM, measured by FibroScan® using M and/or XL probe. Baveno VI and expanded Baveno VI criteria were tested. The main variable used to optimize criteria was the percentage of endoscopies spared, keeping the risk of missing large VNT below a 5% threshold. LSM was measured by both M and XL probes (training set) in 314 patients, while only M or XL probe (validation sets) were used to measure LSM in 338 and 138 patients, respectively. In the training set, use of Baveno VI and expanded Baveno VI criteria reduced the number of EGD by 33.3% and by 58%, with 0.9% and 3.8% of large esophageal varices missed, respectively. The best thresholds to rule-out VNT were identified as platelet count >110,000/mm3 and LSM <30 kPa for M probe, and platelet count >110,000/mm3 and LSM <25 kPa for XL probe (NAFLD cirrhosis criteria). Thus, usage of NAFLD cirrhosis criteria would have led to an absolute reduction in the number of EGD screened patients of 34.7% and 10.5% with respect to Baveno VI and expanded Baveno VI criteria, respectively. The new NAFLD cirrhosis criteria, established for the FibroScan probe, can reduce the use of EGD for screening of VNT in NAFLD cirrhosis by more than half, with a chance of missing VNT below 5%. The pandemic spreading of obesity and diabetes makes non-alcoholic fatty liver disease (NAFLD) the most rapidly growing cause of chronic liver disease and cirrhosis,1 an increasing risk factor for hepatocellular carcinoma,2 and an emerging indication for liver transplantation.3 Consistent with these data, the management of patients with NAFLD-related cirrhosis represents a challenge in terms of epidemiological, clinical and economic burden. In this complex picture, the diagnosis of esophageal varices (EV) and especially large (grade 2/3) EV requiring primary prophylaxis (varices needing treatment [VNT]), is of paramount prognostic importance in all patients with cirrhosis, including those with NAFLD.4,5 However, VNT are not frequent in patients with compensated cirrhosis, and strategies to reduce the number of unnecessary esophagogastroduodenoscopy (EGD) procedures have been proposed. Recently, the Baveno VI guidelines proposed that compensated cirrhotic patients with a liver stiffness measurement (LSM) <20 kPa and a platelet count >150,000/μl can avoid screening endoscopy,6 the specificity of this strategy for excluding VNT being validated in different studies.7 Furthermore, expanded Baveno VI criteria, obtained by optimizing LSM and platelet count (PLT) values (<25 kPa and >110,000/μl, respectively), have also been proposed and demonstrated to spare a higher proportion of unnecessary EGD when compared to Baveno VI criteria.8 
There are conflicting reports on the outcomes after live donor liver transplantation in patients with hepatocellular carcinoma (HCC). We aimed to compare the survival of patients with HCC, with a potential live donor (pLDLT) at listing vs. no potential donor (pDDLT), on an intention-to-treat basis. All patients with HCC listed for liver transplantation between 2000–2015 were included. The pLDLT group was comprised of recipients with a potential live donor identified at listing. Patients without a live donor were included in the pDDLT group. Survival was assessed by the Kaplan-Meier method. Multivariable Cox regression was applied to identify potential predictors of mortality. A total of 219 patients were included in the pLDLT group and 632 patients in the pDDLT group. In the pLDLT group, 57 patients (26%) were beyond the UCSF criteria whereas 119 patients (19%) in the pDDLT group were beyond (p = 0.02). Time on the waiting list was shorter for the pLDLT than the pDDLT group (4.8 [2.9–8.5] months vs. 6.2 [3.0–12.0] months, respectively, p = 0.02). The dropout rate was 32/219 (14.6%) in the pLDLT and 174/632 (27.5%) in the pDDLT group, p <0.001. The 1-, 3- and 5-year intention-to-treat survival rates were 86%, 72% and 68% in the pLDLT vs. 82%, 63% and 57% in the pDDLT group, p = 0.02. Having a potential live donor was a protective factor for death (hazard ratio [HR] 0.67; 95% CI 0.53–0.86). Waiting times of 9–12 months (HR 1.53; 95% CI 1.02–2.31) and ≥12 months (HR 1.69; 95% CI 1.23–2.32) were predictors of death. Having a potential live donor at listing was associated with a significant decrease in the risk of death in patients with HCC in this intention-to-treat analysis. This benefit is related to a lower dropout rate and a shorter waiting period. Live donor liver transplantation (LDLT) improves survival compared to deceased donor liver transplantation (DDLT) when analyzed from the time of listing (intention-to-treat [ITT] analysis) in patients with end-stage liver disease.1,2 However, this benefit has never been shown in patients with hepatocellular carcinoma (HCC). Initial experiences with LDLT for HCC had suggested a higher HCC recurrence rate and a decreased survival after transplantation when compared to DDLT.3–6 Concerns have been raised regarding liver regeneration after LDLT, the potential of suboptimal oncological resection since the inferior vena cava is not removed, and the fact that some patients with more aggressive tumor biology might be “fast tracked” to LT.3 More recent studies have reported similar outcomes for LDLT and DDLT for patients with HCC.7,8 To the best of our knowledge, only 2 studies have compared the outcomes between LDLT and DDLT for HCC on an ITT analysis.7,8 Both studies had a limited number of patients undergoing LDLT (36 and 79) and showed no survival differences between LDLT and DDLT.7,8 
The presence of hepatocellular adenoma (HCA) in pregnant women requires special consideration, as it has been reported to carry the risk of growth and clinically significant haemorrhage. In this prospective study we assessed aspects of growth of HCA <5 cm during pregnancy. This was a multicentre prospective cohort study in pregnant women with suspected HCA <5 cm on imaging. Definitive HCA diagnosis was established by MRI with hepatobiliary contrast agents (LCE-MRI), preferably before pregnancy. If at study inclusion a definitive diagnosis was lacking, LCE-MRI was performed after giving birth. Growth of the adenoma (defined as an increase of >20%) was closely monitored with ultrasound examinations throughout pregnancy. Of the 66 women included, 18 were excluded from analysis because postpartum LCE-MRI did not confirm the diagnosis of HCA and showed the lesion to be focal nodular hyperplasia. The remaining 48 women, with an HCA confirmed by LCE-MRI, were followed during 51 pregnancies. Median age was 30 years (IQR 27–33) and median body mass index 31.9 kg/m2 (IQR 26.3–36.6). Growth of HCA was seen in 13 of the pregnancies (25.5%); the median increase was 14 mm (IQR 8–19). One woman whose HCA grew to >70 mm successfully underwent transarterial embolization at week 26 of pregnancy to prevent further growth. The other 50 pregnancies proceeded without complications. This study suggests that an HCA <5 cm confers minimal risk to a pregnant woman and none to her child. HCA increased in size during a quarter of pregnancies, so we recommend close monitoring with ultrasound examinations, enabling intervention if needed. In light of the large proportion of misdiagnosed HCA, LCE-MRI should be performed to prevent unnecessary anxiety in women with a benign liver lesion. Hepatocellular adenoma (HCA) occurs particularly among reproductive women and is associated with the use of oestrogen-containing oral contraceptives, androgen intake, obesity, and metabolic disorders.1,2 The tumour may regress upon cessation of oestrogen-containing oral contraceptives and weight reduction.3,4 
The risk of hepatocellular carcinoma (HCC) during antiviral therapy in patients with chronic hepatitis B (CHB) is inadequately predicted by the scores built from untreated patients. We aimed at developing and validating a risk score to predict HCC in patients with CHB on entecavir or tenofovir treatment. This study analysed population-wide data from the healthcare databases in Taiwan and Hong Kong to identify patients with CHB continuously receiving entecavir or tenofovir. The development cohort included 23,851 patients from Taiwan; 596 (2.50%) of them developed HCC with a three-year cumulative incidence of 3.56% (95% CI 3.26–3.86%). The multivariable Cox proportional hazards model found that cirrhosis, age (cirrhosis and age interacted with each other), male sex, and diabetes mellitus were the risk determinants. These variables were weighted to develop the cirrhosis, age, male sex, and diabetes mellitus (CAMD) score ranging from 0 to 19 points. The score was externally validated in 19,321 patients from Hong Kong. The c indices for HCC in the development cohort were 0.83 (95% CI 0.81–0.84), 0.82 (95% CI 0.81–0.84), and 0.82 (95% CI 0.80–0.83) at the first, second, and third years of therapy, respectively. In the validation cohort, the c indices were 0.74 (95% CI 0.71–0.77), 0.75 (95% CI 0.73–0.78), and 0.75 (95% CI 0.72–0.77) during the first three years, and 0.76 (95% CI 0.74–0.78) and 0.76 (95% CI 0.74–0.77) in the extrapolated fourth and fifth years, respectively. The predicted and observed probabilities of HCC were calibrated in both cohorts. A score <8 and >13 points identified patients at distinctly low and high risks. The easily calculable CAMD score can predict HCC and may inform surveillance policy in patients with CHB during oral antiviral therapy. Hepatitis B virus (HBV) infection is the leading aetiology of hepatocellular carcinoma (HCC) around the globe.1,2 The risk of HCC is a lifelong threat to patients with chronic hepatitis B (CHB).3 Antiviral therapy using nucleos(t)ide analogues (NAs) inhibits HBV replication,4–6 ameliorates hepatic inflammation,7 reverses liver fibrosis,8 and may attenuate hepatocellular carcinogenesis. We and others have shown that NA treatment is associated with risk reduction of HCC in patients with CHB.9–12 In addition, the incidences of HCC decreased over the years while on therapies.13–15 However, antiviral treatment does not completely eliminate the risk of HCC.16 Beyond viral suppression, it remains unclear how to lower the risk further. 
Histological classifications used to diagnose/stage nonalcoholic fatty liver disease (NAFLD) are based on morphology, thus empirical, with undetermined clinical correlates and relevance. We assessed the clinical relevance of the fatty liver inhibition of progression (FLIP) algorithm and the steatosis, activity and fibrosis (SAF) scoring system. 140 consecutive patients with suspected NAFLD and a separate validation cohort of 78 patients enrolled in a therapeutic trial, all with central reading of liver biopsy, were included. FLIP and SAF defined patients with steatohepatitis (NASH), NAFL (non-NASH NAFLD) or non-NAFLD. SAF activity score assessed hepatocyte ballooning and lobular inflammation; histologically severe disease was defined as a SAF activity score of >3 and/or bridging fibrosis or cirrhosis. Clinical, biochemical and metabolic data were analyzed in relation to histology. Patients with NASH according to the FLIP algorithm had a distinct clinical profile from those labeled NAFL, with a higher prevalence of metabolic risk factors (increased BMI, central obesity, serum glucose and HbA1c), more severe insulin resistance (fasting insulin, HOMA-IR values) and higher level of aminotransferases. Similar findings were documented for patients with severe disease vs. those without. Positive linear trends existed between NASH or severe disease and increasing BMI and HOMA-IR. There was a strong association between liver fibrosis and NASH or SAF-defined scores of activity. Patients with either significant or bridging fibrosis overwhelmingly had NASH, and bridging fibrosis most often coexisted with severe activity. The FLIP algorithm/SAF score, although based on purely morphological grounds, are clinically relevant as they identify patients with distinct clinical and biological profiles of disease severity. Disease activity in NAFLD is associated with fibrosis severity.  
The clinical efficacy of ursodeoxycholic acid (UDCA) in primary biliary cholangitis (PBC) remains subject to debate as definitive randomized controlled trials are lacking. We aimed to determine whether UDCA prolongs liver transplant (LT)-free survival in patients with PBC. This international cohort study included patients from the Global PBC Study Group database, originating from 8 countries in Europe and North America. Both UDCA-treated and untreated patients were included. LT and death were assessed as a combined endpoint through Cox regression analyses, with inverse probability treatment weighting (IPTW). In the 3,902 patients included, the mean (SD) age was 54.3 (11.9) years, 3,552 patients (94.0%) were female, 3,529 patients (90.4%) were treated with UDCA and 373 patients (9.6%) were not treated. The median (interquartile range) follow-up was 7.8 (4.1–12.1) years. In total, 721 UDCA-treated patients and 145 untreated patients died or underwent LT. After IPTW, the 10-year cumulative LT-free survival was 79.7% (95% CI 78.1–81.2) among UDCA-treated patients and 60.7% (95% CI 58.2–63.4) among untreated patients (p <0.001). UDCA was associated with a statistically significant reduced risk of LT or death (hazard ratio 0.46, 95% CI 0.40–0.52; p <0.001). The hazard ratio remained statistically significant in all stages of disease. Patients classified as inadequate biochemical responders after 1 year of UDCA had a lower risk of LT or death than patients who were not treated (adjusted hazard ratio 0.56; 95% CI 0.45–0.69; p <0.001). The use of UDCA improves LT-free survival among patients with PBC, regardless of the disease stage and the observed biochemical response. These findings support UDCA as the current universal standard of care in PBC. Primary biliary cholangitis (PBC) is a chronic and usually slowly progressive liver disease with autoimmune features, histologically characterized by destruction of the small intrahepatic bile ducts.1,2 The disease is primarily diagnosed based on an otherwise unexplained chronic elevation of serum alkaline phosphatase levels and the presence of anti-mitochondrial antibodies. Early identification of individuals with PBC is clinically challenging as symptoms are frequently absent. Identifying and managing patients with PBC is important, however, as the disease may silently progress towards cirrhosis and the survival of affected patients is substantially impaired.3 
Recent studies reveal that the rate of normal on-treatment alanine aminotransferase (ALT) appears different for different nucleos(t)ide analogues (NAs); yet its clinical significance is unclear. We aimed to evaluate the impact of normal on-treatment ALT during antiviral treatment with entecavir (ETV) or tenofovir disoproxil fumarate (TDF) in patients with chronic hepatitis B (CHB). A territory-wide cohort of patients with CHB who received ETV and/or TDF in 2005–2016 was identified. Serial on-treatment ALT levels were collected and analyzed. Normal on-treatment ALT (ALT-N) was defined as ALT <30 U/L in males and <19 U/L in females. The primary and secondary outcomes were composite hepatic events (including hepatocellular carcinoma) based on diagnostic codes. Patients with hepatic events before or during the first year of antiviral treatment or follow-up <1 year were excluded. A total of 21,182 patients with CHB (10,437 with and 10,745 without ALT-N at 12 months after antiviral treatment) were identified and followed for 4.0 ± 1.7 years. Patients with and without ALT-N differed in baseline ALT (58 vs. 61 U/L), hepatitis B virus DNA (4.9 vs. 5.1 log10 IU/ml) and cirrhosis status (8.8% vs. 10.5%). A total of 627 (3.0%) patients developed composite hepatic events. Compared to no ALT-N, ALT-N at 3, 6, 9 and 12 months reduced the risk of hepatic events, after adjustment for baseline ALT and other important covariates, with adjusted hazard ratios (95% CI) of 0.61 (0.49–0.77), 0.55 (0.45–0.67), 0.54 (0.44–0.65) and 0.51 (0.42–0.61) respectively (all p <0.001). The cumulative incidence (95% CI) of composite hepatic events at six years was 3.51% (3.06%-4.02%) in ALT-N and 5.70% (5.15%–6.32%) in the no ALT-N group (p <0.001). Normal on-treatment ALT is associated with a lower risk of hepatic events in patients with CHB receiving NA treatment, translating into improved clinical outcomes in these patients. Elevated alanine aminotransferase (ALT) above two times the upper limit of normal (ULN) in patients with chronic hepatitis B (CHB) is one of the key indications for antiviral treatment recommended by international guidelines.1–3 Normal on-treatment ALT is often regarded as a biochemical response to antiviral treatment. Being one of the most commonly used tests for patients with CHB, ALT level correlates with hepatic necroinflammation.4 The optimal ALT cutoffs are now set at 30 IU/L for men and 19 IU/L for women by the American Association for the Study of Liver Diseases (AASLD),3 as high-normal ALT levels according to traditional cutoffs ranging from 40 U/L to 70 U/L are also associated with cirrhosis5 and liver-related mortality.6 
Obeticholic acid (OCA), a farnesoid X receptor agonist, increases total and low-density lipoprotein cholesterol (LDL-C) in patients with non-alcoholic steatohepatitis. In the present study, we aimed to evaluate the impact of OCA therapy on lipoprotein sub-particles. This study included 196 patients (99 OCA group and 97 placebo group) who were enrolled in the FLINT trial and had samples available for lipid analysis and liver biopsies at enrollment and end-of-treatment (EOT) at 72 weeks. Very low-density lipoprotein (VLDL), low-density lipoprotein (LDL), and high-density lipoprotein (HDL) particles were evaluated at baseline, 12 and 72 weeks after randomization, and 24 weeks following EOT. Baseline lipoprotein profiles were similar among OCA and placebo groups. OCA did not affect total VLDL particle concentrations, but OCA vs. placebo treatment was associated with decreased large VLDL particle concentration at 12 weeks (baseline-adjusted mean: 6.8 vs. 8.9 nmol/L; p = 0.002), mirrored by an increase in less atherogenic, small VLDL particle concentration (33.9 vs. 28.0 nmol/L; p = 0.02). After 12 weeks, total LDL particle concentration was higher in the OCA group than the placebo group (1,667 vs. 1,329 nmol/L; p <0.0001), characterized by corresponding increases in both less atherogenic, large-buoyant LDL (475 vs. 308 nmol/L; p ≤0.001) and more atherogenic small-dense LDL particles (1,015 vs. 872 nmol/L; p = 0.002). The changes in LDL particle concentrations were similar between treatment groups (OCA and placebo) 24 weeks following EOT due to improvement in the OCA cohort. Compared to placebo, a reduction in total HDL particle concentration, particularly large and medium HDL particles, was noted in the OCA-treated patients, but this resolved after drug discontinuation. OCA therapy is associated with increases in small VLDL particles, large and small LDL particles, and a reduction in HDL particles at 12 weeks. These lipoprotein concentrations reverted to baseline values 24 weeks after drug discontinuation. Non-alcoholic fatty liver disease (NAFLD) is the most common etiology of chronic liver disease, affecting nearly a third of the United States population.1 Non-alcoholic steatohepatitis (NASH) is the clinically aggressive variant of NAFLD that is characterized histologically by hepatic steatosis along with necro-inflammatory activity.1 In the absence of approved pharmacological therapy, NASH is increasingly being targeted for drug development efforts.3 Obeticholic acid (OCA) is a farnesoid X receptor (FXR) agonist that was recently shown to improve liver histology in patients with NASH in a randomized, placebo-controlled clinical trial.4 FXR is a bile acid-binding transcription factor belonging to the super family of nuclear receptors. Because of its central role in inflammation, glucose and lipid metabolism, FXR agonism is an attractive therapeutic target in NASH. 
Lactation lowers blood glucose and triglycerides, and increases insulin sensitivity. We hypothesized that a longer duration of lactation would be associated with lower prevalence of non-alcoholic fatty liver disease (NAFLD), which is the leading cause of chronic liver disease in the United States. Participants from the Coronary Artery Risk Development in Young Adults cohort study who delivered ≥ 1 child post-baseline (Y0: 1985–1986), and underwent CT quantification of hepatic steatosis 25 years following cohort entry (Y25: 2010–2011) were included (n = 844). The duration of lactation was summed for all post-baseline births, and NAFLD at Y25 was assessed by central review of CT images and defined by liver attenuation ≤ 40 Hounsfield Units after exclusion of other causes of hepatic steatosis. Unadjusted and multivariable logistic regression analyses were performed using an a priori set of confounding variables; age, race, education, and baseline body mass index. Of 844 women who delivered after baseline (48% black, 52% white, mean age 49 years at Y25 exam), 32% reported lactation duration of 0 to 1 month, 25% reported >1 to 6 months, 43% reported more than 6 months, while 54 (6%) had NAFLD. Longer lactation duration was inversely associated with NAFLD in unadjusted logistic regression. For women who reported >6 months lactation compared to those reporting 0–1 month, the odds ratio for NAFLD was 0.48 (95% CI 0.25–0.94; p = 0.03) and the association remained after adjustment for confounders (adjusted odds ratio 0.46; 95% CI 0.22–0.97; p = 0.04). A longer duration of lactation, particularly greater than 6 months, is associated with lower odds of NAFLD in mid-life and may represent a modifiable risk factor for NAFLD. Non-alcoholic fatty liver disease (NAFLD) is an increasingly common cause of cirrhosis and hepatocellular carcinoma and is on trajectory to become the most frequent indication for liver transplantation in the United States,1,2 however therapeutic options are limited. While NAFLD is recognized as the hepatic manifestation of metabolic dysfunction, it also portends an increased risk for incident diabetes mellitus (DM) and metabolic syndrome suggesting a complex bidirectional relationship.3,4 
Hepatocellular carcinoma (HCC) is a common cancer worldwide and remains a major clinical challenge. Ketoconazole, a traditional antifungal agent, has attracted considerable attention as a therapeutic option for cancer treatment. However, its mechanism of action is still not clearly defined. We aimed to evaluate the effect of ketoconazole on HCC and investigate the underlying mechanisms. We examined the antitumor effect of ketoconazole on HCC cells, cell line-derived xenografts, and a patient-derived xenograft (PDX) model. Ketoconazole-induced mitophagy was quantified by immunofluorescence, immunoblotting and transmission electron microscopy analysis. We used mitophagy inhibitors to study the role of mitophagy on HCC cell death induced by ketoconazole. The role of cyclooxygenase-2 (COX-2 [encoded by PTGS2]) on ketoconazole-induced mitophagy was evaluated using gain- and loss-of-function methods. The synergistic effect of ketoconazole with sorafenib on HCC was measured in vivo and in vitro. Ketoconazole stimulated apoptosis in HCC cells by triggering mitophagy in vitro and in vivo. Mechanistically, ketoconazole downregulated COX-2, which led to PINK1 accumulation and subsequent mitochondrial translocation of Parkin (PRKN), and thereby promoted mitophagy-mediated mitochondrial dysfunction. Inhibiting mitophagy alleviated ketoconazole-induced mitochondrial dysfunction and apoptosis, supporting a causal role for mitophagy in the antitumor effect of ketoconazole. In the HCC PDX model, ketoconazole demonstrated a marked antitumor effect characterized by COX-2 downregulation, mitophagy activation, and apoptosis induction. Moreover, ketoconazole acted synergistically with sorafenib to suppress HCC xenograft growth in vivo. Our results demonstrate a novel link between ketoconazole and mitophagy machinery, providing preclinical proof of concept for the use of ketoconazole in HCC treatment. Hepatocellular carcinoma (HCC) is the second leading cause of cancer-related death worldwide.1 Among all the treatment options for HCC, surgical resection remains the main approach, with the best long-term survival for early-onset disease.2 However, the majority of patients with HCC are diagnosed at late stages when curative strategies are not applicable. Thus, the options become extremely limited for those patients who are referred to systemic therapy because of the low response rates of chemotherapeutic agents in HCC treatment.3,4 Despite the current use of an oral multikinase inhibitor, sorafenib, in the standard care for advanced HCC, multiple clinical trials on sorafenib revealed limited survival benefits in patients with HCC, which is normally attributed to primary and acquired drug resistance.5 As such, new therapeutic agents are urgently needed to improve outcomes in patients with HCC. 
To improve outcomes of two-staged hepatectomies for large/multiple liver tumors, portal vein ligation (PVL) has been combined with parenchymal transection (associating liver partition and portal vein ligation for staged hepatectomy [coined ALPPS]) to greatly accelerate liver regeneration. In a novel ALPPS mouse model, we have reported paracrine Indian hedgehog (IHH) signaling from stellate cells as an early contributor to augmented regeneration. Here, we sought to identify upstream regulators of IHH. ALPPS in mice was compared against PVL and additional control surgeries. Potential IHH regulators were identified through in silico mining of transcriptomic data. c-Jun N-terminal kinase (JNK1 [Mapk8]) activity was reduced through SP600125 to evaluate its effects on IHH signaling. Recombinant IHH was injected after JNK1 diminution to substantiate their relationship during accelerated liver regeneration. Transcriptomic analysis linked Ihh to Mapk8. JNK1 upregulation after ALPPS was validated and preceded the IHH peak. On immunofluorescence, JNK1 and IHH co-localized in alpha-smooth muscle actin-positive non-parenchymal cells. Inhibition of JNK1 prior to ALPPS surgery reduced liver weight gain to PVL levels and was accompanied by downregulation of hepatocellular proliferation and the IHH-GLI1-CCND1 axis. In JNK1-inhibited mice, recombinant IHH restored ALPPS-like acceleration of regeneration and re-elevated JNK1 activity, suggesting the presence of a positive IHH-JNK1 feedback loop. JNK1-mediated induction of IHH paracrine signaling from hepatic stellate cells is essential for accelerated regeneration of parenchymal mass. The JNK1-IHH axis is a mechanism unique to ALPPS surgery and may point to therapeutic alternatives for patients with insufficient regenerative capacity. The unique ability of mammalian liver to regain mass after tissue loss has revolutionized the treatment and cure of many patients with liver tumors. However, there are limitations to effective regeneration, as hepatic failure may develop after extensive liver resection. This entity, known as the small-for-size syndrome (SFSS), results from an insufficient functional volume of the liver remnant and remains the most frequent cause of death due to liver surgery.1,2 Two-staged hepatectomies were introduced to reduce the SFSS risk. Typically, the portal vein draining the part of the liver containing the tumor is occluded (step 1), causing growth of the contralateral liver part (defined as the functional liver remnant [FLR]); when the FLR has gained sufficient functional volume, step 2 (resection of the diseased part) is performed.2,3 Nevertheless, in some cases regeneration is still insufficient, or the considerable time period between step 1 and 2 allows for further progression of the disease.4,5 To additionally reduce the risk of SFSS in patients with large or multiple liver tumors, associating liver partition and portal vein ligation (PVL) for staged hepatectomy ([ALPPS] the combination of PVL with parenchymal transection) has been showcased as a procedure that induces accelerated liver regeneration and greatly reduces the interval between steps, allowing treatment of patients otherwise deemed unresectable.6,7 This procedure, introduced about five years ago, has gained sustained acceptance with more than 1,000 cases included in an international registry (http://www.alpps.net/?q=registry). 
Cells of hematopoietic origin, including macrophages, are generally radiation sensitive, but a subset of Kupffer cells (KCs) is relatively radioresistant. Here, we focused on the identity of the radioresistant KCs in unmanipulated mice and the mechanism of radioresistance. We employed Emr1- and inducible CX3Cr1-based fate-mapping strategies combined with the RiboTag reporter to identify the total KCs and the embryo-derived KCs, respectively. The KC compartment was reconstituted with adult bone-marrow-derived KCs (bm-KCs) using clodronate depletion. Mice were lethally irradiated and transplanted with donor bone marrow, and the radioresistance of bone-marrow- or embryo-derived KCs was studied. Gene expression was analyzed using in situ mRNA isolation via RiboTag reporter mice, and the translatomes were compared among subsets. Here, we identified the radioresistant KCs as the long-lived subset that is derived from CX3CR1-expressing progenitor cells in fetal life, while adult bm-KCs do not resist irradiation. While both subsets upregulated the Cdkn1a gene, encoding p21-cip1/WAF1 protein, radioresistant embryo-derived KCs showed a greater increase in response to irradiation. In the absence of this molecule, the radioresistance of KCs was compromised. Replacement KCs, derived from adult hematopoietic stem cells, differed from radioresistant KCs in their expression of genes related to immunity and phagocytosis. Here, we show that, in the murine liver, a subset of KCs of embryonic origin resists lethal irradiation through Cdkn1a upregulation and is maintained for a long period, while bm-KCs do not survive lethal irradiation. Kupffer cells (KCs), the liver-resident macrophages, are important innate immune sensors that respond to liver stress, and may either stimulate or suppress immunity.1 In contrast to most other leukocytes, tissue-resident macrophages in the brain (microglia) and epidermis (Langerhans cells) are highly radioresistant.2,3 However, in the liver, only a subset of KCs resists lethal irradiation, while the other subset is replaced by donor bone-marrow-monocyte-derived KCs (bm-KCs).4,5 This radioresistant KC subset is long lived, and they are not recruited to foci of inflammation, and thus, are termed sessile KCs.4,6 Gene expression and epigenetic chromatin modification analysis have confirmed that bm-KCs become broadly similar to sessile KC post-recovery, but not identical.5,7 While the sessile KC subset is identified post-irradiation, the identity of this subset in unmanipulated mice and why only a subset of KCs resists irradiation are not yet understood. 
The popular sense of the word “cure” implies that a patient treated for a specific disease will return to have the same life-expectancy as if he/she had never had the disease. In analytic terms, it translates into the concept of statistical cure which occurs when a group of patients returns to having similar mortality to a reference population. Aim of the study was to assess the probability of being cured from hepatocellular carcinoma (HCC) by hepatic resection. Data from 2523 patients undergoing resection for HCC were used to fit statistical cure models to compare disease-free survival (DFS) after surgery to survival expected for chronic hepatitis – cirrhotic patients and the general population, matched by sex, age, race/ethnicity and year of diagnosis. The probability of resection to provide the same life-expectancy of patients with chronic hepatitis – cirrhosis, was 26.3%. The conditional probability to achieve this result was time-dependent, requiring about 8.9 year for being accomplished with 95% certainty. Considering the general population as reference, the cure fraction decreased to 17.1%. Uncured patients had a median DFS of 1.5 years. In multi-variable analysis, patient’s age and the risk for early HCC recurrence (within 2 years) were independent determinants of the chance of cure (p<0.001). The chances of being cured ranged between 36.0% for subjects at low risk for early recurrence and only about 3.6% for those at high risk. Estimates of the chance of being cured of HCC by resection showed that this goal is achievable and its likelihood increases with the passing of recurrence-free time. Present information can be used to accurately inform patients and make informed clinical decisions.  
Hepatic ischemia-reperfusion injury (IRI) is a major complication of hemorrhagic shock, liver resection and transplantation. YAP, a key downstream effector of the Hippo pathway, is essential for determining cell fate and maintaining homeostasis in the liver. We aimed to elucidate its role in IRI. The role of YAP/Hippo signaling was systematically studied in biopsy specimens from 60 patients after orthotopic liver transplantation (OLT), and in a mouse model of liver warm IRI. Human biopsy specimens were collected after 2–10 h of cold storage and 3 h post-reperfusion, before being screened by western blot. In the mouse model, the role of YAP was probed by activating or inhibiting YAP prior to ischemia-reperfusion. In human biopsies, high post-OLT YAP expression was correlated with well-preserved histology and improved hepatocellular function at postoperative day 1–7. In mice, the ischemia insult (90 min) triggered intrinsic hepatic YAP expression, which peaked at 1–6 h of reperfusion. Activation of YAP protected the liver against IR-stress, by promoting regenerative and anti-oxidative gene induction, while diminishing oxidative stress, necrosis/apoptosis and the innate inflammatory response. Inhibition of YAP aggravated hepatic IRI and suppressed repair/anti-oxidative genes. In mouse hepatocyte cultures, activating YAP prevented hypoxia-reoxygenation induced stress. Interestingly, YAP activation suppressed extracellular matrix synthesis and diminished hepatic stellate cell (HSC) activation, whereas YAP inhibition significantly delayed hepatic repair, potentiated HSC activation, and enhanced liver fibrosis at 7 days post-IRI. Notably, YAP activation failed to protect Nrf2-deficient livers against IR-mediated damage, leading to extensive fibrosis. Our novel findings document the crucial role of YAP in IR-mediated hepatocellular damage and liver fibrogenesis, providing evidence of a potential therapeutic target for the management of sterile liver inflammation in transplant recipients. Orthotopic liver transplantation (OLT) is considered the standard treatment for end-stage liver disease. Following organ retrieval, cold preservation and warm ischemia-reperfusion injury (IRI) can lead to impaired graft function, including primary graft non-function, which may predispose patients to acute and chronic rejection. Indeed, by contributing to a shortage of available donor organs, IRI represents one of the most challenging problems in transplantation.1–4 Thus, novel therapeutic concepts to combat IRI are needed to improve OLT outcomes and expand the donor organ pool. 
Binge alcohol exposure causes gut leakiness, contributing to increased endotoxemia and inflammatory liver injury, although the molecular mechanisms are still elusive. This study was aimed at investigating the roles of apoptosis of enterocytes and nitration followed by degradation of intestinal tight junction (TJ) and adherens junction (AJ) proteins in binge alcohol-induced gut leakiness. The levels of intestinal (ileum) junctional complex proteins, oxidative stress markers and apoptosis-related proteins in rodents, T84 colonic cells and autopsied human ileums were determined by immunoblot, immunoprecipitation, immunofluorescence, and mass-spectral analyses. Binge alcohol exposure caused apoptosis of gut enterocytes with elevated serum endotoxin and liver injury. The levels of intestinal CYP2E1, iNOS, nitrated proteins and apoptosis-related marker proteins were significantly elevated in binge alcohol-exposed rodents. Differential, quantitative mass-spectral analyses of the TJ-enriched fractions of intestinal epithelial layers revealed that several TJ, AJ and desmosome proteins were decreased in binge alcohol-exposed rats compared to controls. Consistently, the levels of TJ proteins (claudin-1, claudin-4, occludin and zonula occludens-1), AJ proteins (β-catenin and E-cadherin) and desmosome plakoglobin were very low in binge alcohol-exposed rats, wild-type mice, and autopsied human ileums but not in Cyp2e1-null mice. Additionally, pretreatment with specific inhibitors of CYP2E1 and iNOS prevented disorganization and/or degradation of TJ proteins in alcohol-exposed T84 colonic cells. Furthermore, immunoprecipitation followed by immunoblot confirmed that intestinal TJ and AJ proteins were nitrated and degraded via ubiquitin-dependent proteolysis, resulting in their decreased levels. These results demonstrated for the first time the critical roles of CYP2E1, apoptosis of enterocytes, and nitration followed by ubiquitin-dependent proteolytic degradation of the junctional complex proteins, in promoting binge alcohol-induced gut leakiness and endotoxemia, contributing to inflammatory liver disease. Excessive alcohol intake can damage many organs, causing deaths in severe cases. Heavy alcohol intake is also known to cause gut leakiness, contributing to increased endotoxemia and inflammatory tissue damage in the liver and brain.1–4 Various pathological conditions, such as HIV infection,5–7 obesity,8 and burn injury,9 are known to increase gut leakiness and endotoxemia. In addition, binge alcohol10 and non-alcoholic substances such as western-style high-fat diets11 and fructose12 can stimulate gut leakiness, leading to elevated serum endotoxin and liver inflammation. Furthermore, alcoholic patients with cirrhosis had higher levels of endotoxin than those without cirrhosis.4,13 These conditions observed in humans can be replicated in experimental animals or cultured Caco-2 and T84 colonic cells. In fact, both other laboratories and ourselves have reported that binge alcohol caused gut leakiness in an ethanol-inducible cytochrome P450-2E1 (CYP2E1)-dependent manner,1,2 since Cyp2e1-null mice were resistant to alcohol-induced gut leakiness despite extremely high doses of alcohol (three oral doses of 6 g ethanol/kg/dose) at 12 h intervals.10 Furthermore, pretreatment with a specific siRNA to CYP2E1 in Caco-2 human colonic cells2 or a chemical inhibitor of CYP2E1 chlormethiazole (CMZ) or an antioxidant N-acetylcysteine10 efficiently prevented alcohol-induced epithelial barrier dysfunction or gut leakiness, supporting the contributing role of CYP2E1-dependent oxidative stress in gut leakiness. Despites numerous reports, the detailed molecular mechanisms by which binge alcohol and CYP2E1-mediated oxidative stress stimulate gut leakiness are still poorly understood. 
The efficacy of fresh frozen plasma (FFP) transfusion in enhancing thrombin generation in patients with cirrhosis and impaired conventional coagulation tests has not been sufficiently explored. Thus, we aimed to assess the effect of FFP transfusion on thrombin generation in these patients. Fifty-three consecutive patients receiving a standard dose of FFP to treat bleeding and/or before invasive procedures – if international normalized ratio (INR)/prothrombin time (PT) ratio were ≥1.5 – were prospectively enrolled. The primary endpoint was the amelioration of endogenous thrombin potential (ETP) with thrombomodulin (ETP-TM) after transfusion, which corresponds to the total amount of generated thrombin. INR/PT ratio and activated partial thromboplastin time (aPTT) were also assessed before and after transfusion. FFP enhanced ETP-TM by 5.7%, from 973 (731–1,258) to 1,028 (885–1,343 nM × min; p = 0.019). Before transfusion, evidence of normal or high ETP-TM was found in 94% of patients, even in those with bacterial infections. Only 1 (1.9%) patient had ETP-TM values reverting to the normal range after transfusion. Notably, no patients with low ETP-TM had bleeding. The median decrease in ETP-TM was 8.3% and the mean was 12.8% in 18 (34%) patients after transfusion (from 1,225 [1,071–1,537] to 1,124 [812–1,370] nM × min; p ≤0.0001). Similar responses to FFP transfusion were observed in patients with compensated and acute decompensated cirrhosis, acute-on-chronic liver failure, infection or shock. FFP significantly ameliorated INR and aPTT values (p <0.0001), but in a minority of patients the values were reduced to less than the cut-off point of 1.5. FFP transfusion enhanced thrombin generation and ameliorated conventional coagulation tests to normal values in a limited number of patients, and slightly decreased thrombin generation in 34% of cases. Over the last decade there has been considerable progress in understanding the complex mechanisms behind the coagulopathy of cirrhosis. Landmark studies have demonstrated that there is a substantial balance between pro- and anticoagulant factors in cirrhosis. Despite preserving normal thrombin generation, this balance is relatively unstable and prone to tip towards hemorrhage or thrombosis, depending on the prevailing circumstantial risk factors to which patients are exposed.1,2 In this process, factor VIII, an endothelial-released coagulation factor (typically increased in cirrhosis) to some extent counteracts the decline of the other procoagulant factors synthesized by the liver. As the levels of the naturally occurring anticoagulants are also decreased in cirrhosis, the anticoagulant effect of protein C, protein S and antithrombin on thrombin generation is attenuated when compared to normal individuals.3 Indeed, it has been shown in vitro that plasma of patients with cirrhosis may generate adequate thrombin amounts when exposed to tissue factor and exogenous phospholipids, provided that plasmatic protein C is activated by its main physiological activator, thrombomodulin (TM).3–4 
Fatty liver disease, including non-alcoholic fatty liver (NAFLD) and steatohepatitis (NASH), has been associated with increased intestinal barrier permeability and translocation of bacteria or bacterial products into the blood circulation. In this study, we aimed to unravel the role of both intestinal barrier integrity and microbiota in NAFLD/NASH development. C57BL/6J mice were fed with high-fat diet (HFD) or methionine-choline-deficient diet for 1 week or longer to recapitulate aspects of NASH (steatosis, inflammation, insulin resistance). Genetic and pharmacological strategies were then used to modulate intestinal barrier integrity. We show that disruption of the intestinal epithelial barrier and gut vascular barrier (GVB) are early events in NASH pathogenesis. Mice fed HFD for only 1 week undergo a diet-induced dysbiosis that drives GVB damage and bacterial translocation into the liver. Fecal microbiota transplantation from HFD-fed mice into specific pathogen-free recipients induces GVB damage and epididymal adipose tissue enlargement. GVB disruption depends on interference with the WNT/β-catenin signaling pathway, as shown by genetic intervention driving β-catenin activation only in endothelial cells, preventing GVB disruption and NASH development. The bile acid analogue and farnesoid X receptor agonist obeticholic acid (OCA) drives β-catenin activation in endothelial cells. Accordingly, pharmacologic intervention with OCA protects against GVB disruption, both as a preventive and therapeutic agent. Importantly, we found upregulation of the GVB leakage marker in the colon of patients with NASH. We have identified a new player in NASH development, the GVB, whose damage leads to bacteria or bacterial product translocation into the blood circulation. Treatment aimed at restoring β-catenin activation in endothelial cells, such as administration of OCA, protects against GVB damage and NASH development. Fatty liver disease is characterized by a series of pathological conditions ranging from hepatic lipid accumulation (steatosis), to hepatocyte degeneration (ballooning), inflammation (steatohepatitis) and, eventually, cirrhosis and hepatocellular carcinoma.1,2 Fatty liver disease may be the result of long-term excessive ethanol consumption (alcoholic liver disease) or of visceral obesity and metabolic syndrome without ethanol consumption, leading to non-alcoholic fatty liver disease (NAFLD) which can evolve to non-alcoholic steatohepatitis (NASH).3 
The degree of cholestasis is an important disease driver in alcoholic hepatitis, a severe clinical condition that needs new biomarkers and targeted therapies. We aimed to identify the largely unknown mechanisms and biomarkers linked to cholestasis in alcoholic hepatitis. Herein, we analyzed a well characterized cohort of patients with alcoholic hepatitis and correlated clinical and histological parameters and outcomes with serum bile acids and fibroblast growth factor 19 (FGF19), a major regulator of bile acid synthesis. We found that total and conjugated bile acids were significantly increased in patients with alcoholic hepatitis compared with controls. Serum FGF19 levels were strongly increased and gene expression of FGF19 was induced in biliary epithelial cells and ductular cells of patients with alcoholic hepatitis. De novo bile acid synthesis (CYP7A1 gene expression and C4 serum levels) was significantly decreased in patients with alcoholic hepatitis. Importantly, total and conjugated bile acids correlated positively with FGF19 and with disease severity (model for end-stage liver disease score). FGF19 correlated best with conjugated cholic acid, and model for end-stage liver disease score best with taurine-conjugated chenodeoxycholic acid. Univariate analysis demonstrated significant associations between FGF19 and bilirubin as well as gamma glutamyl transferase, and negative correlations between FGF19 and fibrosis stage as well as polymorphonuclear leukocyte infiltration, in all patients with alcoholic hepatitis. Serum FGF19 and bile acids are significantly increased in patients with alcoholic hepatitis, while de novo bile acid synthesis is suppressed. Modulation of bile acid metabolism or signaling could represent a promising target for treatment of alcoholic hepatitis in humans. Alcohol abuse is the most important cause of liver disease worldwide.1 The most severe form of alcoholic liver disease is alcoholic hepatitis with mortality rates of 20–40% at 1–6 months, and a 90-day mortality rate of up to 75% in severe alcoholic hepatitis.2–4 Corticosteroids are the only effective medical therapy but failure in many patients has been reported.2 There is no cure for patients not responding to medical therapy, except for early liver transplantation that is offered in some centers to a highly selected group of patients.5 
Although the majority of patients with non-alcoholic fatty liver disease (NAFLD) have only steatosis without progression, a sizeable fraction develop non-alcoholic steatohepatitis (NASH), which can lead to cirrhosis and hepatocellular carcinoma (HCC). Many established diet-induced mouse models for NASH require 24–52 weeks, which makes testing for drug response costly and time consuming. We have sought to establish a murine NASH model with rapid progression of extensive fibrosis and HCC by using a western diet (WD), which is high-fat, high-fructose and high-cholesterol, combined with low weekly dose of intraperitoneal carbon tetrachloride (CCl4), which serves as an accelerator. C57BL/6J mice were fed a normal chow diet ± CCl4 or WD ± CCl4 for 12 and 24 weeks. Addition of CCl4 exacerbated histological features of NASH, fibrosis, and tumor development induced by WD, which resulted in stage 3 fibrosis at 12 weeks and HCC development at 24 weeks. Furthermore, whole liver transcriptomic analysis indicated that dysregulated molecular pathways in WD/CCl4 mice and immunologic features were similar to those of human NASH. Our mouse NASH model exhibits rapid progression of advanced fibrosis and HCC, and mimics histological, immunological and transcriptomic features of human NASH, suggesting that it will be a useful experimental tool for preclinical drug testing. Non-alcoholic fatty liver disease (NAFLD) is a rising cause of chronic liver disease worldwide. Although the majority of patients with NAFLD have only steatosis without progression, a sizeable fraction develop non-alcoholic steatohepatitis (NASH), which can lead to cirrhosis, hepatocellular carcinoma (HCC), and increased liver-related mortality.1 The prevalence of NAFLD in the US population is estimated at ∼24% (or ∼65 million) and up to a third of these individuals have NASH.2,3 The prevalence of NAFLD is steadily increasing in parallel with the rising prevalence of obesity. NAFLD/NASH is already the third leading indication for liver transplantation, and the second leading cause of HCC requiring liver transplantation in the US,4 and it is likely to be the leading indication for transplantation by 2020. 
Current antiviral therapies lack the potential to eliminate persistent hepatitis B virus (HBV) infection. HBV-specific T cells are crucial for HBV control and have recently been shown to be protective in patients following discontinuation of antiviral therapy. Thus, T cell-based approaches may greatly improve the therapeutic landscape of HBV infection. We aimed to augment HBV-specific CD4 T cells from chronically infected patients by targeting different immunological pathways. Expression of various co-stimulatory and inhibitory receptors on HBV- and influenza-specific CD4 T cells was analyzed directly ex vivo by MHC class II-tetramers. Patients infected with HBV genotype D were screened for CD4 T cell responses by IFN-γ ELISpot and intracellular cytokine staining following stimulation with overlapping peptides (OLPs) spanning the HBV-polyprotein. Stimulation with recombinant IL-7, an agonistic OX40-antibody or blockade of PD-L1 was performed in antigen-specific in vitro cultures. Cytokine secretion and expression of transcription factors were analyzed by flow cytometry. Responses targeting influenza, Epstein-Barr virus and tetanus toxoid served as controls. Tetramer-staining revealed that the IL-7 receptor-alpha (CD127), OX40 and PD-1 constitute possible therapeutic targets as they were all strongly expressed on HBV-specific CD4 T cells ex vivo. The HBV-specific CD4 T cell responses identified by OLP screening targeted predominantly the HBV-polymerase and core proteins. Combined OX40 stimulation and PD-L1 blockade significantly augmented IFN-γ and IL-21 producing HBV-specific CD4 T cells in vitro, suggesting active T helper type 1 cell and follicular T helper cell programs. Indeed, transcription factors T-bet and Bcl6 were strongly expressed in cytokine-producing cells. Combined OX40 stimulation and PD-L1 blockade augmented secretion of the helper T cell signature cytokines IFN-γ and IL-21, suggesting that immunotherapeutic approaches can improve HBV-specific CD4 T cell responses. Persistent infection with the hepatitis B virus (HBV) is a major risk factor for the development of chronic liver injury, cirrhosis and hepatocellular carcinoma and affects an estimated 350 million people worldwide.1 While there is a prophylactic vaccination to prevent chronic infection, therapeutic approaches for persistent infection are limited and mostly fail to eliminate the virus.2 Promising therapeutic approaches include T cell-based immunotherapy as both HBV-specific CD4 and CD8 T cells have been shown to be required for viral clearance but are functionally impaired in the context of chronic infection.3–5 
Recently revised international guidelines for hepatocellular carcinoma (HCC) suggest that patients with inadequate ultrasonography be assessed by alternative imaging modalities. Given short scan time and the absence of contrast agent-associated risks, non-enhanced magnetic resonance imaging (MRI) has potential as a surveillance tool. This study compared the performance of non-enhanced MRI and ultrasonography for HCC surveillance in high-risk patients. We included 382 high-risk patients in a prospective cohort who underwent 1 to 3 rounds of paired gadoxetic acid-enhanced MRI and ultrasonography. Non-enhanced MRI, consisting of diffusion-weighted imaging (DWI) and T2-weighted imaging, was simulated and retrospectively analyzed, with results considered positive when lesion(s) ≥ 1cm showed diffusion restriction or mild–moderate T2 hyperintensity. Ultrasonography results were retrieved from patient records. HCC was diagnosed histologically and/or radiologically. Sensitivity, positive predictive value (PPV), specificity, and negative predictive value (NPV) were evaluated using generalized estimating equations. Forty-eight HCCs were diagnosed in 43 patients. Per-lesion and per-exam sensitivities of non-enhanced MRI were 77.1% and 79.1%, respectively, higher than ultrasonography (25.0% and 27.9%, respectively, P <0.001). Per-lesion and per-exam PPVs were higher for non-enhanced MRI (56.9% and 61.8%, respectively) than ultrasonography (16.7% and 17.7%, respectively). Specificities of non-enhanced MRI (97.9%) and ultrasonography (94.5%) differ significantly (P <0.001). NPV was higher for non-enhanced MRI (99.1%) than ultrasonography (96.9%). Estimated scan time of non-enhanced MRI was <6 minutes. Given the high performance, short scan time, and the lack of contrast agent-associated risks, non-enhanced MRI is a promising option for HCC surveillance in high-risk patients. International guidelines recommend that patients at high risk for hepatocellular carcinoma (HCC) undergo ultrasonography (US) every 6 months.1-3 Recent studies, however, revealed that US had low sensitivity: only 63% in detecting early HCC and approximately 20% in detecting very early stage HCC.4-6 Recently revised guidelines now suggest that selected patients with inadequate US examinations be assessed by alternative methods, such as computed tomography (CT) or magnetic resonance imaging (MRI).1-3 Various HCC surveillance protocols using CT or MRI have been investigated.6-12 Because of the repetitive nature of surveillance tests, the cumulative radiation hazard of periodic CT scans should not be neglected.13 Gadoxetic acid-enhanced MRI has shown excellent performance in diagnosing HCC,14,15 suggesting that periodic MRI may be the best option for HCC surveillance.6 However, the high cost and long imaging acquisition time of full-protocol gadoxetic acid-enhanced MRI can hamper its widespread use. Abbreviated MRI protocols, including hepatobiliary phase (HBP) imaging using gadoxetic acid without dynamic enhanced images, have therefore been tested.8,9,12 Regardless of the inclusion of dynamic sequences, however, gadoxetic acid-enhanced MRI has various drawbacks associated with the use of gadolinium agent, including its long-term retention in human tissues.16,17 
Recently the Amsterdam-Oxford model (AOM) was introduced as a prognostic model to assess the risk of death and/or liver transplantation (LT) in primary sclerosing cholangitis (PSC). We aimed to validate and assess the utility of the AOM. Clinical and laboratory data were collected from the time of PSC diagnosis until the last visit or time of LT or death. The AOM was calculated at yearly intervals following PSC diagnosis. Discriminatory performance was assessed by calculation of the C-statistic and prediction accuracy by comparing the predicted survival with the observed survival in Kaplan-Meier estimates. A grid search was performed to identify the most discriminatory AOM threshold. A total of 534 patients with PSC and a mean (SD) age of 39.2 (13.1) years were included. The diagnosis was large duct PSC in 466 (87%), PSC with features of autoimmune hepatitis in 52 (10%) and small-duct PSC in 16 (3%). During the median (IQR) follow-up of 7.8 (4.0–12.6) years, 167 patients underwent LT and 65 died. The median LT-free survival was 13.2 (11.8–14.7) years. The C-statistic of the AOM ranged from 0.67 at baseline to 0.75 at 5 years of follow-up. The difference between the predicted and observed survival ranged from −1.6% at 1 year to + 3.9% at 5 years of follow-up. Patients that developed AOM scores >2.0 were at significant risk of LT or death (time-dependent hazard ratio 4.09; 95% CI 2.99–5.61). In this large cohort of patients with PSC, the AOM showed an adequate discriminative performance and good prediction accuracy at PSC diagnosis and during follow-up. This study further validates the AOM as a valuable risk stratification tool in PSC and extends its utility. Primary sclerosing cholangitis (PSC) is a chronic, variably progressive cholestatic liver disease characterized by inflammation of the intrahepatic and extrahepatic bile ducts, sclerosis and destruction of the biliary tract.1–4 This leads to chronic cholestasis, biliary fibrosis and (decompensated) cirrhosis, which may eventually culminate into liver failure requiring liver transplantation; the only potential curative treatment for PSC.2,3 Following a PSC diagnosis a median transplant-free survival of 13 years has been reported in studies from tertiary referral centres, although this may be longer in a population-based setting.5 
In Iceland a nationwide program has been launched offering direct-acting antiviral (DAA) treatment for everyone living with hepatitis C virus (HCV). We estimate (i) the time and treatment scale-up required to achieve the World Health Organization’s HCV elimination target of an 80% reduction in incidence; and (ii) the ongoing frequency of HCV testing and harm reduction coverage among people who inject drugs (PWID) required to minimize the likelihood of future HCV outbreaks occurring. We used a dynamic compartmental model of HCV transmission, liver disease progression and the HCV cascade of care, calibrated to reproduce the epidemic of HCV in Iceland. The model was stratified according to injecting drug use status, age and stage of engagement. Four scenarios were considered for the projections. The model estimated that an 80% reduction in domestic HCV incidence was achievable by 2030, 2025 or 2020 if a minimum of 55/1,000, 75/1,000 and 188/1,000 PWID were treated per year, respectively (a total of 22, 30 and 75 of the estimated 400 PWID in Iceland per year, respectively). Regardless of time frame, this required an increased number of PWID to be diagnosed to generate enough treatment demand, or a 20% scale-up of harm reduction services to complement treatment-as-prevention incidence reductions. When DAA scale-up was combined with annual antibody testing of PWID, the incidence reduction target was reached by 2024. Treatment scale-up with no other changes to current testing and harm reduction services reduced the basic reproduction number of HCV from 1.08 to 0.59, indicating that future outbreaks would be unlikely. HCV elimination in Iceland is achievable by 2020 with some additional screening of PWID. Maintaining current monitoring and harm reduction services while providing ongoing access to DAA therapy for people diagnosed with HCV would ensure that outbreaks are unlikely to occur once elimination targets have been reached. The recent availability of highly tolerable direct-acting antiviral (DAA) treatments for hepatitis C virus (HCV) has led to the development of World Health Organization (WHO) HCV elimination targets,1 which propose an 80% reduction in HCV incidence and a 65% reduction in HCV-related mortality by 2030. Many countries are currently either formulating HCV strategies or determining what resources and policies will be required to reach elimination targets. 
Primary sclerosing cholangitis (PSC) is an inflammatory, cholestatic and progressively fibrotic liver disease devoid of effective medical intervention. NGM282, an engineered, non-tumorigenic FGF19 analogue, potently regulates CYP7A1-mediated bile acid homeostasis. We assessed the activity and safety of NGM282 in patients with PSC. In this double-blind, placebo-controlled phase II trial, 62 patients who had PSC confirmed by cholangiography or biopsy and an elevated alkaline phosphatase (ALP) >1.5 × the upper limit of normal were randomly assigned 1:1:1 to receive NGM282 1 mg, 3 mg or placebo once daily for 12 weeks. The primary outcome was the change in ALP from baseline to week 12. Secondary and exploratory outcomes included changes in serum biomarkers of bile acid metabolism and fibrosis. Efficacy analysis was by intention-to-treat. At 12 weeks, there were no significant differences in the mean change from baseline in ALP between the NGM282 and placebo groups, and therefore, the primary endpoint was not met. However, NGM282 significantly reduced levels of 7alpha-hydroxy-4-cholesten-3-one (a marker of hepatic CYP7A1 activity, LS mean differences −6.2 ng/ml (95% CI −10.7 to −1.7; p = 0.008) and −9.4 ng/ml (−14.0 to −4.9; p <0.001) in the NGM282 1 mg and 3 mg groups, respectively, compared with placebo) and bile acids. Importantly, fibrosis biomarkers that predict transplant-free survival, including Enhanced Liver Fibrosis score and Pro-C3, were significantly improved following NGM282 treatment. Most adverse events were mild to moderate in severity, with gastrointestinal symptoms more frequent in the NGM282 treatment groups. In patients with PSC, NGM282 potently inhibited bile acid synthesis and decreased fibrosis markers, without significantly affecting ALP levels. Primary sclerosing cholangitis (PSC) is a chronic liver disease characterized by strictures of the biliary tree for which there is presently a dearth of effective medical treatment.1 Of the histopathological hallmarks of PSC, periductal inflammation and “onion skin”-like fibrosis, referring to concentric layers of collagen fibers circumferential to the cholangiocyte lining of the bile ducts, characterize a progressive fibrosing cholangiopathy.2 Patients frequently present with concurrent inflammatory bowel disease (IBD), and are at increased risk of developing hepatobiliary and colon cancers. More than 50% of patients need liver transplantation within 10–15 years of symptom development.1 Biochemically, PSC is characterized by elevated serum liver tests, and alkaline phosphatase (ALP) levels associate with future risk of adverse events. 
The accurate diagnosis of occult hepatitis B virus (HBV) infection (OBI) requires the demonstration of HBV DNA in liver biopsies of hepatitis B surface antigen-negative individuals. However, in clinical practice a latent OBI is deduced by the finding of the antibody to the hepatitis B core antigen (anti-HBc). We investigated the true prevalence of OBI and the molecular features of intrahepatic HBV in anti-HBc-positive individuals. The livers of 100 transplant donors (median age 68.2 years; 64 males, 36 females) positive for anti-HBc at standard serologic testing, were examined for total HBV DNA by nested-PCR and for the HBV covalently closed circular DNA (HBV cccDNA) with an in-house droplet digital PCR assay (ddPCR) (Linearity: R2 = 0.9998; lower limit of quantitation and detection of 2.4 and 0.8 copies/105 cells, respectively). A total of 52% (52/100) of the individuals studied were found to have OBI. cccDNA was found in 52% (27/52) of the OBI-positive, with a median 13 copies/105 cells (95% CI 5–25). Using an assay specific for anti-HBc of IgG class, the median antibody level was significantly higher in HBV cccDNA-positive than negative donors (17.0 [7.0–39.2] vs. 5.7 [3.6–9.7] cut-off index [COI], respectively, p = 0.007). By multivariate analysis, an anti-HBc IgG value above 4.4 COI was associated with the finding of intrahepatic HBV cccDNA (odds ratio 8.516, p = 0.009); a lower value ruled out its presence with a negative predictive value of 94.6%. With a new in-house ddPCR-based method, intrahepatic HBV cccDNA was detectable in quantifiable levels in about half of the OBI cases examined. The titer of anti-HBc IgG may be a useful surrogate to predict the risk of OBI reactivation in immunosuppressed patients. Occult hepatitis B virus (HBV) infection (OBI) refers to the presence of intrahepatic HBV DNA in the absence of detectable hepatitis B surface antigen (HBsAg).1 OBI is secondary to overt HBV infections; it guarantees the persistence of the virus in a cryptic form protected from the immune response of the host. The virologic key is the covalently closed circular DNA (cccDNA), an HBV DNA form generated as a plasmid-like episome from the protein-linked relaxed circular DNA genome; it resides in the nucleus of infected cells and gives rise to viral sequences, which act as a transcription template for all viral RNAs.2 
Non-alcoholic fatty liver disease (NAFLD) and non-alcoholic steatohepatitis (NASH) are increasingly a cause of cirrhosis and hepatocellular carcinoma globally. This burden is expected to increase as epidemics of obesity, diabetes and metabolic syndrome continue to grow. The goal of this analysis was to use a Markov model to forecast NAFLD disease burden using currently available data. A model was used to estimate NAFLD and NASH disease progression in eight countries based on data for adult prevalence of obesity and type 2 diabetes mellitus (DM). Published estimates and expert consensus were used to build and validate the model projections. If obesity and DM level off in the future, we project a modest growth in total NAFLD cases (0–30%), between 2016–2030, with the highest growth in China as a result of urbanization and the lowest growth in Japan as a result of a shrinking population. However, at the same time, NASH prevalence will increase 15–56%, while liver mortality and advanced liver disease will more than double as a result of an aging/increasing population. NAFLD and NASH represent a large and growing public health problem and efforts to understand this epidemic and to mitigate the disease burden are needed. If obesity and DM continue to increase at current and historical rates, both NAFLD and NASH prevalence are expected to increase. Since both are reversible, public health campaigns to increase awareness and diagnosis, and to promote diet and exercise can help manage the growth in future disease burden. Non-alcoholic fatty liver disease (NAFLD) is a leading cause of liver disease globally.1–3 This condition is characterized by excess liver fat in the absence of other causes such as alcohol consumption.4,5 Obesity, type 2 diabetes mellitus (DM) and metabolic syndrome are consistently identified as the most important risk factors for NAFLD.4,6 
The burden of hepatitis E virus (HEV) infection among patients with haematological malignancy has only been scarcely reported. Therefore, we aimed to describe this burden in patients with haematological malignancies, including those receiving allogeneic haematopoietic stem cell transplantation. We conducted a retrospective, multicentre cohort study across 11 European centres and collected clinical characteristics of 50 patients with haematological malignancy and RNA-positive, clinically overt hepatitis E between April 2014 and March 2017. The primary endpoint was HEV-associated mortality; the secondary endpoint was HEV-associated liver-related morbidity. The most frequent underlying haematological malignancies were aggressive non-Hodgkin lymphoma (NHL) (34%), indolent NHL (iNHL) (24%), and acute leukaemia (36%). Twenty-one (42%) patients had received allogeneic haematopoietic stem cell transplantation (alloHSCT). Death with ongoing hepatitis E occurred in 8 (16%) patients, including 1 patient with iNHL and 1 patient >100 days after alloHSCT in complete remission, and was associated with male sex (p = 0.040), cirrhosis (p = 0.006) and alloHSCT (p = 0.056). Blood-borne transmission of hepatitis E was demonstrated in 5 (10%) patients, and associated with liver-related mortality in 2 patients. Hepatitis E progressed to chronic hepatitis in 17 (34%) patients overall, and in 10 (47.6%) and 6 (50%) alloHSCT and iNHL patients, respectively. Hepatitis E was associated with acute or acute-on-chronic liver failure in 4 (8%) patients with 75% mortality. Ribavirin was administered to 24 (48%) patients, with an HEV clearance rate of 79.2%. Ribavirin treatment was associated with lower mortality (p = 0.037) and by trend with lower rates of chronicity (p = 0.407) when initiated <24 and <12 weeks after diagnosis of hepatitis E, respectively. Immunosuppressive treatment reductions were associated with mortality in 2 patients (28.6%). Hepatitis E is associated with mortality and liver-related morbidity in patients with haematological malignancy. Blood-borne transmission contributes to the burden. Ribavirin should be initiated early, whereas reduction of immunosuppressive treatment requires caution. The hepatitis E virus (HEV), the causative agent of hepatitis E, is a member of the Hepeviridae family that includes enterically-transmitted, small, non-enveloped positive-sense RNA viruses that can infect mammals (Orthohepevirus A, C and D), birds (Orthohepevirus B) and trout (Piscihepevirus). There are 4 major HEV genotypes (HEV-1 to HEV-4) that can infect humans.1 HEV is predominately transmitted by contaminated water in low-income countries (mostly HEV-1 and HEV-2) and by contaminated meat and offal that originate from HEV-infected animals (HEV-3 and HEV-4) in high-income countries.2,3 Blood-borne transmission of HEV via red blood cells, platelets and plasma has also been reported worldwide.4–7 HEV is pandemic globally, including in industrialised countries.5 
Shp2 is an SH2-tyrosine phosphatase acting downstream of receptor tyrosine kinases (RTKs). Most recent data demonstrated a liver tumor-suppressing role for Shp2, as ablating Shp2 in hepatocytes aggravated hepatocellular carcinoma (HCC) induced by chemical carcinogens or Pten loss. We further investigated the effect of Shp2 deficiency on liver tumorigenesis driven by classical oncoproteins c-Met (receptor for HGF), β-catenin and PIK3CA. We performed hydrodynamic tail vein injection of two pairs of plasmids expressing c-Met and ΔN90-β-catenin (MET/CAT), or c-Met and PIK3CAH1047R (MET/PIK), into WT and Shp2hep−/− mice. We compared liver tumor loads and investigated the pathogenesis and molecular mechanisms involved using multidisciplinary approaches. Despite the induction of oxidative and metabolic stresses, Shp2 deletion in hepatocytes suppressed hepatocarcinogenesis driven by overexpression of oncoproteins MET/CAT or MET/PIK. Shp2 loss inhibited proliferative signaling from c-Met, Wnt/β-catenin, Ras/Erk and PI3K/Akt pathways, but triggered cell senescence following exogenous expression of the oncogenes. Shp2, acting downstream of RTKs, is positively required for hepatocyte-intrinsic tumorigenic signaling from these oncoproteins, even if Shp2 deficiency induces a tumor-promoting hepatic microenvironment. These data suggest a new and more effective therapeutic strategy for HCCs driven by oncogenic RTKs and other upstream molecules, by inhibiting Shp2 and also suppressing any tumor-enhancing stromal factors produced because of Shp2 inhibition. Primary liver cancer, mainly hepatocellular carcinoma (HCC), is a highly malignant disease. While the overall cancer mortality and incidences are decreasing, liver cancer incidences are increasing rapidly in the United States.1 The lack of effective therapeutic drugs is evidently caused by poor understanding of the complicated mechanisms of hepatocarcinogenesis. 
Around 5% of patients with chronic hepatitis C virus (HCV) infection treated with direct-acting antiviral (DAA) agents do not achieve sustained virological response (SVR). The currently approved retreatment regimen for prior DAA failure is a combination of sofosbuvir, velpatasvir, and voxilaprevir (SOF/VEL/VOX), although there is little data on its use in clinical practice. The aim of this study was to analyse the effectiveness and safety of SOF/VEL/VOX in the real-world setting. This was a prospective multicentre study assessing the efficacy of retreatment with SOF/VEL/VOX in patients who had experienced a prior DAA treatment failure. The primary endpoint was SVR 12 weeks after the completion of treatment (SVR12). Data on safety and tolerability were also recorded. A total of 137 patients were included: 75% men, 35% with liver cirrhosis. Most were infected with HCV genotype (GT) 1 or 3. The most common prior DAA combinations were sofosbuvir plus an NS5A inhibitor or ombitasvir/paritaprevir/r+dasabuvir. A total of 136 (99%) patients achieved undetectable HCV RNA at the end of treatment. Overall SVR12 was 95% in the 135 patients reaching this point. SVR12 was lower in patients with cirrhosis (89%, p = 0.05) and those with GT3 infection (80%, p <0.001). Patients with GT3 infection and cirrhosis had the lowest SVR12 rate (69%). Of the patients who did not achieve SVR12, 1 was reinfected and 7 experienced treatment failure (6 GT3, 1 GT1a). The presence of resistance-associated substitutions did not impact SVR12. Adverse effects were mild and non-specific. Real-world data show that SOF/VEL/VOX is an effective, safe rescue therapy for patients with prior DAA treatment failure despite the presence of resistance-associated substitutions. However, patients with liver cirrhosis infected by GT3 remain the most-difficult-to-treat group. Current treatments with direct-acting antivirals (DAAs) for hepatitis C virus (HCV) infection lead to elimination of the virus in more than 95% of patients, regardless of the HCV genotype or presence of advanced liver fibrosis.1 The American Association for the Study of Liver Diseases (AASLD) and the European Association for the Study of the Liver (EASL) guidelines both recommend combinations including an NS5A inhibitor with either a NS3/4 protease inhibitor, such as grazoprevir/elbasvir or glecaprevir/pibrentasvir, or a nucleotide analogue plus an NS5A inhibitor, such as sofosbuvir/velpatasvir, for durations ranging from 8 to 12 weeks.2–4 Sofosbuvir/velpatasvir and glecaprevir/pibrentasvir combinations are pangenotypic and therefore, they are the preferred regimens to simplify HCV therapy.5–7 Despite the high efficacy of these new combinations, the options for patients who do not achieve a sustained virological response (SVR) are limited.8 The latest approved retreatment regimen is combined therapy with sofosbuvir plus the NS55 inhibitor, velpatasvir, and the NS3/4 protease inhibitor, voxilaprevir (SOF/VEL/VOX),9 which is recommended in the AASLD and EASL guidelines for retreating patients previously failing DAA regimens.2–4 
The lipid-binding protein, SEC14L2, is crucial for the efficient viral replication of clinical hepatitis C virus (HCV) isolates in cell culture. Given the role of SEC14L2 in HCV replication, we aimed to study a large number of HCV positive sera carrying genotypes 1–4, to identify viral factors associated with efficient replication in culture. Additionally, we investigated whether 13 single nucleotide polymorphisms (SNPs) of SEC14L2 have an impact on RNA replication of naturally occurring HCV isolates. We generated Huh-7.5 cell lines overexpressing SEC14L2 or 13 coding SNPs and tested 73 different HCV positive sera for in vitro replication. Furthermore, we genotyped a cohort of 262 patients with chronic HCV for the common SNP (rs757660) and investigated its effect on the clinical phenotype. HCV isolates from genotype 1, 2, 3 and 4 replicate in Huh-7.5 cells overexpressing SEC14L2. Interestingly, only subgenomic replicons from genotypes 1 and 3 showed enhanced replication whereas genotypes 2 and 4 remained unaffected. Furthermore, replication was independent of viral load. Importantly, all tested SNPs supported HCV RNA replication in vitro, while 1 SNP was associated with decreased SEC1L2 expression and viral RNA. All SNPs exhibited comparable cellular cholesterol and vitamin E abundance in naïve Huh-7.5 cells. This large screen of natural HCV isolates of 4 genotypes underscores the relevance of SEC14L2 as an in vitro HCV host factor. Additionally, SEC14L2 variants appear to recapitulate the wild-type enhancement of HCV replication. Variant rs191341134 showed a decreased effect due to lowered stability, whereas variant rs757660, a high prevalence mutant, showed a similar phenotype to the wild-type. Hepatitis C virus (HCV) is a positive stranded RNA Hepacivirus in the Flaviviridae family. Today, HCV remains an important element in the aetiology of chronic liver disease and according to the World Health Organization there are estimated to be 71 million chronically infected patients worldwide.1 Since the discovery of the virus in 1989, the development of tools such as a subgenomic replicon system, cell culture adapted variants and highly permissive hepatoma cell lines have enabled researchers to study different aspects of the HCV life cycle. This paved the way for the generation of effective antiviral therapies against the virus.2,3 Nevertheless, replication of non-cell culture adapted variants remained a difficult achievement and later facets of the viral life cycle are yet to be fully described. This status quo was challenged in 2015 when pan-genotype (GT) replication of patient-derived isolates was shown by Saeed et al. They were able to show that the overexpression of the lipid-binding protein SEC14L2 in a Huh-7.5 hepatoma cell line allowed efficient replication of non-cell culture adapted HCV isolates, through a vitamin E mediated mechanism of lipid peroxidation resistance.4 Curiously vitamin E supplementation was not sufficient to allow replicon colony formation in the absence of SEC14L2 which would suggest an alternative mechanism may be at play. The context of lipid peroxidation as a regulatory mechanism of HCV had already been suggested by Yamane et al. Yamane and co-authors showed that sphingosine kinase-2 partly regulated lipid peroxidation and the data implied a significant role of this regulation upon HCV replication. The EC50 of tested HCV antivirals was shown to increase in the presence of vitamin E or the sphingosine kinase inhibitor.5 Further data on the subject of non-cell culture adapted virus replication were made available in 2016 when a mechanism for viral dependence on host phosphatidylinositol 4-kinase IIIα (PI4KA) was reported to be a determinant factor in hepatoma cell lines. Harak et al. were able to show that adaptive mutations with loss of function in NS5A-NS5B were necessary for efficient replication in hepatoma cell lines. The difference in the expression of PI4KA, and consequently, the abundance of PiP4, were pointed out as being detrimental for viral replication as they were present in excess when comparing model hepatoma cell lines with primary human hepatocytes.6 Together these new data provided an important understanding of the role of host factors and lipid peroxidation in the life cycle of HCV, chief among them, SEC14L2. 
The introduction of direct-acting antivirals for hepatitis C virus (HCV) in Egypt led to massive treatment uptake, with Egypt’s national HCV treatment program becoming the largest in the world. The aim of this paper is to present the Egyptian experience in planning and prioritizing mass treatment for patients with HCV, highlighting the difficulties and limitations of the program, as a guide for other countries of similarly limited resources. Baseline data of 337,042 patients, treated between October 2014 to March 2016 in specialized viral hepatitis treatment centers, were grouped into three equal time intervals of six months each. Patients were treated with different combinations of direct-acting antivirals, with or without ribavirin and pegylated interferon. Baseline data, percentage of patients with known outcome, and sustained virological response at week 12 (SVR12) were analyzed for the three cohorts. The outcomes of 94,258 patients treated in the subsequent two months are also included. For cohort-1, treatment was prioritized for patients with advanced fibrosis (F3-F4 fibrosis, liver stiffness ≥9.5 kPa, or Fibrosis-4 ≥3.25). Starting cohort-2, all stages of fibrosis were included (F0-F4). The prioritization strategy in the initial phase caused delays in enrollment and massive backlogs. Cohort-1 patients were significantly older, and more had advanced fibrosis compared to subsequent cohorts. The percentage of patients with known SVR12 results were low initially, and increased with each cohort, as several methods to capture patient results were adopted. Sofosbuvir-ribavirin therapy for 24 weeks had the lowest SVR12 rate (82.7%); while other therapies were associated with SVR12 rates between 94% and 98%. Prioritization based on fibrosis stage was not effective and enrollment increased greatly only after including all stages of fibrosis. The availability of generic drugs reduced costs, and helped massively increase uptake of the program. Post-treatment follow-up was initially very low, and although this has increased, further improvement is still needed. Egypt has the highest prevalence of hepatitis C virus (HCV) infection,1 having an antibody prevalence in the population aged 15–59 years of about 10% and viremic prevalence of 7%,2 with almost all infections due to genotype 4 (GT4).3 This burden necessitated strong commitment on the part of policy-makers to adopt efficient curative interventions. This has translated into the largest national treatment program for HCV in the world.4 Lessons from the successes and difficulties of the program can benefit all resource-limited countries with similar problems caused by hepatitis C. 
Liver fibrosis is characterized by the accumulation of extracellular matrix produced by hepatic myofibroblasts (hMF), the activation of which is critical to the fibrogenic process. Extracellular ATP, released by dying or stressed cells, and its purinergic receptors, constitute a powerful signaling network after injury. Although the purinergic receptor P2X4 (P2RX4) is highly expressed in the liver, its functions in hMF had never been investigated during liver fibrogenesis. In vivo, bile duct ligation was performed and methionine- and choline-deficient diet administered in wild-type and P2x4 knock-out (P2x4-KO) mice. In vitro, hMF were isolated from mouse (wild-type and P2x4-KO) and human liver. P2X4 pharmacological inhibition (in vitro and in vivo) and P2X4 siRNAs (in vitro) were used. Histological, biochemical and cell culture analysis allowed us to study P2X4 expression and its involvement in the regulation of fibrogenic and fibrolytic factors, as well as of hMF activation markers and properties. P2X4 genetic invalidation or pharmacological inhibition protected mice from liver fibrosis and hMF accumulation after bile duct ligation or methionine- and choline-deficient diet. Human and mouse hMFs expressed P2X4, mainly in lysosomes. Invalidation of P2X4 in human and mouse hMFs blunted their activation marker expression and their fibrogenic properties. Finally, we showed that P2X4 regulates calcium entry and lysosomal exocytosis in hMF, impacting on ATP release, profibrogenic secretory profile, and transcription factor activation. P2X4 expression and activation is critical for hMF to sustain their activated and fibrogenic phenotype. Therefore, the inactivation of P2X4 may be of therapeutic interest during liver fibrotic diseases. Beside huge regenerative properties, the liver classically undergoes fibrogenesis in association with liver repair during chronic injury, impairing hepatic functions with high morbidity and mortality.1 As well as in other organs, in the liver, the main cellular contributor to excessive extracellular matrix (ECM) deposition is the myofibroblast, derived from hepatic stellate cells (HSCs) and/or portal fibroblasts2 after acute or during chronic liver injury. Hepatic myofibroblasts (hMF) are characterized by αSMA (ACTA2) expression, contractile properties, and by overexpressed ECM components and regulators. The acquisition of an ECM remodeling phenotype in hMF during liver injury is strongly dependent on a complex network of cellular interactions within the liver microenvironment, involving a large number of cytokines and growth factors.1 Nevertheless, the so-called “danger activated molecular patterns”, released by dying or stressed cells during injury, including extracellular ATP, also constitute a powerful signaling network which impact on liver fibrogenetic processes.3,4 In general, signaling via extracellular ATP and its receptors, i.e. P2Y (G protein-coupled receptors) and P2X (ligand-gated ion channels), is reported to have regulatory impact on fundamental processes like secretion, contraction, survival and proliferation in a wide spectrum of cells and tissues, including the liver.5,6 ATP release from cells occurs under different stress conditions, particularly under osmotic or mechanical challenge.7 In the liver, each cell type expresses its own repertoire of purinoceptor subtypes and ectoATPases,8 P2X4 (P2RX4) and P2X7 (P2RX7) often being predominant among P2 receptor subtypes.9,10 We previously observed robust ATP release from the liver immediately after partial hepatectomy (PH) in rats and in human living transplant donors, resulting from intrahepatic mechanical stress and contributing to hepatocyte cell cycle entry.11 In a recent study in mice, we proposed that during liver regeneration after PH, ATP signaling through P2X4 contributed to the control of biliary homeostasis, with a resulting impact on hepatocyte protection and proliferation, through basic mechanisms involving exocytosis of lysosomes, an organelle in which P2X4 is strongly expressed.10 Beside evidence for a purinergic-dependent regulation of liver repair after acute injury,10–13 only few data are available as far as chronic liver injury and fibrogenesis are concerned. Previous studies in the context of liver fibrosis reported that adenosine (ATP's breakdown product), and its A1 and A2 receptors provide profibrogenic signals,14 at least in part through modulation of macrophage or HSC activation,15 although some contradictory results have been reported.16 Regarding the P2 side of purinergic input signaling during liver fibrosis, while some P2Y isoforms have been reported to be expressed and to play some role in HSC-myofibroblast differentiation,17,18 most of the recent studies were focused on P2X7 signaling. In mice, pharmacological inhibition or genetic deletion of P2X7 resulted in less carbon tetrachloride (CCl4) or bile duct ligation (BDL)-induced liver fibrosis,19,20 through incompletely defined cell-specific mechanisms, including impact on splanchnic hemodynamics20 and myofibroblast activation and/or inflammasome induction.21–23 However, a clear picture of the purinergic-related control of liver fibrogenesis is still lacking, in particular according to cell types, ligands and receptors involved, and finally depending on the experimental model used. Indeed, biliary and non-biliary type fibrosis appeared to exhibit differential sensitivity to purinergic signals and the underlying mechanisms remain unclear.16,18 Importantly, other P2 receptors, including the highly expressed P2X4, were not investigated in the context of liver fibrogenesis. As P2X4 was reported to regulate secretion of inflammatory mediators,24 cytoskeleton organisation25 and collagen deposition26 in extrahepatic sites, we aimed at studying the impact of P2X4 on hMF phenotype and liver fibrogenesis. 
Non-alcoholic fatty liver disease (NAFLD) is a multifactorial condition and the most common liver disease worldwide, affecting more than one-third of the population. So far there have been no reports on mendelian inheritance in families with NAFLD. We performed whole-exome or targeted next-generation sequencing on patients with autosomal dominant NAFLD. We report a heritable form of NAFLD and/or dyslipidemia due to monoallelic ABHD5 mutations, with complete clinical expression after the fourth decade of life, in 7 unrelated multiplex families encompassing 39 affected individuals. The prevalence of ABHD5-associated NAFLD was estimated to be 1 in 1,137 individuals in a normal population. We associate a Mendelian form of NAFLD and/or dyslipidemia with monoallelic ABHD5 mutations. Non-alcoholic fatty liver disease (NAFLD) is an increasingly common disorder that is strongly associated with the metabolic syndrome, and which may progress from simple steatosis to non-alcoholic steatohepatitis (NASH), cirrhosis, hepatic failure, and hepatocellular carcinoma. NAFLD is a major health issue worldwide and is associated with significant morbidity and mortality. The prevalence of this condition reaches 12–18% in the European countries and 27–38% in the US.1,2 NAFLD is a multifactorial disease and up to 50% of its relative risk has been attributed to genetic susceptibility, with evidence coming from familial aggregation, twin studies, and differential ethnic predisposition.3 Genome-wide association studies identified a number of variants that are associated with NAFLD, but in most cases the odds ratios were relatively small. However, in large-scale population studies, variants in the PNPLA3, TM6SF2, LYPLAL1, and GCKR genes were associated with elevated liver fat levels.4 The existence of inherited forms of NAFLD has been suspected, but neither a specific causal gene or a susceptibility locus has been identified.5 We hypothesized that genetic alterations might account for some cases of NAFLD, especially those that exhibit Mendelian inheritance. SECTION Patients and methods 
MicroRNAs (MiRNAs) derived from parasites, and even from plants, have been detected in body fluids and are known to modulate host genes. In this study, we aimed to investigate if the schistosome miRNAs are involved in the occurrence and progression of hepatic fibrosis during Schistosoma japonicum (S. japonicum) infection. The presence of miRNAs from S. japonicum (sja-miRNAs) in hepatic stellate cells (HSCs) was detected by RNA sequencing. sja-miRNAs were screened by transfecting HSCs with sja-miRNA mimics. The role of sja-miR-2162 in hepatic fibrosis was evaluated by either elevating its expression in naïve mice or by inhibiting its activity in infected mice, through administration of recombinant adeno-associated virus serotype 8 vectors expressing sja-miR-2162 or miRNA sponges, respectively. We identified a miRNA of S. japonicum, sja-miR-2162, that was consistently present in the HSCs of infected mice. Transfection of sja-miR-2162 mimics led to activation of HSC cells in vitro, characterized by elevation of collagens and α-SMA. The rAAV8-mediated delivery of sja-miR-2162 to naïve mice induced hepatic fibrosis, while sustained inhibition of sja-miR-2162 in infected mice attenuated hepatic fibrosis. The transforming growth factor beta receptor III (TGFBR3), a negative regulator of TGF-β signaling, was a direct target of sja-miR-2162 in HSCs. This study demonstrated that pathogen-derived miRNAs directly promote hepatic fibrogenesis in a cross-species manner, and their efficient and sustained inhibition might present a promising therapeutic intervention for infectious diseases. Schistosomiasis is one of the most prevalent, but unfortunately neglected, tropical infectious diseases, affecting more than 240 million people across 78 countries.1 The 2 most important species that cause liver disease in humans are Schistosoma mansoni and Schistosoma japonicum (S. japonicum). Female schistosoma living in the mesenteric veins of hosts lay numerous eggs, many of which are trapped in the liver via the portal venous system. Highly immunogenic substances released by parasite eggs induce a granulomatous and fibrotic response, which is characterized by T helper-2 cytokines (interleukin [IL]-4 and IL-13), eosinophils, and alternatively activated macrophages.2 Hepatic fibrosis is the primary cause of morbidity and mortality associated with schistosomiasis. It is a very complicated process that involves many host-derived mediators, such as cytokines, chemokines, growth factors, and microRNAs.3 These host-derived mediators collectively contribute to promote the trans-differentiation of hepatic stellate cells (HSCs) into activated myofibroblasts, which are responsible for production of collagen and fibrogenesis within the granuloma site.4 It would be interesting to explore parasite-derived factors that are associated with the pathogenesis and progression of this disease, in order to identify potential therapeutic targets. 
To compare the overall survival (OS) and disease progression free survival (PFS) in patients with advanced hepatocellular carcinoma (Ad-HCC) who are undergoing hepatic arterial infusion (HAI) of oxaliplatin, fluorouracil/leucovorin (FOLFOX) treatment vs. sorafenib. This retrospective study was approved by the ethical review committee, and informed consent was obtained from all patients before treatment. HAI of FOLFOX (HAIF) was recommended as an alternative treatment option for patients who refused sorafenib. Of the 412 patients with Ad-HCC (376 men and 36 women) between Jan 2012 to Dec 2015, 232 patients were treated with sorafenib; 180 patients were given HAIF therapy. The median age was 51 years (range, 16–82 years). Propensity-score matched estimates were used to reduce bias when evaluating survival. Survival curves were calculated by performing the Kaplan-Meier method and compared by using the log-rank test and Cox regression models. The median PFS and OS in the HAIF group were significantly longer than those in the sorafenib group (PFS 7.1 vs. 3.3 months [RECIST]/7.4 vs. 3.6 months [mRECIST], respectively; OS 14.5 vs. 7.0 months; p <0.001 for each). In the propensity-score matched cohorts (147 pairs), both PFS and OS in the HAIF group were longer than those in the sorafenib group (p <0.001). At multivariate analysis, HAIF treatment was an independent factor for PFS (hazard ratio [HR] 0.389 [RECIST]/0.402 [mRECIST]; p <0.001 for each) and OS (HR 0.129; p <0.001). HAIF therapy may improve survival compared to sorafenib in patients with Ad-HCC. A prospective randomized trial is ongoing to confirm this finding. Hepatocellular carcinoma (HCC) is the fourth leading cause of cancer worldwide.1 A total of 25%–70% of HCC is diagnosed at an advanced stage, with a median overall survival (OS) of only 4.2–7.9 months, because of limited treatment options.2,3 To date, sorafenib is still the only treatment shown to extend OS for advanced HCC (Ad-HCC).4 However limitations including, low response rates,2 modest survival advantages,3 high-level heterogeneity of individual response5 and insensitivity for populations with hepatitis B virus (HBV) infection,6 prohibit sorafenib’s widespread use in Ad-HCC. Thus, alternative therapies for Ad-HCC are urgently required.1,7,8 
Interleukin (IL)-1-type cytokines including IL-1α, IL-1β and interleukin-1 receptor antagonist (IL-1Ra) are among the most potent molecules of the innate immune system and exert biological activities through the ubiquitously expressed interleukin-1 receptor type 1 (IL-1R1). The role of IL-1R1 in hepatocytes during acute liver failure (ALF) remains undetermined. The role of IL-1R1 during ALF was investigated using a novel transgenic mouse model exhibiting deletion of all signaling-capable IL-1R isoforms in hepatocytes (Il1r1Hep−/−). ALF induced by D-galactosamine (D-GalN) and lipopolysaccharide (LPS) was significantly attenuated in Il1r1Hep−/− mice leading to reduced mortality. Conditional deletion of Il1r1 decreased activation of injurious c-Jun N-terminal kinases (JNK)/c-Jun signaling, activated nuclear factor-kappa B (NF-κB) p65, inhibited extracellular signal-regulated kinase (ERK) and prevented caspase 3-mediated apoptosis. Moreover, Il1r1Hep−/− mice exhibited reduced local and systemic inflammatory cytokine and chemokine levels, especially TNF-α, IL-1α/β, IL-6, CC-chemokine ligand 2 (CCL2), C-X-C motif ligand 1 (CXCL-1) and CXCL-2, and a reduced neutrophil recruitment into the hepatic tissue in response to injury. NLRP3 inflammasome expression and caspase 1 activation were suppressed in the absence of the hepatocellular IL-1R1. Inhibition of IL-1R1 using IL-1ra (anakinra) attenuated the severity of liver injury, while IL-1α administration exaggerated it. These effects were lost ex vivo and at later time points, supporting a role of IL-1R1 in inflammatory signal amplification during acute liver injury. IL-1R1 in hepatocytes plays a pivotal role in an IL-1-driven auto-amplification of cell death and inflammation in the onset of ALF. Acute liver failure (ALF) and acute-on-chronic liver failure (ACLF) both lead to a high mortality rate, with limited treatment options available. An altered inflammatory response has been implicated in their pathophysiology.1,2 Despite obvious differences in their etiology, both conditions exhibit activation of immune mechanisms that augment inflammation after the initial insult and drive a lethal loss of hepatic function from increased cell death. In order to improve the poor prognosis of patients with ALF and ACLF, mechanistic studies that address the underlying pathomechanisms are urgently required with the aim of identifying potential, selective immune-modulatory therapies. 
Hepatitis B virus (HBV) and D virus (HDV) co-infections cause the most severe form of viral hepatitis. HDV induces an innate immune response, but it is unknown how the host cell senses HDV and if this defense affects HDV replication. We aim to characterize interferon (IFN) activation by HDV, identify the responsible sensor and evaluate the effect of IFN on HDV replication. HDV and HBV susceptible hepatoma cell lines and primary human hepatocytes (PHH) were used for infection studies. Viral markers and cellular gene expression were analyzed at different time points after infection. Pattern recognition receptors (PRRs) required for HDV-mediated IFN activation and the impact on HDV replication were studied using stable knock-down or overexpression of the PRRs. Microarray analysis revealed that HDV but not HBV infection activated a broad range of interferon stimulated genes (ISGs) in HepG2NTCP cells. HDV strongly activated IFN-β and IFN-λ in cell lines and PHH. HDV induced IFN levels remained unaltered upon RIG-I (DDX58) or TLR3 knock-down, but were almost completely abolished upon MDA5 (IFIH1) depletion. Conversely, overexpression of MDA5 but not RIG-I and TLR3 in HuH7.5NTCP cells partially restored ISG induction. During long-term infection, IFN levels gradually diminished in both HepG2NTCP and HepaRGNTCP cell lines. MDA5 depletion had little effect on HDV replication despite dampening HDV-induced IFN response. Moreover, treatment with type I or type III IFNs did not abolish HDV replication. Active replication of HDV induces an IFN-β/λ response, which is predominantly mediated by MDA5. This IFN response and exogenous IFN treatment have only a moderate effect on HDV replication in vitro indicating the adaption of HDV replication to an IFN-activated state. Among the 240 million hepatitis B virus (HBV)-infected people worldwide,1 15–25 million are coinfected with hepatitis D virus (HDV) (WHO fact sheet, July 2016), a satellite virus which requires HBV envelope proteins for particle assembly and spread. HBV/HDV coinfection leads to the most severe form of viral hepatitis with an accelerated progression to liver fibrosis, cirrhosis and hepatocellular carcinoma.2 Therapeutic options for chronically HBV/HDV coinfected patients are still limited to IFN-α therapy for eligible patients, however, this therapy is not curative in the vast majority of patients.3,4 HDV consists of a single-stranded circular 1.7 kb RNA genome of negative polarity and represents the smallest mammalian virus genome identified to date.5 HDV RNA replication takes place in the nucleus of hepatocytes through a double rolling circle mechanism, producing antigenomic RNA, genomic RNA and mRNAs encoding the small and large hepatitis delta antigens (S- and L-HDAg). The circular genomic HDV RNA is highly self-complementary (∼74% base pairing) and therefore forms rod-shaped viroid-like tertiary structures.6 During assembly, the newly synthesized genomic RNA associates with HDAg molecules to form a ribonucleoprotein (RNP) complex.7 These RNPs become enveloped by budding into the ER lumen at HBV envelope protein (L-/M- and S-HBsAg) assembly sites, which are provided by either covalently closed circular DNA of HBV coinfected hepatocytes or possibly integrated HBV sequences.8 
Radical resection is the best treatment for patients with advanced hepatic alveolar echinococcosis (AE). Liver transplantation is considered for selected advanced cases; however, a shortage of organ donors and the risk of postoperative recurrence are major challenges. The aim of this study was to assess the clinical outcomes of ex vivo liver resection and autotransplantation for end-stage AE. In this prospective study, 69 consecutive patients with end-stage hepatic AE were treated with ex vivo resection and liver autotransplantation between January 2010 and February 2017. The feasibility, safety and long-term clinical outcome of this technique were assessed. Ex vivo extended hepatectomy with autotransplantation was successful in all patients without intraoperative mortality. The median weight of the graft and AE lesion were 850 (370–1,600) g and 1,650 (375–5,000) g, respectively. The median duration of the operation and anhepatic phase were 15.9 (8–24) h and 360 (104–879) min, respectively. Six patients did not need any blood transfusion. Complications higher than IIIa according to Clavien classification were observed in 10 patients. The 30-day-mortality and overall mortality (>90 days) were 7.24% (5/69) and 11.5% (8/69), respectively. The mean hospital stay was 34.5 (12–128) days. Patients were followed-up systematically for a median of 22.5 months (14–89) without recurrence. This is the largest series assessing ex vivo liver resection and autotransplantation in end-stage hepatic AE. This technique could be an effective alternative to liver transplantation in patients with end-stage hepatic AE, with the advantage that it does not require an organ nor immunosuppressive agents. Hepatic alveolar echinococcosis (AE) is a lethal infectious disease caused by the larval stage of Echinococcus multilocularis (E. multilocularis). This severe disease remains a major public health issue in pastoral areas in China, Turkey, Central Asia, the Mediterranean and some European countries.1 AE is usually chronic and asymptomatic while primary hepatic involvement presents with a mortality rate of 75% to 90% after 10 to 15 years if untreated. So far surgery associated with albendazole medication has been considered a major radical procedure for clinically diagnosed patients with AE. Of note, in late diagnosed cases, very few patients can benefit from surgery, due to extensive disease progression.2 Retrospective studies have shown that palliative surgical procedures should be avoided in such cases, and the only feasible treatment is the long-term use of oral benzimidazoles.3 Nevertheless, lifelong medications and their major potential side effects and the numerous complications of palliative resections with endless biliary drainage have motivated more radical approaches.4 Given the devastating complications of AE and the limited surgical options available in advanced cases, transplantations emerged as an option for both curative and often palliative care. Although it was presented as a legitimate approach, the need for an organ donor is a limitation and lifelong immuno-suppressants increase the risk of recurrence. For these reasons, the decision to proceed to transplantation should be considered extremely cautiously.5 
Chronic failure of mechanisms that promote effective regeneration of dead hepatocytes causes replacement of functional hepatic parenchyma with fibrous scar tissue, ultimately resulting in cirrhosis. Therefore, defining and optimizing mechanisms that orchestrate effective regeneration might prevent cirrhosis. We hypothesized that effective regeneration of injured livers requires hepatocytes to evade the growth-inhibitory actions of TGFβ, since TGFβ signaling inhibits mature hepatocyte growth but drives cirrhosis pathogenesis. Wild-type mice underwent 70% partial hepatectomy (PH); TGFβ expression and signaling were evaluated in intact tissue and primary hepatocytes before, during, and after the period of maximal hepatocyte proliferation that occurs from 24–72 h after PH. To determine the role of Yap1 in regulating TGFβ signaling in hepatocytes, studies were repeated after selectively deleting Yap1 from hepatocytes of Yap1flox/flox mice. TGFβ expression and hepatocyte nuclear accumulation of pSmad2 and Yap1 increased in parallel with hepatocyte proliferative activity after PH. Proliferative hepatocytes also upregulated Snai1, a pSmad2 target gene that promotes epithelial-to-mesenchymal transition (EMT), suppressed epithelial genes, induced myofibroblast markers, and produced collagen 1α1. Deleting Yap1 from hepatocytes blocked their nuclear accumulation of pSmad2 and EMT-like response, as well as their proliferation. Interactions between the TGFβ and Hippo-Yap signaling pathways stimulate hepatocytes to undergo an EMT-like response that is necessary for them to grow in a TGFβ-enriched microenvironment and regenerate injured livers. Adult liver has robust regenerative capacity as demonstrated by efficient restitution of fully functional liver mass within days to weeks after acute 70% partial hepatectomy (PH) in mice and humans, respectively.1 Paradoxically, chronically dysregulated repair of even minor liver damage causes defective recovery of healthy liver parenchyma and ultimately results in cirrhosis and/or liver cancer. Preventing these bad outcomes of liver injury is necessary to reduce mortality from liver disease, a major cause of premature death in adulthood worldwide.2 Success has been elusive because of relatively limited understanding of the mechanisms that promote effective reconstruction of this complex organ in adults. Old dogma posited that adult liver regeneration mainly involved replication of surviving mature hepatocytes. However, it is becoming clear that reconstruction of healthy liver parenchyma after both acute and chronic injury requires an orderly cascade of diverse repair responses that are precisely and coordinately regulated to assure that all resident liver cell populations are replaced and appropriately integrated to reconstitute diverse tissue-specific functions. PH is a useful model for teasing apart these regenerative mechanisms.3 
Intratumor heterogeneity has frequently been reported in patients with hepatocellular carcinoma (HCC). Thus, the reliability of single-region tumor samples for evaluation of the tumor immune microenvironment is also debatable. We conducted a prospective study to analyze the similarity in tumor immune microenvironments among different regions of a single tumor. Multi-region sampling was performed on newly resected tumors. The tumor immune microenvironment was evaluated by immunohistochemical staining of PD-L1, CD4, CD8, CD20, FoxP3, DC-LAMP (or LAMP3), CD68, MPO, and tertiary lymphoid structures (TLSs). PD-L1 expression was manually quantified according to the percentage of PD-L1-stained tumor or stromal cells. The densities (number/mm2) of immune cells and the number of TLSs per sample were determined by whole-section counting. RNA-sequencing was applied in selected samples. Similarities in tumor immune microenvironments within each tumor were evaluated by multivariate Mahalanobis distance analyses. Thirteen tumors were collected from 12 patients. The median diameter of tumors was 9 cm (range 3–16 cm). A median of 6 samples (range 3–12) were obtained from each tumor. Nine (69.2%) tumors exhibited uniform expression of PD-L1 in all regions of the tumor. Out of 13 tumors analyzed by immunohistochemical staining, 8 (61.5%) tumors displayed a narrow Mahalanobis distance for all regions within the tumor; while 8 (66.7%) of the 12 tumors analyzed by RNA-sequencing displayed a narrow Mahalanobis distance. Immunohistochemistry and RNA-sequencing had a high concordance rate (83.3%; 10 of 12 tumors) for the evaluation of similarities between tumor immune microenvironments within a tumor. A single-region tumor sample might be reliable for the evaluation of tumor immune microenvironments in approximately 60–70% of patients with HCC. Identification of tissue-based immunological biomarkers is a critical step toward personalized immunotherapy and is usually conducted using formalin-fixed paraffin-embedded (FFPE) sections from a core biopsy specimen or a wax block from a previously resected tumor. However, the identification of predictive biomarkers for programmed cell death-1 (PD-1) inhibitors in patients with hepatocellular carcinoma (HCC) has not been achieved. Theoretically, the expression of programmed cell death ligand 1 (PD-L1) or the quantification of tumor-infiltrating T cells could predict responses to PD-1/PD-L1 blockade. However, none of these methods could clearly differentiate responders from non-responders in clinical trials.1,2 
Pathological bacterial translocation (PBT) in cirrhosis is the hallmark of spontaneous bacterial infections, increasing mortality several-fold. Increased intestinal permeability is known to contribute to PBT in cirrhosis, although the role of the mucus layer has not been addressed in detail. A clear route of translocation for luminal intestinal bacteria is yet to be defined, but we hypothesize that the recently described gut-vascular barrier (GVB) is impaired in experimental portal hypertension, leading to increased accessibility of the vascular compartment for translocating bacteria. Cirrhosis was induced in mouse models using bile-duct ligation (BDL) and CCl4. Pre-hepatic portal-hypertension was induced by partial portal vein ligation (PPVL). Intestinal permeability was compared in these mice after GFP-Escherichia coli or different sized FITC-dextrans were injected into the intestine. Healthy and pre-hepatic portal-hypertensive (PPVL) mice lack translocation of FITC-dextran and GFP-E. coli from the small intestine to the liver, whereas BDL and CCl4-induced cirrhotic mice demonstrate pathological translocation, which is not altered by prior thoracic-duct ligation. The mucus layer is reduced in thickness, with loss of goblet cells and Muc2-staining and expression in cirrhotic but not PPVL mice. These changes are associated with bacterial overgrowth in the inner mucus layer and pathological translocation of GFP-E. coli through the ileal epithelium. GVB is profoundly altered in BDL and CCl4-mice with Ileal extravasation of large-sized 150 kDa-FITC-dextran, but only slightly altered in PPVL mice. This pathological endothelial permeability and accessibility in cirrhotic mice is associated with augmented expression of PV1 in intestinal vessels. OCA but not fexaramine stabilizes the GVB, whereas both FXR-agonists ameliorate gut to liver translocation of GFP-E. coli. Cirrhosis, but not portal hypertension per se, grossly impairs the endothelial and muco-epithelial barriers, promoting PBT to the portal-venous circulation. Both barriers appear to be FXR-modulated, with FXR-agonists reducing PBT via the portal-venous route. The gut-liver axis represents the pathophysiological hallmark for initiation and/or perpetuation of multiple liver diseases1 and has been proposed to be fueled by pathological bacterial translocation (PBT) from the gut.2 In liver cirrhosis, PBT from the gut into the liver and systemic circulation is one of the causes of bacterial infections and the augmented pro-inflammatory response to gut-derived products.2,3 In fact, failure to control invading bacteria and bacterial products in concert with host susceptibility determines remote organ injury in liver cirrhosis. This may include acute-on-chronic liver failure, hepatorenal syndrome and hepatic encephalopathy, which are all associated with worsening prognosis.4 PBT in liver cirrhosis has been attributed to small intestinal bacterial overgrowth, increased intestinal permeability and lack of host defense mechanisms.5 Herein, we focused on the first and last barrier separating luminal bacteria and the vascular compartment, namely intestinal mucus and the newly defined gut-vascular barrier (GVB),6 neither of which have been addressed so far in liver cirrhosis and PBT. 
Alpha-1 antitrypsin deficiency (AATD) is a genetic disorder causing pulmonary and liver disease. The PiZ mutation in AAT (SERPINA1) results in mis-folded AAT protein (Z-AAT) accumulating in hepatocytes, leading to fibrosis and cirrhosis. RNAi-based therapeutics silencing production of hepatic Z-AAT might benefit patients with AATD-associated liver disease. This study evaluated an RNAi therapeutic to silence production of AAT. Part A of this double-blind first-in-human study randomized 54 healthy volunteers (HVs) into single dose cohorts (two placebo: four active), receiving escalating doses of the investigational agent ARC-AAT from 0.38 to 8.0 mg/kg or placebo. Part B randomized 11 patients with PiZZ (homozygous for Z-AAT) genotype AATD, who received up to 4.0 mg/kg of ARC-AAT or placebo. Patients with baseline FibroScan® >11 kPa or forced expiratory volume in one second (FEV1) <60% were excluded. Assessments included safety, pharmacokinetics, and change in serum AAT concentrations. A total of 36 HVs received ARC-AAT and 18 received placebo (part A). Seven PiZZ individuals received ARC-AAT and four received placebo (part B). A dose response in serum AAT reduction was observed at doses ≥4 mg/kg with similar relative reductions in PiZZ patients and HVs at 4 mg/kg and a maximum reduction of 76.1% (HVs) vs. 78.8% (PiZZ) at this dose. The time it took for serum AAT to return to baseline was similar for HV and PiZZ. There were no notable differences between HV and PiZZ safety parameters. The study was terminated early because of toxicity findings related to the delivery vehicle (ARC-EX1) seen in a non-human primate study. PiZZ patients and HVs responded similarly to ARC-AAT. Deep and durable knockdown of hepatic AAT production based on observed reduction in serum AAT concentrations was demonstrated. Alpha-1 antitrypsin deficiency (AATD) is an autosomal co-dominant disease which predisposes patients to early-onset emphysema and liver cirrhosis.1 Estimates of the number of adult patients with AATD exhibiting clinically significant liver disease have varied from 10% (defined by aminotransferase abnormalities)2,3 to 18% in a UK cohort assessed by ultrasound (+/− biopsy)4 to approximately 35% in those with an Ishak fibrosis score ≥2 assessed histologically after liver biopsy.5 A Swedish autopsy series from patients with AATD and the PiZZ genotype found liver disease to be present in the majority of patients, albeit unrecognized in many cases during life.6 In common with other causes of cirrhosis, many factors are thought to affect clinical presentation, including alcohol use,7 body mass index,8 gender9 and genetic modifiers. Liver disease shows a bimodal clinical presentation. Prolonged neonatal jaundice is common in newborns with the PiZZ genotype, with most recovering but others progressing to cirrhotic disease that requires transplantation at a young age.10,11 A second peak occurs in adults who can present with clinical liver disease that can also progress to cirrhosis and transplant, even in the absence of a childhood history of liver disease.3 No definitive methods to predict progression to cirrhosis exist at present in either the pediatric or adult populations. Current management of liver disease in AATD is largely supportive, with transplantation the only definitive treatment. In the U.S., AATD accounts for approximately 1.5% of liver transplants with ∼76 transplants performed annually12 with outcomes similar to other common indications for transplant.13 
The pathogenesis of non-alcoholic fatty liver disease (NAFLD) and steatohepatitis (NASH) is likely due to the interaction between a deranged metabolic milieu and local mediators of hepatic inflammation and fibrosis. We undertook this study to elucidate the interplay between macrophage activation, insulin resistance (IR) in target organs/tissues and hepatic damage. In 40 non-diabetic patients with biopsy-proven NAFLD we assessed: i) endogenous glucose production (EGP), glucose clearance and indexes of IR in the adipose tissue (Adipo-IR and Lipo-IR) and in the liver (Hep-IR) by tracer infusion ([6,6-2H2]glucose and [2H5]glycerol); ii) macrophage activity (by soluble sCD163) and iii) hepatic expression of CD163 (hCD163). We found that sCD163 levels paralleled both the plasma free fatty acid (FFA) levels and lipolysis from adipose tissue. Consistently, sCD163 significantly correlated with adipose tissue IR (Adipo-IR: r = 0.32, p = 0.042; Lipo-IR: r = 0.39, p = 0.012). At multiple regression analysis, sCD163 levels were associated with FFA levels (rp = 0.35, p = 0.026). In vitro exposure of human monocyte-derived macrophages to palmitate enhanced sCD163 secretion. Conversely, sCD163 did not correlate with EGP or with Hep-IR. In the liver, hCD163 positively correlated with sCD163 (r = 0.58, p = 0.007) and the degree of steatosis (r = 0.34, p = 0.048), but not with EGP or Hep-IR (r = −0.27 and r = 0.11, respectively, p >0.10, both). Our findings suggest a link between deranged metabolism in the adipose tissue and activation of hepatic macrophages in patients with NAFLD, possibly in response to FFA overflow and independent of obesity and diabetes. Conversely, our findings do not support a link between activated hepatic macrophages and glucose metabolism (EGP or Hep-IR). The relationship between adipose tissue IR and hepatic macrophages should be considered to define therapeutic targets for NAFLD. The recent epidemic of chronic liver disease is related to the burden of non-alcoholic fatty liver disease (NAFLD), paralleling the worldwide increase of obesity.1 NAFLD is a complex condition related to metabolic derangements in insulin resistance (IR), but in a subset of patients the liver becomes the target of multiple hits leading to non-alcoholic steatohepatitis (NASH), the histological phenotype that may progressively lead to the development of liver fibrosis, cirrhosis and possibly hepatocellular carcinoma.1,2 Understanding the biological and environmental factors that drive the progression to NASH and beyond in some individuals is fundamental to the development of robust methods for diagnosis, risk stratification and therapy.2 
Cholangiocarcinoma is an aggressive hepatobiliary malignancy originating from biliary tract epithelium. Whether cholangiocarcinoma is responsive to immune checkpoint antibody therapy is unknown, and knowledge of its tumor immune microenvironment is limited. We aimed to characterize tumor-infiltrating lymphocytes (TILs) in cholangiocarcinoma and assess functional effects of targeting checkpoint molecules on TILs. We isolated TILs from resected tumors of patients with cholangiocarcinoma and investigated their compositions compared with their counterparts in tumor-free liver (TFL) tissues and blood, by flow cytometry and immunohistochemistry. We measured expression of immune co-stimulatory and co-inhibitory molecules on TILs, and determined whether targeting these molecules improved ex vivo functions of TILs. Proportions of cytotoxic T cells and natural killer cells were decreased, whereas regulatory T cells were increased in tumors compared with TFL. While regulatory T cells accumulated in tumors, the majority of cytotoxic and helper T cells were sequestered at tumor margins, and natural killer cells were excluded from the tumors. The co-stimulatory receptor GITR and co-inhibitory receptors PD1 and CTLA4 were over-expressed on tumor-infiltrating T cells compared with T cells in TFL and blood. Antagonistic targeting of PD1 or CTLA4 or agonistic targeting of GITR enhanced effector molecule production and T cell proliferation in ex vivo stimulation of TILs derived from cholangiocarcinoma. The inter-individual variations in TIL responses to checkpoint treatments were correlated with differences in TIL immune phenotype. Decreased numbers of cytotoxic immune cells and increased numbers of suppressor T cells that over-express co-inhibitory receptors suggest that the tumor microenvironment in cholangiocarcinoma is immunosuppressive. Targeting GITR, PD1 or CTLA4 enhances effector functions of tumor-infiltrating T cells, indicating that these molecules are potential immunotherapeutic targets for patients with cholangiocarcinoma. Liver cancer is the second most common cause of cancer-related mortality worldwide.1 Cholangiocarcinoma (CCA) accounts for 10% of primary liver cancers and the incidence is significantly increasing. CCA is an aggressive hepatobiliary malignancy originating from the biliary tract epithelium with features of cholangiocyte differentiation.2 It is classified into the following types according to its anatomic location along the biliary tree: intrahepatic (iCCA), perihilar (pCCA) and distal (dCCA).2,3 The median overall survival after diagnosis is 24 months and 5-year survival rate is around 10%.4 The current treatment options for CCA are very limited. Surgical resection is potentially curative, but only 10% of patients are eligible for surgical resection and it is associated with a high recurrence rate (>50%).2 Liver transplantation is a curative option for selected patients with pCCA but not with iCCA or dCCA.3 The therapeutic effect of chemotherapy for advanced CCA is disappointing.2 Therefore, more effective therapies for curing CCA and preventing recurrence are urgently needed. 
HCV infection is associated with several extrahepatic manifestations (EHMs). We evaluated the impact of sustained virological response (SVR) on the risk of 7 EHMs that contribute to the burden of extrahepatic disease: type 2 diabetes mellitus, chronic kidney disease or end-stage renal disease, stroke, ischemic heart disease, major adverse cardiac events, mood and anxiety disorders, and rheumatoid arthritis. A longitudinal cohort study was conducted using data from the British Columbia Hepatitis Testers Cohort, which included ~1.3 million individuals screened for HCV. We identified all HCV-infected individuals who were treated with interferon-based therapies between 1999 and 2014. SVR was defined as a negative HCV RNA test ≥24 weeks post-treatment or after end-of-treatment, if unavailable. We computed adjusted subdistribution hazard ratios (asHR) for the effect of SVR on each EHM using competing risk proportional hazard models. Subgroup analyses by birth cohort, sex, injection drug exposure and genotype were also performed. Overall, 10,264 HCV-infected individuals were treated with interferon, of whom 6,023 (59%) achieved SVR. Compared to those that failed treatment, EHM risk was significantly reduced among patients with SVR for type 2 diabetes mellitus (asHR 0.65; 95% CI 0.55–0.77), chronic kidney disease or end-stage renal disease (asHR 0.53; 95% CI 0.43–0.65), ischemic or hemorrhagic stroke (asHR 0.73; 95% CI 0.49–1.09), and mood and anxiety disorders (asHR 0.82; 95% CI 0.71–0.95), but not for ischemic heart disease (asHR 1.23; 95% CI 1.03–1.47), major adverse cardiac events (asHR 0.93; 95% CI 0.79–1.11) or rheumatoid arthritis (asHR 1.09; 95% CI 0.73–1.64). SVR was associated with a reduction in the risk of several EHMs. Increased uptake of antiviral therapy may reduce the growing burden of EHMs in this population. Chronic HCV infection, which affects more than 71 million people worldwide, is an important source of morbidity and mortality.1 Although liver-related sequelae have been well characterized, extrahepatic manifestations (EHM) associated with HCV have received considerably less attention.2–4 These EHMs include metabolic,5 cardiovascular,6 renal,7 autoimmune,8 lymphoproliferative,9 and neurologic conditions which are estimated to be present in as many as 31% of HCV-infected individuals.3,10,11 Direct medical costs associated with EHMs are substantial and range between $72 million and $443 million US dollars per year.3 Increasing rates of healthcare resource utilization associated with EHMs have also been reported12,13 and, with a largely aging HCV-infected population, there is a greater need to assess the broader HCV-related disease burden and interventions to reduce future morbidity and mortality. 
The liver is the main hematopoietic site in embryos, becoming a crucial organ in both immunity and metabolism in adults. However, how the liver adapts both the immune system and enzymatic profile to challenges in the postnatal period remains elusive. We aimed to identify the mechanisms underlying this adaptation. We analyzed liver samples from mice on day 0 after birth until adulthood. Human biopsies from newborns and adults were also examined. Liver immune cells were phenotyped using mass cytometry (CyTOF) and expression of several genes belonging to immune and metabolic pathways were measured. Mortality rate, bacteremia and hepatic bacterial retention after E. coli challenge were analyzed using intravital and in vitro approaches. In a set of experiments, mice were prematurely weaned and the impact on gene expression of metabolic pathways was evaluated. Human and mouse newborns have a sharply different hepatic cellular composition and arrangement compared to adults. We also found that myeloid cells and immature B cells primarily compose the neonatal hepatic immune system. Although neonatal mice were more susceptible to infections, a rapid evolution to an efficient immune response was observed. Concomitantly, newborns displayed a reduction of several macronutrient metabolic functions and the normal expression level of enzymes belonging to lipid and carbohydrate metabolism was reached around the weaning period. Interestingly, early weaning profoundly disturbed the expression of several hepatic metabolic pathways, providing novel insights into how dietary schemes affect the metabolic maturation of the liver. In newborns, the immune and metabolic profiles of the liver are dramatically different to those of the adult liver, which can be explained by the differences in the liver cell repertoire and phenotype. Also, dietary and antigen cues may be crucial to guide liver development during the postnatal phase. The liver has multiple functions, becoming vitally important from the gestational phase.1 The primordial liver is formed in the initial stages of embryonic development (week 4 in humans and E8.5 in mouse), and it is the destination of several immune cell precursors that arise from the yolk sac.2 Fetal liver harbors different hepatic progenitor cells and a great number of other mesenchyme-derived cells, including immune cells in different stages of maturation.3 In fact, macrophages, hematopoietic stem cells (HSCs), endothelial and mesenchymal stem cells compose a complex cellular repertoire during the major part of hepatic embryogenesis.4 The dynamics by which HSCs seed the liver has become a growing field of interest,5 and understanding how tissue macrophages mature within organs and are maintained throughout life may have a significant impact on both basic and clinical investigations.6 
It is currently unclear which antiviral agent, entecavir (ETV) or tenofovir disoproxil fumarate (TDF), is superior for improving prognosis in patients with chronic hepatitis B (CHB). Here, we assessed the ability of these 2 antivirals to prevent liver-disease progression in treatment-naïve patients with CHB. From 2012 to 2014, treatment-naïve patients with CHB who received ETV or TDF as a first-line antiviral agent were recruited from 4 academic teaching hospitals. Patients with decompensated cirrhosis or hepatocellular carcinoma (HCC) at enrollment were excluded. Cumulative probabilities of HCC and death or orthotopic liver transplant (OLT) were assessed. In total, 2,897 patients (1,484 and 1,413 in the ETV and TDF groups, respectively) were recruited. The annual HCC incidence was not statistically different between the ETV and TDF groups (1.92 vs. 1.69 per 100 person-years [PY], respectively; adjusted hazard ratio [HR] 0.975 [p = 0.852] by multivariate analysis). Propensity score (PS)-matched and inverse probability of treatment weighting (ITPW) analyses yielded similar patterns of results (HR 1.021 [p = 0.884] and 0.998 [p = 0.988], respectively). The annual incidence of death or OLT was not statistically different between the ETV and TDF groups (0.52 vs. 0.53 per 100 PY, respectively; adjusted HR 1.202 [p = 0.451]). PS-matched and ITPW analyses yielded similar patterns of results (HR 1.248 [p = 0.385] and 1.239 [p = 0.360], respectively). These findings were consistently reproduced in patients with compensated cirrhosis (all p >0.05). The overall prognosis in terms of HCC and death or OLT was not statistically different between the ETV and TDF groups. Further studies are needed to validate our results. Chronic hepatitis B (CHB) is the most common chronic viral infection worldwide, affecting approximately 350 million people.1 Because persistently high hepatitis B virus (HBV) replication is associated with an increased risk of compensated cirrhosis and hepatocellular carcinoma (HCC),2,3 replication-suppressing antiviral therapy is administered to patients with CHB to prevent liver-disease progression.4 As a matter of fact, oral antiviral agents, particularly entecavir (ETV), reduce the risk of long-term complications such as cirrhosis and HCC, ultimately improving survival compared to controls.5,6 Nevertheless, because HBV is rarely eradicated from hepatocytes, most patients with CHB require long-term antiviral therapy.7,8 
Upon ligand binding, tyrosine kinase receptors, such as epidermal growth factor receptor (EGFR), are recruited into clathrin-coated pits for internalization by endocytosis, which is relevant for signalling and/or receptor degradation. In liver cells, transforming growth factor-β (TGF-β) induces both pro- and anti-apoptotic signals; the latter are mediated by the EGFR pathway. Since EGFR mainly traffics via clathrin-coated vesicles, we aimed to analyse the potential role of clathrin in TGF-β-induced signalling in liver cells and its relevance in liver cancer. Real-Time PCR and immunohistochemistry were used to analyse clathrin heavy-chain expression in human (CLTC) and mice (Cltc) liver tumours. Transient knockdown (siRNA) or overexpression of CLTC were used to analyse its role on TGF-β and EGFR signalling in vitro. Bioinformatic analysis was used to determine the effect of CLTC and TGFB1 expression on prognosis and overall survival in patients with hepatocellular carcinoma (HCC). Clathrin expression increased during liver tumorigenesis in humans and mice. CLTC knockdown cells responded to TGF-β phosphorylating SMADs (canonical signalling) but showed impairment in the anti-apoptotic signals (EGFR transactivation). Experiments of loss or gain of function in HCC cells reveal an essential role for clathrin in inhibiting TGF-β-induced apoptosis and upregulation of its pro-apoptotic target NOX4. Autocrine TGF-β signalling in invasive HCC cells upregulates CLTC expression, switching its role to pro-tumorigenic. A positive correlation between TGFB1 and CLTC was found in HCC cells and patients. Patients expressing high levels of TGFB1 and CLTC had a worse prognosis and lower overall survival. This work describes a novel role for clathrin in liver tumorigenesis, favouring non-canonical pro-tumorigenic TGF-β pathways. CLTC expression in human HCC samples could help select patients that would benefit from TGF-β-targeted therapy. Primary hepatic endocytic functions are important in several physiological and pathological processes. Despite this, their molecular mechanisms remain poorly defined and remarkably understudied.1 Ligand-induced internalization and degradation of receptor tyrosine kinases, such as epidermal growth factor receptor (EGFR) or hepatocyte growth factor receptor (c-MET), are relevant for maintenance and inhibition of their signalling pathways. Upon binding their respective ligands, each of these receptors are recruited into clathrin-coated pits eventually leading to endocytosis. However, clathrin might play additional roles, since Akt signalling following EGFR or MET activation requires clathrin, but could not require receptor endocytosis.2 EGFR mediates differential signalling depending on its localization in the cell.3,4 At the plasma membrane, clathrin is present in microdomains,5,6 where EGFR clustering occurs.7 In these microdomains, clathrin may act as a scaffold protein, recruiting signalling adaptors. In human hepatocellular carcinoma (HCC), levels of clathrin heavy-chain protein help to distinguish early HCC from benign tumours and its expression is stronger in poorly differentiated HCC than in well differentiated HCC.8–10 However, little is known about the molecular mechanisms behind these results. 
Controlled attenuation parameter (CAP) is a novel non-invasive measure of hepatic steatosis, but it has not been evaluated in alcoholic liver disease. Therefore, we aimed to validate CAP for the assessment of biopsy-verified alcoholic steatosis and to study the effect of alcohol detoxification on CAP. This was a cross-sectional biopsy-controlled diagnostic study in four European liver centres. Consecutive alcohol-overusing patients underwent concomitant CAP, regular ultrasound, and liver biopsy. In addition, we measured CAP before and after admission for detoxification in a separate single-centre cohort. A total of 562 patients were included in the study: 269 patients in the diagnostic cohort with steatosis scores S0, S1, S2, and S3 = 77 (28%), 94 (35%), 64 (24%), and 34 (13%), respectively. CAP diagnosed any steatosis and moderate steatosis with fair accuracy (area under the receiver operating characteristic curve [AUC] ≥S1 = 0.77; 0.71–0.83 and AUC ≥S2 = 0.78; 0.72–0.83), and severe steatosis with good accuracy (AUC S3 = 0.82; 0.75–0.88). CAP was superior to bright liver echo pattern by regular ultrasound. CAP above 290 dB/m ruled in any steatosis with 88% specificity and 92% positive predictive value, while CAP below 220 dB/m ruled out steatosis with 90% sensitivity, but 62% negative predictive value. In the 293 patients who were admitted 6.3 days (interquartile range 4–6) for detoxification, CAP decreased by 32 ± 47 dB/m (p <0.001). Body mass index predicted higher CAP in both cohorts, irrespective of drinking pattern. Obese patients with body mass index ≥30 kg/m2 had a significantly higher CAP, which did not decrease significantly during detoxification. CAP has a good diagnostic accuracy for diagnosing severe alcoholic liver steatosis and can be used to rule in any steatosis. In non-obese but not in obese, patients, CAP rapidly declines after alcohol withdrawal. Alcohol is a key risk factor for liver-related and overall mortality, contributing to 3.3 million annual deaths worldwide.1 Simple steatosis is the most common liver manifestation of harmful drinking, but is considered a benign condition by many, since the disturbed lipid metabolism normalises with abstinence.2,3 The high prevalence of steatosis in most liver diseases has even cast doubt on the role of hepatic fat on fibrosis progression, with steatosis being discussed as a bystander rather than a causative factor.4,5 This notion may, however, be too credulous, as 7% of patients with simple alcoholic steatosis have been shown to progress to cirrhosis within five years.6 Additionally, in non-alcoholic fatty-liver disease, steatosis is an independent risk factor for the development of type 2 diabetes and cardiovascular disease.7 Finally, harmful drinking is often accompanied by similar unfavourable health behaviours, such as overeating, smoking, and a sedentary lifestyle. Thus, components of the metabolic syndrome (MetS) are common in alcoholic patients, where they may aggravate steatosis and act in synergy with alcohol to progress fibrosis.8 Consequently, reliable non-invasive tools to diagnose and monitor hepatic steatosis in patients with alcoholic liver disease (ALD) are needed. 
Liver failure results in hyperammonaemia, impaired regulation of cerebral microcirculation, encephalopathy, and death. However, the key mediator that alters cerebral microcirculation remains unidentified. In this study we show that topically applied ammonium significantly increases periarteriolar adenosine tone on the brain surface of healthy rats and is associated with a disturbed microcirculation. Cranial windows were prepared in anaesthetized Wistar rats. The flow velocities were measured by speckle contrast imaging and compared before and after 30 min of exposure to 10 mM ammonium chloride applied on the brain surface. These flow velocities were compared with those for control groups exposed to artificial cerebrospinal fluid or ammonium plus an adenosine receptor antagonist. A flow preservation curve was obtained by analysis of flow responses to a haemorrhagic hypotensive challenge and during stepwise exsanguination. The periarteriolar adenosine concentration was measured with enzymatic biosensors inserted in the cortex. After ammonium exposure the arteriolar flow velocity increased by a median (interquartile range) of 21.7% (23.4%) vs. 7.2% (10.2%) in controls (n = 10 and n = 6, respectively, p <0.05), and the arteriolar surface area increased. There was a profound rise in the periarteriolar adenosine concentration. During the hypotensive challenge the flow decreased by 27.8% (14.9%) vs. 9.2% (14.9%) in controls (p <0.05). The lower limit of flow preservation remained unaffected, 27.7 (3.9) mmHg vs. 27.6 (6.4) mmHg, whereas the autoregulatory index increased, 0.29 (0.33) flow units per millimetre of mercury vs. 0.03 (0.21) flow units per millimetre of mercury (p <0.05). When ammonium exposure was combined with topical application of an adenosine receptor antagonist, the autoregulatory index was normalized. Vasodilation of the cerebral microcirculation during exposure to ammonium chloride is associated with an increase in the adenosine tone. Application of a specific adenosine receptor antagonist restores the regulation of the microcirculation. This indicates that adenosine could be a key mediator of the brain dysfunction seen during hyperammonaemia and is a potential therapeutic target. Ammonium has repeatedly been found to be a key factor in the pathogenesis of hepatic encephalopathy (HE)1 and development of brain oedema in acute liver failure.2 Yet, the underlying mechanisms are still not understood in detail; specifically, a mediator responsible for disturbances in the microcirculation and vasodilation remains to be identified. 
Ledipasvir/sofosbuvir (LDV/SOF) for 8 to 24 weeks is approved for the treatment of chronic hepatitis C virus infection (HCV). In the ION-3 study, 8 weeks of LDV/SOF was non-inferior to 12 weeks in previously untreated genotype 1 (GT1) patients without cirrhosis. According to the Summary of Product Characteristics (SmPC), 8-week treatment may be considered in naïve non-cirrhotic GT1-patients. However, there are only limited data on the effectiveness of an 8-week regimen of LDV/SOF under real-world conditions. The aim of the present study was to characterise patients receiving 8 weeks of LDV/SOF compared to those receiving 12 weeks of LDV/SOF, and to describe therapeutic outcomes in routine clinical practice. The German Hepatitis C-Registry is a large national real-world cohort that analyses effectiveness and safety of antiviral therapies in chronic HCV. This data set is based on 2,404 patients. Treatment with SOF/LDV (without RBV) for 8 or 12 weeks was initiated on or before September 30, 2015. Overall, 84.6% (2,034/2,404) of the safety population (intention-to-treat-1 [ITT1]) and 98.2% (2,029/2,066) of the per protocol (PP) population achieved sustained virological response at week 12 (SVR12). In the 8-week group, 85.1% (824/968) of ITT1 and 98.3% (821/835) of PP patients achieved SVR12, while in the 12-week group, 85.5% (1,210/1,415) of ITT1, and 98.1% (1,208/1,231) of PP patients achieved SVR12. When treated according to the SmPC, 98.7% (739/749) of the patients achieved SVR12 (PP). Relapse was observed in 9.5% (2/21) of cirrhotic patients treated for 8 weeks (PP). Under real-world conditions a high proportion of eligible patients receiving 8-week LDV/SOF treatment achieved SVR12. Relapse occurred more frequently in patients who did not meet the selection criteria according to the SmPC. The availability of all-oral direct-acting antivirals (DAAs) has led to rapid advances in the treatment of chronic hepatitis C virus (HCV) infection over recent years. Clinical trials have shown sustained virological response (SVR) rates above 90% with well-tolerated combinations of DAAs, including in patients with traditionally more difficult to treat disease, such as those with cirrhosis and HIV co-infection. Ledipasvir-sofosbuvir (LDV/SOF) is a fixed-dose combination of DAAs which inhibit HCV non-structural (NS) 5A and 5B proteins. The large-scale ION series of phase III clinical trials demonstrated SVR rates between 93–100% in treatment-naïve and treatment-experienced patients, with and without cirrhosis, with once-daily administration of LDV/SOF ± ribavirin (RBV) for 8 to 24 weeks.1–3 Based on the findings of these and other studies, LDV/SOF with or without RBV is indicated in Europe for the treatment of adult patients with HCV genotype 1, 3, 4, 5 and 6 infections, with a treatment duration of 8, 12 or 24 weeks depending on HCV genotype and patient factors, including previous treatment history and the presence of cirrhosis. In the ION-3 study, treatment with LDV/SOF for 8 weeks was shown to be non-inferior to 12 weeks in previously untreated genotype 1 patients without cirrhosis, with no benefit of the addition of RBV.3 Accordingly, the European Summary of Product Characteristics (SmPC) recommends that 8-week treatment with LDV/SOF can be considered in these patients. Relapse rates in ION-3 were low overall, although slightly higher in the 8-week treatment group compared with the 12-week group (5% vs. 1%, respectively).3 Subsequently, a post hoc analysis of data from ION-3 determined a cut-off value for baseline HCV RNA levels of <6 million IU/ml, which may identify patients less likely to relapse with 8 weeks of therapy. While this cut-off is also listed as an indicator for 8 weeks of treatment in US prescribing information for LDV/SOF, the European SmPC does not include this as a recommendation, although European treatment guidelines suggest that this should be considered.4 
The main stages of cirrhosis (compensated and decompensated) have been sub-staged based on clinical, endoscopic, and portal pressure (determined by the hepatic venous pressure gradient [HVPG]) features. Vasodilation leading to a hyperdynamic circulatory state is central in the development of a late decompensated stage, with inflammation currently considered a key driver. We aimed to assess hepatic/systemic hemodynamics and inflammation (by C-reactive protein [CRP]) among the different sub-stages of cirrhosis and to investigate their interrelationship and prognostic relevance. A single center, prospective cohort of patients with cirrhosis undergoing per protocol hepatic and right-heart catheterization and CRP measurement, were classified into recently defined prognostic stages (PS) of compensated (PS1: HVPG ≥6 mmHg but <10 mmHg; PS2: HVPG ≥10 mmHg without gastroesophageal varices; PS3: patients with gastroesophageal varices) and decompensated (PS4: diuretic-responsive ascites; PS5: refractory ascites) disease. Cardiodynamic states based on cardiac index (L/min/m2) were created: relatively hypodynamic (<3.2), normodynamic (3.2–4.2) and hyperdynamic (>4.2). Of 238 patients, 151 were compensated (PS1 = 25; PS2 = 36; PS3 = 90) and 87 were decompensated (PS4 = 48; PS5 = 39). Mean arterial pressure decreased progressively from PS1 to PS5, cardiac index increased progressively from PS1-to-PS4 but decreased in PS5. HVPG, model for end-stage liver disease (MELD), and CRP increased progressively from PS1-to-PS5. Among compensated patients, age, HVPG, relatively hypodynamic/hyperdynamic state and CRP were predictive of decompensation. Among patients with ascites, MELD, relatively hypodynamic/hyperdynamic state, post-capillary pulmonary hypertension, and CRP were independent predictors of death/liver transplant. Our study demonstrates that, in addition to known parameters, cardiopulmonary hemodynamics and CRP are predictive of relevant outcomes, both in patients with compensated and decompensated cirrhosis. Cirrhosis represents the end-stage of chronic liver disease, with a course characterized by a transition from an asymptomatic compensated stage to a symptomatic decompensated stage.1 These stages have entirely different mortalities and should therefore be considered distinct entities. Decompensation is defined by the development of overt clinical complications of cirrhosis: ascites, variceal hemorrhage (VH), encephalopathy, and jaundice.1 
Yttrium-90 transarterial radioembolization (TARE) has shown promising efficacy in the treatment of patients with hepatocellular carcinoma (HCC), associated with portal vein tumor thrombus (PVTT). The aim of this study is to identify prognostic factors for survival in patients with HCC and PVTT undergoing TARE, and build a prognostic classification for these patients. This is a single center retrospective study conducted over six years (2010–2015), on consecutive patients undergoing TARE. Patients were included if they met the following criteria: presence of at least one measurable HCC, presence of PVTT not occluding the main portal trunk, absence of extrahepatic metastases, Child-Pugh score within B7, Eastern Cooperative Oncology Group performance status 0–1. Uni- and multivariable analysis was used to explore the variables that showed an independent relationship with survival. A prognostic score was then derived, and three prognostic categories were identified. A total of 120 patients were included in the study. Median overall survival (OS) was 14.1 months (95% CI 10.7–17.5) and median progression-free survival (PFS) was 6.5 months (95% CI 3.8–9.2). The only variables independently correlated with OS were bilirubin, extension of PVTT and tumor burden. Three prognostic categories were identified: favourable prognosis (0 points), intermediate prognosis (2–3 points) and dismal prognosis (>3 points). Median OS in the three categories was 32.2 months, 14.9 months and 7.8 months respectively (p <0.0001). PFS (p = 0.045) and the risk of liver decompensation (p <0.0001) also significantly differed along the same prognostic categories. Radioembolization with Yttrium-90 is an effective therapy for patients with HCC and PVTT. The proposed prognostic stratification may help to better identify good candidates for the treatment, and those for whom TARE may be futile. Hepatocellular carcinoma (HCC) is a global health problem and one of the leading causes of cancer-related death especially in patients with cirrhosis.1,2 By reason of the improvements in surveillance protocols, diagnostic tools and therapeutic armamentaria, diagnosis of early HCC is feasible in 30–60% of cases.3 However, a substantial proportion of patients still present with portal vein tumor thrombus (PVTT) either at onset of the disease or as a result of HCC recurrence or progression, leading to an advanced stage of the disease not amenable to curative treatments.4 Patients with HCC and PVTT may have an asymptomatic presentation, although in most instances they have a significant degree of synthetic dysfunction and an impending liver decompensation that precludes any attempt at surgical cure. Moreover, when the portal circulation is compromised by thrombosis, transarterial embolo-therapies may increase the risk of liver failure; therefore, the presence of PVTT is generally considered a contraindication to transarterial chemoembolization (TACE).5,6 
Plectin, a highly versatile cytolinker protein, controls intermediate filament cytoarchitecture and cellular stress response. In the present study, we investigate the role of plectin in the liver under basal conditions and in experimental cholestasis. We generated liver-specific plectin knockout (PleΔalb) mice and analyzed them using two cholestatic liver injury models: bile duct ligation (BDL) and 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC) feeding. Primary hepatocytes and a cholangiocyte cell line were used to address the impact of plectin on keratin filament organization and stability in vitro. Plectin deficiency in hepatocytes and biliary epithelial cells led to aberrant keratin filament network organization, biliary tree malformations, and collapse of bile ducts and ductules. Further, plectin ablation significantly aggravated biliary damage upon cholestatic challenge. Coincidently, we observed a significant expansion of A6-positive progenitor cells in PleΔalb livers. After BDL, plectin-deficient bile ducts were prominently dilated with more frequent ruptures corresponding to an increased number of bile infarcts. In addition, more abundant keratin aggregates indicated less stable keratin filaments in PleΔalb hepatocytes. A transmission electron microscopy analysis revealed a compromised tight junction formation in plectin-deficient biliary epithelial cells. In addition, protein profiling showed increased expression of the adherens junction protein E-Cadherin, and inefficient upregulation of the desmosomal protein desmoplakin in response to BDL. In vitro analyses revealed a higher susceptibility of plectin-deficient keratin networks to stress-induced collapse, paralleled by elevated activation of p38 MAP kinase. Our study shows that by maintaining proper keratin network cytoarchitecture and biliary epithelial stability, plectin plays a critical role in protecting the liver from stress elicited by cholestasis. The biliary tree is a complex three-dimensional (3D) tubular network that drains the bile produced by hepatocytes to the small intestine. The bile is secreted into the bile canaliculi and flows through interconnected small and large bile ducts (BDs), which are lined with cuboidal biliary epithelial cells (BECs). Disorders affecting the function of BECs underlie diverse cholangiopathies (e.g. primary sclerosing cholangitis and primary biliary cirrhosis), often characterized by cholestatic condition.1 Biliary obstructions elicit a toxic response, and increased biliary pressure leads to epithelial ruptures and leakage of bile into the parenchyma. In response to a subsequent hepatocellular injury, BECs and hepatic progenitor cells (termed the oval cells in rodents) start to proliferate in a “ductular reaction”. Their expansion gives rise to an increased number of biliary ductules. Thus, by forming a significantly denser duct meshwork around the portal vein, the biliary system adapts to effectively drain the accumulated biliary fluid. 
Glecaprevir plus pibrentasvir (G/P) is a pangenotypic, once-daily, ribavirin-free direct-acting antiviral (DAA) treatment for hepatitis C virus (HCV) infection. In nine phase II or III clinical trials, G/P therapy achieved rates of sustained virologic response 12 weeks after treatment (SVR12) of 93–100% across all six major HCV genotypes (GTs). An integrated efficacy analysis of 8- and 12-week G/P therapy in patients without cirrhosis with HCV GT 1–6 infection was performed. Data were pooled from nine phase II and III trials including patients with chronic HCV GT 1–6 infection without cirrhosis who received G/P (300 mg/120 mg) for either 8 or 12 weeks. Patients were treatment naïve or treatment experienced with peginterferon, ribavirin, and/or sofosbuvir; all patients infected with HCV GT 3 were treatment naïve. Efficacy was evaluated as the SVR12 rate. The analysis included 2,041 patients without cirrhosis. In the intent-to-treat population, 943/965 patients (98%) achieved SVR12 when treated for eight weeks, and 1,060/1,076 patients (99%) achieved SVR12 when treated for 12 weeks; the difference in rates was not significant (p = 0.2). A subgroup analysis demonstrated SVR12 rates > 95% across baseline factors traditionally associated with lower efficacy. G/P was well tolerated, with one DAA-related serious adverse event (<0.1%); grade 3 laboratory abnormalities were rare. G/P therapy for eight weeks in patients with chronic HCV GT 1–6 infection without cirrhosis achieved an overall SVR12 rate of 98% irrespective of baseline patient or viral characteristics; four additional weeks of treatment did not significantly increase the SVR12 rate, demonstrating that the optimal treatment duration in this population is eight weeks. Hepatitis C virus (HCV) infects an estimated 71–80 million individuals worldwide, and is a leading cause of cirrhosis, hepatocellular carcinoma, and liver-related deaths.1 The era of direct-acting antiviral (DAA) agents for the treatment of HCV infection has resulted in a rapid and steady decline in the number of infected patients and an improvement in disease outcomes. As more patients with chronic HCV infection are being treated, it is projected that up to 75% of new patients will be HCV treatment naïve and without cirrhosis.2 
Alcohol-associated liver disease is a leading indication for liver transplantation and a leading cause of mortality. Alterations to the gut microbiota contribute to the pathogenesis of alcohol-associated liver disease. Patients with alcohol-associated liver disease have increased proportions of Candida spp. in the fecal mycobiome, yet little is known about the effect of intestinal Candida on the disease. Herein, we evaluated the contributions of Candida albicans and its exotoxin candidalysin in alcohol-associated liver disease. C. albicans and the extent of cell elongation 1 (ECE1) were analyzed in fecal samples from controls, patients with alcohol use disorder and those with alcoholic hepatitis. Mice colonized with different and genetically manipulated C. albicans strains were subjected to the chronic-plus-binge ethanol diet model. Primary hepatocytes were isolated and incubated with candidalysin. The percentages of individuals carrying ECE1 were 0%, 4.76% and 30.77% in non-alcoholic controls, patients with alcohol use disorder and patients with alcoholic hepatitis, respectively. Candidalysin exacerbates ethanol-induced liver disease and is associated with increased mortality in mice. Candidalysin enhances ethanol-induced liver disease independently of the β-glucan receptor C-type lectin domain family 7 member A (CLEC7A) on bone marrow-derived cells, and candidalysin does not alter gut barrier function. Candidalysin can damage primary hepatocytes in a dose-dependent manner in vitro and is associated with liver disease severity and mortality in patients with alcoholic hepatitis. Candidalysin is associated with the progression of ethanol-induced liver disease in preclinical models and worse clinical outcomes in patients with alcoholic hepatitis. Alcohol-associated liver disease is one of the most prevalent liver diseases worldwide,1 and the leading cause of liver transplantation in the US.2 Alcohol-related liver disease is associated with changes in the intestinal microbiota. Gut dysbiosis induces intestinal inflammation and gut barrier dysfunction, which allows viable bacteria, bacterial (such as lipopolysaccharide [LPS]) and fungal products (such as β-glucan) to translocate to the liver. Bacteria and microbial products bind to pathogen recognition receptors causing an inflammatory response driven by resident Kupffer cells and an infiltration of macrophages. Although many efforts have been made to evaluate the role of the bacterial microbiota in alcohol-associated liver disease, the interaction between fungi and their host, and especially their contribution to alcohol-associated liver disease remains poorly understood. We have shown that β-glucan, a cell wall component of many commensal fungi, binds to C-type lectin domain family 7 member A (CLEC7A; also known as DECTIN1) on hepatic macrophages to release interleukin-1β (IL-1β) and increase ethanol-induced liver disease in mice.3 
Chronic infection with hepatitis B virus (HBV) in children is a serious health problem worldwide. How to treat children with immune-tolerant chronic hepatitis B infection, commonly characterized by hepatitis B e antigen (HBeAg) positivity, high viral load, normal or mildly elevated alanine aminotransferase and no or minimal inflammation in liver histology, remains unresolved. This trial aims to study the benefits of antiviral therapy in children with these characteristics. This is a pilot open-label randomized controlled study. From May 2014 to April 2015, 69 treatment-naive chronically HBV-infected children, aged 1 to 16 years, who had immune-tolerant characteristics were recruited to this trial and randomly assigned, in a 2:1 ratio, to treatment group and control group. Patients in the treatment group received either interferon-α (IFN) monotherapy or consecutively received IFN monotherapy, combination therapy of IFN and lamivudine (LAM), and LAM therapy alone. All patients were observed until week 96. At baseline, epidemiological, biochemical, serological, virological and histological indices were consistent across the treatment and control groups. Of the 46 patients in the treatment group, 73.91% had undetectable serum HBV DNA, 32.61% achieved HBeAg seroconversion and 21.74% lost hepatitis B surface antigen (HBsAg) at the endpoint. No LAM resistance emerged at week 96. In the control group, only one (4.35%) patient underwent spontaneous HBeAg seroconversion and had undetectable serum HBV DNA during observation, and moreover, none developed HBsAg clearance. For all patients, no serious adverse events were observed. Antiviral treatment with a sequential combination of IFN and LAM resulted in a significant improvement in the rates of undetectable serum HBV DNA, HBeAg seroconversion and HBsAg loss in children with chronic HBV infection and immune-tolerant characteristics. Despite extensive vaccination programs, chronic infection with hepatitis B virus (HBV) in children remains a serious health problem worldwide.1–5 Even though a benign course of chronic HBV infection during childhood has been described, 3–5% and 0.01–0.03% of chronic carriers develop cirrhosis or hepatocellular carcinoma (HCC) before adulthood.6 Considering the whole lifetime, the risk of HCC rises to 9–24% and the incidence of cirrhosis to 2–3% per year.7 In children, chronic hepatitis B (CHB) is characterized by its natural course, including immune-tolerant phase, immune-active state and inactive carrier. Ideally, a child with CHB should be treated early to prevent the development of cirrhosis and HCC. Nevertheless, to date, no therapeutic interventions have been recommended by the guidelines for the pediatric immune-tolerant CHB, because of the scarcity of available data.6,8,9 Low therapeutic coverage exacerbated by restrictive treatment guidelines may facilitate disease progression in many patients. Thus, earlier treatment than recommended by current guidelines should be considered.10 
Recent studies suggest an association between hepatitis C virus (HCV) infection and cardiovascular damage, including carotid atherosclerosis, with a possible effect of HCV clearance on cardiovascular outcomes. We aimed to examine whether HCV eradication by direct-acting antiviral agents (DAA) improves carotid atherosclerosis in HCV-infected patients with advanced fibrosis/compensated cirrhosis. One hundred eighty-two consecutive patients with HCV and advanced fibrosis or compensated cirrhosis were evaluated. All patients underwent DAA-based antiviral therapy according to AISF/EASL guidelines. Intima-media thickness (IMT), carotid thickening (IMT ≥1 mm) and carotid plaques, defined as focal thickening of ≥1.5 mm at the level of the common carotid, were evaluated by ultrasonography (US) at baseline and 9–12 months after the end of therapy. Fifty-six percent of patients were male, mean age 63.1 ± 10.4 years, and 65.9% had compensated cirrhosis. One in five had diabetes, 14.3% were obese, 41.8% had arterial hypertension and 35.2% were smokers. At baseline, mean IMT was 0.94 ± 0.29 mm, 42.8% had IMT ≥1 mm, and 42.8% had carotid plaques. All patients achieved a 12-week sustained virological response. IMT significantly decreased from baseline to follow-up (0.94 ± 0.29 mm vs. 0.81 ± 0.27, p <0.001). Consistently, a significant reduction in the prevalence of patients with carotid thickening from baseline to follow-up was observed (42.8% vs. 17%, p <0.001), while no changes were reported for carotid plaques (42.8% vs. 47.8%, p = 0.34). These results were confirmed in subgroups of patients stratified for cardiovascular risk factors and liver disease severity. HCV eradication by DAA improves carotid atherosclerosis in patients with severe fibrosis with or without additional metabolic risk factors. The impact of this improvement in the atherosclerotic burden in terms of reduction of major cardiovascular outcomes is worth investigating in the long term. Hepatitis C virus (HCV) infection affected roughly 71.1 million of individuals in 2015 with an estimated global prevalence of 1.0% even if with major geographical heterogeneity.1 The clinical burden and the prognosis of HCV infection depends not only on the higher risk of liver-related complications and death, but also of the increase in extrahepatic complications.2 Consistent with these data, a recent meta-analysis highlighted that HCV-infected patients are at higher risk of extrahepatic manifestations related to immune dysregulation (mixed cryoglobulinemia, lymphoma, etc.) and metabolic dysfunction (type 2 diabetes, etc.) compared to subjects without infection.3 Notably, these epidemiological data are supported by experimental evidence, and by studies reporting a positive effect of HCV eradication by both interferon-based and direct-acting antiviral agents (DAA)-based therapies on HCV-related extrahepatic manifestations.2 
Georgia, with a high prevalence of hepatitis C virus (HCV) infection, launched the world’s first national hepatitis C elimination program in April 2015. A key strategy is the identification, treatment, and cure of the estimated 150,000 HCV infected persons living in the country. We report on progress and key challenges from Georgia’s experience. We constructed a care cascade by analyzing linked data from the national hepatitis C screening registry and treatment databases during 2015–2018. We assessed the impact of reflex hepatitis C core antigen (HCVcAg) testing on rates of viremia testing and treatment initiation (i.e. linkage to care). As of December 31, 2018, 1,101,530 adults (39.6% of the adult population) were screened for HCV antibody, of whom 98,430 (8.9%) tested positive, 78,484 (79.7%) received viremia testing, of these, 66,916 persons (85.3%) tested positive for active HCV infection. A total of 52,576 persons with active HCV infection initiated treatment, 48,879 completed their course of treatment. Of the 35,035 who were tested for cure (i.e., sustained virologic response [SVR]), 34,513 (98.5%) achieved SVR. Reflex HCVcAg testing, implemented in March 2018, increased rates of monthly viremia testing among persons screening positive for anti-HCV by 97.5%, however, rates of treatment initiation decreased by 60.7% among diagnosed viremic patients. Over one-third of persons living with HCV in Georgia have been detected and linked to care and treatment, however, identification and linkage to care of the remaining persons with HCV infection is challenging. Novel interventions, such as reflex testing with HCVcAg can improve rates of viremia testing, but may result in unintended consequences, such as decreased rates of treatment initiation. Linked data systems allow for regular review of the care cascade, allowing for identification of deficiencies and development of corrective actions. Hepatitis C virus RNA (HCV RNA) or HCV core antigen (HCVcAg) diagnostic testing and initiation of treatment by test method and month of diagnosis, Georgia hepatitis C elimination program, January 2015 – December 2018; the implementation of reflex viral diagnostic testing in March 2018 increased the rate of identification of viremic individuals, but did not increase the rate of infected persons initiating treatment.  
Alcoholic liver disease (ALD) is characterized by gut dysbiosis and increased gut permeability. Hypoxia inducible factor 1α (HIF-1α) has been implicated in transcriptional regulation of intestinal barrier integrity and inflammation. We aimed to test the hypothesis that HIF-1α plays a critical role in gut microbiota homeostasis and the maintenance of intestinal barrier integrity in a mouse model of ALD. Wild-type (WT) and intestinal epithelial-specific Hif1a knockout mice (IEhif1α−/−) were pair-fed modified Lieber-DeCarli liquid diet containing 5% (w/v) alcohol or isocaloric maltose dextrin for 24 days. Serum levels of alanine aminotransferase and endotoxin were determined. Fecal microbiota were assessed. Liver steatosis and injury, and intestinal barrier integrity were evaluated. Alcohol feeding increased serum levels of alanine aminotransferase and lipopolysaccharide, hepatic triglyceride concentration, and liver injury in the WT mice. These deleterious effects were exaggerated in IEhif1α−/− mice. Alcohol exposure resulted in greater reduction of the expression of intestinal epithelial tight junction proteins, claudin-1 and occludin, in IEhif1α−/− mice. In addition, cathelicidin-related antimicrobial peptide and intestinal trefoil factor were further decreased by alcohol in IEhif1α−/− mice. Metagenomic analysis showed increased gut dysbiosis and significantly decreased Firmicutes/Bacteroidetes ratio in IEhif1α−/− mice compared to the WT mice exposed to alcohol. An increased abundance of Akkermansia and a decreased level of Lactobacillus in IEhif1α−/− mice were also observed. Non-absorbable antibiotic treatment reversed the liver steatosis in both WT and IEhif1α−/− mice. Intestinal HIF-1α is essential for the adaptative response to alcohol-induced changes in intestinal microbiota and barrier function associated with elevated endotoxemia and hepatic steatosis and injury. Alcoholic liver disease (ALD) ranges from hepatic steatosis to steatohepatitis, cirrhosis, and, potentially, hepatocellular carcinoma.1 While alcohol induces deleterious effects in the liver, it also disrupts gut microbiota homeostasis and intestinal epithelial integrity resulting in increased permeability, bacterial translocation and release of bacteria-derived endotoxin into the circulation.2–4 Clinical and experimental studies have demonstrated that serum levels of lipopolysaccharide (LPS) are indeed increased in alcoholic individuals.5,6 LPS binding to Toll-like receptors on the surface of Kupffer cells leads to an elevated pro-inflammatory cytokine production, which, in turn, damages hepatocyte function.7,8 
Egypt has one of the highest burdens of HCV infection worldwide, and a large treatment programme, but reaching rural communities represents a major challenge. We report the feasibility and effectiveness of a comprehensive community-based HCV prevention, testing and treatment model in 73 villages across Egypt, with the goal to eliminate infection from all adult villagers. An HCV “educate, test and treat” programme was implemented in 73 villages across 7 governorates in Egypt between 06/2015 and 06/2018. The programme model comprised community mobilization facilitated by a network of village promoters to support the education, test and treat campaign as well as fund raising in the local community; a comprehensive testing, linkage to care and treatment of all eligible villagers aged 12 to 80 years using HCV antibody and HBsAg rapid diagnostic tests (RDTs), HCV RNA confirmation of positive cases, staging of liver disease using transient elastography (FibroScan) or FIB4 score, treatment with 12 or 24 weeks of a direct acting antiviral (DAA) regimen, and an assessment of cure at 12 weeks after completion of treatment (SVR12); and an education campaign to raise awareness and disseminate messages about safer practices to reduce transmission through public events, promotional materials and house-to-house visits. Key outcomes assessed in each village were: uptake of serological HCV and HBV testing, HCV viral load confirmation and treatment, and SVR12. 204749 (92.3%, 95% CI 91.6-93.5) of 221855 eligible villagers were screened for HCV antibody and HBsAg, and 33839 (16.5%, 95% CI 12.2-16.1) and 763 (0.4%, 95% CI 0.3-0.5) were positive, respectively. Nearly all 33839 HCV antibody positive individuals had immediate sample collected for HCV RNA and 15892 (47.0%, 95% CI 44.8-53.8) were positive. Overall, prevalence of HCV viremia was 7.8%, 95% CI 6.6-7.9. 14495 (91.2%, 95% CI 89.9-96.4) received treatment within a median of 2.1 weeks from serological diagnosis (IQR: 0.6 to 3.3 weeks). SVR12) was achieved among 14238 of the treated cases (98.3%, 95% CI 96.7-98.6). 3192 (20.1%) had cirrhosis, and of these 166 (5.2%) were diagnosed with HCC. Chronic hepatitis C infection is a major public health problem with an estimated 71 million people chronically infected with hepatitis C worldwide [1]. The global response has been transformed by the availability of low-cost, curative, short course direct acting antiviral (DAA) therapy. The World Health Organization (WHO) has a stated goal to eliminate viral hepatitis C (and B) virus infection as a public health threat by 2030 – defined as a reduction in mortality by 65% and new infections by 90% [1,2]. Achievement of these impact targets is feasible through a combination of substantial scale-up of access to affordable testing and treatment with the highly effective new direct-acting antiviral drug regimens alongside HCV preventative interventions such as harm reduction in key populations and reduction in unnecessary and unsafe injection practices [3]. While there has been some encouraging progress in global treatment scale-up with more than 3 million treated with direct acting antivirals since 2015, access to and uptake of testing remains poor, and less than 10% of those with chronic HCV infection in low-resource settings (LRS) have been diagnosed [1]. 
Around 10–20% of patients with non-alcoholic fatty liver disease (NAFLD) are non-obese. The benefit of weight reduction in such patients is unclear. We aim to study the efficacy of lifestyle intervention in non-obese patients with NAFLD and to identify factors that predict treatment response. A total of 154 community NAFLD patients were randomised to a 12-month lifestyle intervention programme involving regular exercise, or to standard care. The primary outcome was remission of NAFLD at Month 12 by proton-magnetic resonance spectroscopy. After the programme, the patients were prospectively followed until Year 6. The Asian body mass index (BMI) cut-off of 25 kg/m2 was used to define non-obese NAFLD. Patients were assigned to the intervention (n = 77) and control (n = 77) groups (39 and 38 in each group had baseline BMI <25 and ≥25 kg/m2, respectively). More patients in the intervention group achieved the primary outcome than the control group regardless of baseline BMI (non-obese: 67% vs. 18%, p <0.001; obese: 61% vs. 21%, p <0.001). Lifestyle intervention, lower baseline intrahepatic triglyceride, and reduction in body weight and waist circumference were independent factors associated with remission of NAFLD in non-obese patients. Half of non-obese patients achieved remission of NAFLD with 3–5% weight reduction; the same could only be achieved in obese patients with 7–10% weight reduction. By Year 6, non-obese patients in the intervention group remained more likely to maintain weight reduction and alanine aminotransferase normalisation than the control group. Lifestyle intervention is effective in treating NAFLD in both non-obese and obese patients. Weight reduction predicts remission of NAFLD in non-obese patients, but a modest weight reduction may be sufficient in this population. Non-alcoholic fatty liver disease (NAFLD) is currently the most common chronic liver disease and is one of the leading causes of end-stage liver disease and hepatocellular carcinoma worldwide.1,2 Although NAFLD is strongly associated with metabolic syndrome and obesity,3 around 10–20% of patients with NAFLD have a relatively normal body mass index (BMI), a condition often described as non-obese or lean NAFLD.4 Studies based on liver histology or non-invasive tests of fibrosis suggest that these non-obese patients may also harbour non-alcoholic steatohepatitis (NASH) and advanced fibrosis.5–8 
Sofosbuvir/velpatasivr/voxilaprevir (SOF/VEL/VOX) is approved for retreatment of patients with HCV and a previous failure on direct-acting antivirals (DAAs), however real-life data are limited. The aim of this study was to assess the effectiveness and safety of SOF/VEL/VOX in a real-life setting. All consecutive patients with HCV receiving SOF/VEL/VOX between May-October 2018 in 27 centers in Northern Italy were enrolled. Bridging fibrosis (F3) and cirrhosis (F4) were diagnosed by liver stiffness measurement: >10 and >13 kPa respectively. Sustained virological response (SVR) was defined as undetectable HCV-RNA 4 (SVR4) or 12 (SVR12) weeks after the end-of-treatment. A total of 179 patients were included: median age 57 (18–88) years, 74% males, median HCV-RNA 1,081,817 (482–25,590,000) IU/ml. Fibrosis stage was F0-F2 in 32%, F3 in 21%, F4 in 44%. HCV genotype was 1 in 58% (1b 33%, 1a 24%, 1nc 1%), 2 in 10%, 3 in 23% and 4 in 9%; 82% of patients carried resistance-associated substitutions in the NS3, NS5A or NS5B regions. Patients received SOF/VEL/VOX for 12 weeks, ribavirin was added in 22% of treatment schedules. Undetectable HCV-RNA was achieved by 74% of patients at week 4 and by 99% at week 12. Overall, 162/179 (91%) patients by intention to treat analysis and 162/169 (96%) by per protocol analysis achieved SVR12, respectively; treatment failures included 6 relapsers and 1 virological non-responder. Cirrhosis (p = 0.005) and hepatocellular carcinoma (p = 0.02) were the only predictors of treatment failure. Most frequent adverse events included fatigue (6%), hyperbilirubinemia (6%) and anemia (4%). SOF/VEL/VOX is an effective and safe retreatment for patients with HCV who have failed on a previous DAA course in a real-life setting. The development of direct-acting antivirals (DAA) for the treatment of HCV has dramatically increased rates of sustained virological response (SVR) to antiviral therapy, reaching more than 95–98% in all patient groups.1 Despite optimal efficacy and safety, approximately 2% of patients still fail to achieve an SVR and need alternative treatment options.1 Retreatment of patients with DAA failure could be challenging, as emergence of resistance-associated substitutions (RASs) in HCV non-structural (NS) regions can negatively impact on the chances of SVR.2,3 Indeed, RASs have been shown to confer cross-resistance among DAAs of the same class.4,5 Moreover, due to preserved viral fitness, RASs in the NS5A region have been shown to persist long-term, being detectable even many years after treatment completion.5 Recently, the approval of the protease inhibitor voxilaprevir (VOX) combined with the NS5B inhibitor sofosbuvir (SOF) and the NS5A inhibitor velpatasvir (VEL) has provided a potent retreatment option targeting all steps of HCV replication. The SOF/VEL/VOX combination has been evaluated in more than 800 patients enrolled in phase II and phase III studies, where it demonstrated excellent safety and efficacy, achieving overall SVR rates of more than 95%.6–10 The POLARIS-1 and 4 phase III trials also included a significant proportion of patients with cirrhosis (46%) and/or carrying multiple RASs (49–83%). Despite this difficult-to-treat population, the POLARIS trials achieved 96% and 97% SVR rates, and led to approval of SOF/VEL/VOX, which is currently recommended as the first-line retreatment option for compensated HCV patients by EASL and AASLD recommendations.1,11 More recently, the first real-life experiences with SOF/VEL/VOX have been published in US Veterans and in Spanish patients, reporting dismal effectiveness in patients who previously failed on SOF/VEL and in those with HCV genotype 3 infection.12,13 However, data about SOF/VEL/VOX in clinical practice are still limited. Therefore, the aim of this study was to assess the effectiveness and safety of SOF/VEL/VOX in a large real-life patient cohort in Northern Italy. SECTION Materials and methods SECTION Patient population 
The macrophage scavenger receptor 1 (Msr1, also called SRA) is a pattern recognition receptor primarily expressed on myeloid cells, which plays an important role in the maintenance of immune homeostasis. Since MSR1 expression was upregulated in the livers of patients with fulminant hepatitis (FH), we investigated the functional mechanism of Msr1 in FH pathogenesis. Msr1-deficient (Msr1−/−) mice and their wild-type (WT) littermates were infected with mouse hepatitis virus strain-A59 (MHV-A59) to induce FH, and the levels of tissue damage, serum alanine aminotransferase, inflammatory cytokines and complement component 5a (C5a) were measured and compared. Liver injury was studied after MHV infection with or without neutrophil depletion. Our results showed that Msr1−/− mice were resistant to MHV-induced hepatitis. Treatment with the C5a receptor antagonist (C5aRa) diminished the differences in inflammatory responses and liver injury between MHV-infected wild-type and Msr1−/− mice, suggesting that C5a-induced pro-inflammatory response plays a critical role in the Msr1-mediated regulation of FH pathogenesis. We demonstrated that Msr1 efficiently enhanced transforming growth factor-activated kinase-1 phosphorylation in neutrophils upon MHV-A59 stimulation, thereby promoting the activation of the extracellular signal-regulated kinase pathway and subsequent NETosis formation. Moreover, we provided evidence that blockage of Msr1 attenuated the liver damage caused by MHV-A59 infection. Msr1 promotes the pathogenesis of virus-induced FH by enhancing induction of neutrophil NETosis and subsequent complement activation. Targeting Msr1 may be employed as a new immunotherapeutic strategy for FH. Fulminant hepatitis (FH) is a rare but potentially fatal disease caused by viral infection or inflammatory destruction of liver tissue. It is characterized by rapid deterioration of hepatic functions, massive hepatocyte necrosis, and hepatic encephalopathy.1 Despite the recent therapeutic advances, FH is still associated with significant mortality. Mouse hepatitis virus (MHV) is a positive-strand RNA virus that causes a variety of diseases, including hepatitis, enteritis, and encephalitis in mice.2 Noteworthy, the MHV-induced disease profiles are dependent on the viral strain and infection route, as well as genetic background and immune status of the mice.2,3 Intraperitoneal infection of susceptible strains of mice (i.e. BALB/cJ, C57BL/6) with hepatotropic MHV, such as MHV-3 and MHV-A59, results in sinusoidal thrombosis and hepatocellular necrosis, and thus, has served as a useful model for FH in humans.4,5 
In non-alcoholic steatohepatitis (NASH), the function of urea cycle enzymes (UCEs) may be affected, resulting in hyperammonemia and the risk of disease progression. We aimed to determine whether the expression and function of UCEs are altered in an animal model of NASH and in patients with non-alcoholic fatty liver disease (NAFLD), and whether this process is reversible. Rats were first fed a high-fat, high-cholesterol diet for 10 months to induce NASH, before being switched onto a normal chow diet to recover. In humans, we obtained liver biopsies from 20 patients with steatosis and 15 with NASH. Primary rat hepatocytes were isolated and cultured with free fatty acids. We measured the gene and protein expression of ornithine transcarbamylase (OTC) and carbamoylphosphate synthetase (CPS1), as well as OTC activity, and ammonia concentrations. Moreover, we assessed the promoter methylation status of OTC and CPS1 in rats, humans and steatotic hepatocytes. In NASH animals, gene and protein expression of OTC and CPS1, and the activity of OTC, were reversibly reduced. Hypermethylation of Otc promoter genes was also observed. Additionally, in patients with NAFLD, OTC enzyme concentration and activity were reduced and ammonia concentrations were increased, which was further exacerbated in those with NASH. Furthermore, OTC and CPS1 promoter regions were hypermethylated. In primary hepatocytes, induction of steatosis was associated with Otc promoter hypermethylation, a reduction in the gene expression of Otc and Cps1, and an increase in ammonia concentration in the supernatant. NASH is associated with a reduction in the gene and protein expression, and activity, of UCEs. This results in hyperammonemia, possibly through hypermethylation of UCE genes and impairment of urea synthesis. Our investigations are the first to describe a link between NASH, the function of UCEs, and hyperammonemia, providing a novel therapeutic target. Alarming increases in the rates of obesity and non-alcoholic fatty liver disease (NAFLD) in an ageing population, have made liver disease a major public health concern for the next decade.1,2 NAFLD is a spectrum of liver diseases ranging from steatosis, through non-alcoholic steatohepatitis (NASH) to cirrhosis.3 Recently, NASH has been defined as a “multiple parallel hits” disease4 that can progress to liver fibrosis, which is the principal factor contributing to NASH-associated morbidity and mortality.5,6 The main cell type responsible for extracellular matrix deposition is hepatic stellate cells (HSCs), which undergo activation in conditions of frank hepatocellular injury, enabling them to participate in the wound healing process7 and making them key in the development of fibrosis. 
Donation after circulatory death (DCD) liver transplantation is known for potentially worse outcomes because of higher rates of graft non-function or irreversible cholangiopathy. The impact of machine liver perfusion techniques on these complications remains elusive. We aimed to provide data on 5-year outcomes in patients receiving DCD liver transplants, after donor organs had been treated by hypothermic oxygenated perfusion (HOPE). Fifty HOPE-treated DCD liver transplants performed in Zurich between 2012 and 3/2017 were matched with 50 primary donation after brain death (DBD) liver transplants, and with 50 untreated DCD liver transplants in Birmingham. Match factors focussed on short cold ischaemia, comparable recipient age and low recipient laboratory model for end-stage liver disease scores. Primary endpoints were post-transplant complications, and non-tumour-related patient death or graft loss. Despite extended donor warm ischaemia, HOPE-treated DCD liver transplants achieved similar overall graft survival, compared to standard DBD liver transplants. Particularly, graft loss due to any non-tumour-related causes occurred in 8% (4/50) of cases. In contrast, untreated DCD livers resulted in non-tumour-related graft failure in one-third (16/50) of cases (p = 0.005), despite significantly (p <0.001) shorter functional donor warm ischaemia. Five-year graft survival, censored for tumour death, was 94% for HOPE-treated DCD liver transplants vs. 78% in untreated DCD liver transplants (p = 0.024). The 5-year outcomes of HOPE-treated DCD liver transplants were similar to those of DBD primary transplants and superior to those of untreated DCD liver transplants, despite much higher risk. These results suggest that a simple end-ischaemic perfusion approach is very effective and may open the field for safe utilisation of extended DCD liver grafts. Donation after circulatory death (DCD) organs are increasingly used for liver transplantation, due to the persisting organ shortage and waiting list mortality.1 However, several reports suggest inferior graft survival, increased risk of primary non-function (PNF), and biliary complications in DCD livers, with irreversible ischaemic cholangiopathy (IC) being a major concern.2 Severe forms, requiring retransplantation, typically develop within the first 3–6 months after liver transplant.3 While the majority of transplant physicians agree that prolonged periods of donor warm ischaemia contribute significantly to this aggressive biliary complication, others argue that other factors including donor and recipient age, cold ischaemia, donor body mass index (BMI) and hepatic steatosis, or technical issues are equally important.4,5 Therefore, various dynamic preservation techniques designed to optimise liver grafts before implantation are currently under evaluation.6–9 In 2012, a novel machine perfusion concept was introduced in Zurich for DCD liver transplantation, hypothermic oxygenated perfusion (HOPE), applied only for 1–2 h after conventional procurement and cold storage.10,11 While the initial clinical experience with this new technique has already been presented, including the first 25 human DCD livers,9 our study aimed to document a longer follow-up of 5 years after HOPE treatment in human DCD livers. Secondly, we intended to unravel the efficacy of the HOPE perfusion approach. Therefore, we compared HOPE-perfused DCD livers with the best available not machine perfused alternative e.g. untreated DCD livers from a highly experienced transplant unit, in cases where DCD livers were exposed to short cold ischaemia and recipients had low model for end-stage liver disease (MELD) scores. Thirdly, this analysis focussed on cumulative post-transplant complications within the first year of post-transplant follow-up, quantified by the comprehensive complications index.12 Finally, we compared non-tumour-related death or graft loss in HOPE-treated or untreated DCD livers with outcomes in primary donation after brain death (DBD) liver transplants. SECTION Patients and methods SECTION Patient cohort and data collection 
The Wnt/β-catenin pathway is the most frequently deregulated pathway in hepatocellular carcinoma (HCC). Inactivating mutations of the gene encoding AXIN1, a known negative regulator of the Wnt/β-catenin signaling pathway, are observed in about 10% of HCCs. Whole-genome studies usually place HCC with AXIN1 mutations and CTNNB1 mutations in the group of tumors with Wnt/β-catenin activated program. However, it has been shown that HCCs with activating CTNNB1 mutations form a group of HCCs, with a different histology, prognosis and genomic signature to those with inactivating biallelic AXIN1 mutations. We aimed to elucidate the relationship between CTNNB1 mutations, AXIN1 mutations and the activation level of the Wnt/β-catenin program. We evaluated two independent human HCC datasets for the expression of a 23-β-catenin target genes program. We modeled Axin1 loss of function tumorigenesis in two engineered mouse models and performed gene expression profiling. Based on gene expression, we defined three levels of β-catenin program activation: strong, weak or no activation. While more than 80% CTNNB1-mutated tumors were found in the strong or in the weak activation program, most of the AXIN1-mutated tumors (>70%) were found in the subgroup with no activation. We validated this result by demonstrating that mice with a hepatocyte specific AXIN1 deletion developed HCC in the absence of β-catenin induction. We defined a 329-gene signature common in human and mouse AXIN1 mutated HCC that is highly enriched in Notch and YAP oncogenic signatures. AXIN1-mutated HCCs occur independently of the Wnt/β-catenin pathway and involve Notch and YAP pathways. These pathways constitute potentially interesting targets for the treatment of HCC caused by AXIN1 mutations. Hepatocellular carcinoma (HCC) is the third most frequent cause of cancer death worldwide. HCC is a highly heterogeneous disease, occurring in a context of chronic liver injury and inflammation leading to cirrhosis.1 Recent genomic studies have provided an accurate description of the landscape of genetic changes underlying HCC and identified the molecular pathways most frequently altered, among which the Wnt/β-catenin pathway is prominent.2–6 This conserved signaling pathway governs embryonic development, homeostasis and liver zonation in adults. In the absence of Wnt stimulation, the cytosolic concentration of β-catenin remains low because of a multiprotein destruction complex including CK1α, GSK3β, YAP/TAZ, APC and AXIN1, which promotes the phosphorylation of β-catenin. Once phosphorylated, β-catenin is degraded by the proteasome. In response to Wnt stimulation, the destruction complex is recruited to the membrane. This stabilizes β-catenin, which then enters the nucleus and activates the expression of Wnt target genes, mostly through the lymphoid enhancer-binding factor 1 (LEF-1) and T-cell transcription factor (TCF). 
Previous studies demonstrated that autophagy is protective in hepatocytes and macrophages, but detrimental in hepatic stellate cells in chronic liver diseases. The role of autophagy in liver sinusoidal endothelial cells (LSECs) in nonalcoholic steatohepatitis (NASH) is unknown. Our aim was to analyze the potential implication of autophagy in LSECs in NASH and liver fibrosis. We analyzed autophagy in LSECs from patients using transmission electron microscopy. We determined the consequences of a deficiency in autophagy: (a) on LSECs phenotype, using primary LSECs and an LSECs line; (b) on early stages of NASH and on advanced stages of liver fibrosis, using transgenic mice deficient in autophagy specifically in endothelial cells and fed a high-fat diet or chronically treated with carbon tetrachloride, respectively. Patients with NASH had twice less LSECs containing autophagic vacuoles than patient without liver histological abnormalities, or with simple steatosis. LSECs from mice deficient in endothelial autophagy displayed an up-regulation of genes implicated in inflammatory pathways. In LSEC line, deficiency in autophagy enhanced inflammation (CCL2, CCL5, IL-6 and VCAM-1 expression), features of endothelial-to-mesenchymal transition (α-SMA, Tgf-β1, Collagen-1α2 expression) and apoptosis (cleaved Caspase 3). In mice fed a high-fat diet, deficiency in endothelial autophagy induced liver expression of inflammatory markers (Ccl2, Ccl5, Cd68, VCAM-1), liver cell apoptosis (cleaved Caspase-3) and perisinusoidal fibrosis. Mice deficient in endothelial autophagy treated with carbon tetrachloride also developed more perisinusoidal fibrosis. A defect in autophagy in LSECs occurs in patients with NASH. Deficiency in endothelial autophagy promotes the development of liver inflammation, features suggesting endothelial-to-mesenchymal transition, apoptosis and liver fibrosis at early stages of NASH, but also favors more advanced stages of liver fibrosis.  
Understanding the real-world effectiveness of all-oral hepatitis C virus (HCV) regimens informs treatment decisions. We evaluated the effectiveness of daclatasvir + sofosbuvir ± ribavirin (DCV + SOF ± RBV) and velpatasvir/sofosbuvir (VEL/SOF) ± RBV in patients with genotype 2 and genotype 3 infection treated in routine practice. This observational analysis was carried out in an intent-to-treat cohort of patients with HCV genotype 2 and genotype 3. Sustained virologic response (SVR) analysis was performed in 5,400 patients initiated on DCV + SOF ± RBV or VEL/SOF ± RBV at any Department of Veterans Affairs facility. For genotype 2, SVR rates did not differ between DCV + SOF (94.5%) and VEL/SOF (94.4%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (89.5%). For genotype 3, SVR rates did not differ between DCV + SOF (90.8%) and VEL/SOF (92.0%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (86.4%). In multivariate models of patients with genotype 2 and 3 infection, the treatment regimen was not a significant predictor of the odds of SVR. For genotype 3, significant predictors of reduced odds of SVR were prior HCV treatment-experience (odds ratio [OR] 0.51, 95% CI 0.36–0.72; p <0.001), FIB-4 >3.25 (OR 0.60; 95% CI 0.43–0.84; p = 0.002) and a history of decompensated liver disease (OR 0.68; 95% CI 0.47–0.98; p = 0.04). For patients with genotype 2 and 3, treated with VEL/SOF ± RBV, 89% and 85% received 12-weeks of treatment, respectively. For DCV + SOF ± RBV, 56% and 20% of patients with HCV genotype 2 received 12-weeks and 24-weeks of treatment, respectively; while 53% and 23% of patients with HCV genotype 3 received 12-weeks and 24-weeks, with most direct-acting antiviral experienced patients receiving 24-weeks. In patients infected with HCV genotype 2 and 3, DCV + SOF ± RBV and VEL/SOF ± RBV produced similar SVR rates within each genotype, and the regimen did not have a significant impact on the odds of SVR. For patients with genotype 3, prior treatment-experience and advanced liver disease were significant predictors of reduced odds of SVR regardless of regimen. Approximately 71.1 million people are estimated to be chronically infected with hepatitis C virus (HCV) worldwide, with 2.9 million infected in the United States.1 Genotype 2 accounts for 11% of chronic HCV infections both worldwide and in the United States, while genotype 3 accounts for 18% of HCV infections worldwide and 9% in the United States.1,2 
Antibiotic resistance has been increasingly reported in patients with decompensated cirrhosis in single-center studies. Prospective investigations reporting broad epidemiological data are scarce. We aimed to analyze epidemiological changes in bacterial infections in patients with decompensated cirrhosis. This was a prospective evaluation of 2 series of patients hospitalized with decompensated cirrhosis. The Canonic series included 1,146 patients from Northern, Southern and Western Europe in 2011. Data on epidemiology, clinical characteristics of bacterial infections, microbiology and empirical antibiotic schedules were assessed. A second series of 883 patients from Eastern, Southern and Western Europe was investigated between 2017–2018. A total of 455 patients developed 520 infections (39.7%) in the first series, with spontaneous bacterial peritonitis, urinary tract infections and pneumonia the most frequent infections. Nosocomial episodes predominated in this series. Nearly half of the infections were culture-positive, of which 29.2% were caused by multidrug-resistant organisms (MDROs). MDR strains were more frequently isolated in Northern and Western Europe. Extended-spectrum beta-lactamase-producing Enterobacteriaceae were the most frequent MDROs isolated in this series, although prevalence and type differed markedly among countries and centers. Antibiotic resistance was associated with poor prognosis and failure of antibiotic strategies, based on third-generation cephalosporins or quinolones. Nosocomial infection (odds ratio [OR] 2.74; p < 0.001), intensive care unit admission (OR 2.09; p = 0.02), and recent hospitalization (OR 1.93; p = 0.04) were identified as independent predictors of MDR infection. The prevalence of MDROs in the second series (392 infections/284 patients) was 23%; 38% in culture-positive infections. A mild increase in the rate of carbapenem-resistant Enterobacteriaceae was observed in this series. MDR bacterial infections constitute a prevalent, growing and complex healthcare problem in patients with decompensated cirrhosis and acute-on-chronic liver failure across Europe, negatively impacting on prognosis. Strategies aimed at preventing the spread of antibiotic resistance in cirrhosis should be urgently evaluated. Bacterial infections constitute a frequent complication in patients with decompensated cirrhosis and are the most frequent trigger of acute-on-chronic liver failure (ACLF) in Western countries.1–5 Patients with cirrhosis and acute decompensation (AD) are prone to developing spontaneous and secondary bacterial infections, a risk that is magnified in patients with ACLF.1,5,6 Bacterial infection has a critical relevance in the clinical course of decompensated cirrhosis, increasing the rate of short-term mortality by 2–4 fold.7,8 Recent data also show that bacterial infections are severe and associated with intense systemic inflammation, poor clinical course and high mortality in patients with ACLF.6 
Patients with acute-on-chronic liver failure (ACLF) can be listed for liver transplantation (LT) because LT is the only curative treatment option. We evaluated whether the clinical course of ACLF, particularly ACLF-3, between the time of listing and LT affects 1-year post-transplant survival. We identified patients from the United Network for Organ Sharing database who were transplanted within 28 days of listing and categorized them by ACLF grade at waitlist registration and LT, according to the EASL-CLIF definition. A total of 3,636 patients listed with ACLF-3 underwent LT within 28 days. Among those transplanted, 892 (24.5%) recovered to no ACLF or ACLF grade 1 or 2 (ACLF 0–2) and 2,744 (75.5%) had ACLF-3 at transplantation. One-year survival was 82.0% among those transplanted with ACLF-3 vs. 88.2% among those improving to ACLF 0–2 (p <0.001). Conversely, the survival of patients listed with ACLF 0–2 who progressed to ACLF-3 at LT (n = 2,265) was significantly lower than that of recipients who remained at ACLF 0–2 (n = 17,631) at the time of LT (83.8% vs. 90.2%, p <0.001). Cox modeling demonstrated that recovery from ACLF-3 to ACLF 0–2 at LT was associated with reduced 1-year mortality after transplantation (hazard ratio 0.65; 95% CI 0.53–0.78). Improvement in circulatory failure, brain failure, and removal from mechanical ventilation were also associated with reduced post-LT mortality. Among patients >60 years of age, 1-year survival was significantly higher among those who improved from ACLF-3 to ACLF 0–2 than among those who did not. Improvement from ACLF-3 at listing to ACLF 0–2 at transplantation enhances post-LT survival, particularly in those who recovered from circulatory or brain failure, or were removed from the mechanical ventilator. The beneficial effect of improved ACLF on post-LT survival was also observed among patients >60 years of age. Acute-on-chronic liver failure (ACLF) is associated with severe systemic inflammation and is characterized by acute hepatic decompensation, development of organ failures, and high 28-day mortality.1–3 The short-term mortality of patients with ACLF grade 3 (ACLF-3), defined as the development of 3 or more organ failures,1 is particularly high, approaching 80% at 28-days4–6 and possibly surpassing that of acute liver failure.7 In certain patients with ACLF-3, liver transplantation (LT) may be the only viable treatment. However, data regarding LT for individuals with ACLF-3 indicate a reduced survival probability, ranging from less than 50%8,9 to 80% at 1 year.10,11 Although this suggests a greater likelihood of survival than supportive care without transplantation, the limited availability of donor organs necessitates judicious selection of transplant recipients. 
A comprehensive analysis of changes in symptoms and functioning during and after direct-acting antiviral (DAA) therapy for chronic hepatitis C virus (HCV) infection has not been conducted for patients treated in real-world clinical settings. Therefore, we evaluated patient-reported outcomes (PROs) in a diverse cohort of patients with HCV treated with commonly prescribed DAAs. PROP UP is a US multicenter observational study of 1,601 patients with HCV treated with DAAs in 2016-2017. PRO data were collected at baseline (T1), early on-treatment (T2), late on-treatment (T3) and 3-months post-treatment (T4). PRO mean change scores were calculated from baseline and a minimally important change (MIC) threshold was set at 5%. Regression analyses investigated patient and treatment characteristics independently associated with PRO changes on-treatment and post-treatment. Of 1,564 patients, 55% were male, 39% non-white, 47% had cirrhosis. Sofosbuvir/ledipasvir was prescribed to 63%, sofosbuvir/velpatasvir to 21%, grazoprevir/elbasvir to 11%, and paritaprevir/ombitasvir/ritonavir + dasabuvir to 5%. During DAA therapy, mean PRO scores improved slightly in the overall cohort, but did not reach the 5% MIC threshold. Between 21–53% of patients experienced >5% improved PROs while 23–36% experienced >5% worse symptoms. Of 1,410 patients with evaluable sustained virologic response (SVR) data, 95% achieved SVR. Among those with SVR, all mean PRO scores improved, with the 5% MIC threshold met for fatigue, sleep disturbance, and functioning well-being. Regression analyses identified subgroups, defined by age 35–55, baseline mental health issues and a higher number of health comorbidities as predictors of PRO improvements. In real-world clinical practices, we observed heterogeneous patient experiences during and after DAA treatment. Symptom improvements were more pronounced in younger patients, those with baseline mental health issues and multiple comorbidities. Patients with chronic hepatitis C virus (HCV) infection often report neuropsychiatric, somatic, and gastrointestinal symptoms including fatigue, sleep disturbance, musculoskeletal pain, depression, and abdominal pain.1–3 Patients may attribute these symptoms to HCV, a chronic viral infection associated with several extrahepatic disorders. Recent studies show that health-related quality of life and other patient-reported outcomes (PROs) improve during all-oral direct-acting antiviral (DAA) therapy and after patients achieve a sustained virologic response (SVR).4–6 These studies were based exclusively on data derived from industry-sponsored registration trials. It remains critical to determine if these findings can be generalized to patients treated in real-world clinical practices given inherent biases of registration trial data.7,8 
Surgery in cirrhosis is associated with a high morbidity and mortality. Retrospectively reported prognostic factors include emergency procedures, liver function (MELD/Child-Pugh scores) and portal hypertension (assessed by indirect markers). This study assessed the prognostic role of hepatic venous pressure gradient (HVPG) and other variables in elective extrahepatic surgery in patients with cirrhosis. A total of 140 patients with cirrhosis (Child-Pugh A/B/C: 59/37/4%), who were due to have elective extrahepatic surgery (121 abdominal; 9 cardiovascular/thoracic; 10 orthopedic and others), were prospectively included in 4 centers (2002–2011). Hepatic and systemic hemodynamics (HVPG, indocyanine green clearance, pulmonary artery catheterization) were assessed prior to surgery, and clinical and laboratory data were collected. Patients were followed-up for 1 year and mortality, transplantation, morbidity and post-surgical decompensation were studied. Ninety-day and 1-year mortality rates were 8% and 17%, respectively. Variables independently associated with 1-year mortality were ASA class (American Society of Anesthesiologists), high-risk surgery (defined as open abdominal and cardiovascular/thoracic) and HVPG. These variables closely predicted 90-, 180- and 365-day mortality (C-statistic >0.8). HVPG values >16 mmHg were independently associated with mortality and values ≥20 mmHg identified a subgroup at very high risk of death (44%). Twenty-four patients presented persistent or de novo decompensation at 3 months. Low body mass index, Child-Pugh class and high-risk surgery were associated with death or decompensation. No patient with HVPG <10 mmHg or indocyanine green clearance >0.63 developed decompensation. ASA class, HVPG and high-risk surgery were prognostic factors of 1-year mortality in cirrhotic patients undergoing elective extrahepatic surgery. HVPG values >16 mmHg, especially ≥20 mmHg, were associated with a high risk of post-surgical mortality. Cirrhosis is a life-threatening condition and a major cause of morbidity and mortality worldwide. Improvements in the management of its related complications, of its etiologies (i.e. viral eradication), and the option of liver transplantation have increased life expectancy of patients with cirrhosis. In this setting, it is not unusual that major surgical procedures are proposed for patients with cirrhosis to address orthopedic, malignancy or cirrhosis related complications. In fact, patients with cirrhosis have a high incidence of gallstones and abdominal wall hernias that require surgical repair.1–4 Surgery in cirrhosis has always been associated with high perioperative morbidity (about 30%, including infections, renal failure, decompensation, blood transfusion, re-intervention, etc.) and mortality, ranging from 10 to 30% in the most recent series.5–11 The main factors associated with these poor outcomes have been related to liver function (Child-Pugh or model for end-stage liver disease [MELD] scores), to the type of surgery (higher risk in open abdominal, cardiovascular and thoracic surgeries), and to the presence of signs or symptoms of portal hypertension (PHT).5,8,10–14 However, there are no universally accepted prospective scores to assess surgical risk for patients with cirrhosis. The most widely accepted score is probably that from the Mayo Clinic, based on MELD, ASA class and age.11,15 Although it was developed in a very large cohort, this model combines emergency and elective surgery, combining different profiles of patients that may act as confounding factors (for example, MELD score is usually higher in emergency surgery patients). The major weakness of prognostic studies of surgery in cirrhosis is their retrospective nature and the lack of prospective validation studies. 
Hepatitis C virus (HCV)-specific CD8+ T cells are functionally impaired in chronic hepatitis C. Even though HCV can now be rapidly and sustainably cleared from chronically infected patients, the repercussions of HCV clearance on virus-specific CD8+ T cells remain elusive. Here, we aimed to investigate if HCV clearance by direct-acting antivirals (DAAs) could restore the functionality of exhausted HCV-specific CD8+ T cell responses. HCV-specific CD8+ T cells in peripheral blood were obtained from 40 patients with chronic HCV infection, during and 6 months following IFN-free DAA therapy. These cells were analyzed for comprehensive phenotypes, proliferation, cytokine production, mitochondrial fitness and response to immune-checkpoint blockade. We show that, unlike activation markers that decreased, surface expression of multiple co-regulatory receptors on exhausted HCV-specific CD8+ T cells remained unaltered after clearance of HCV. Likewise, cytokine production by HCV-specific CD8+ T cells remained impaired following HCV clearance. The proliferative capacity of HCV multimer-specific CD8+ T cells was not restored in the majority of patients. Enhanced in vitro proliferative expansion of HCV-specific CD8+ T cells during HCV clearance was more likely in women, patients with low liver stiffness and low alanine aminotransferase levels in our cohort. Interestingly, HCV-specific CD8+ T cells that did not proliferate following HCV clearance could preferentially re-invigorate their proliferative capacity upon in vitro immune-checkpoint inhibition. Moreover, altered mitochondrial dysfunction exhibited by exhausted HCV-specific CD8+ T cells could not be normalized after HCV clearance. Taken together, our data implies that exhausted HCV-specific CD8+ T cells remain functionally and metabolically impaired at multiple levels following HCV clearance in most patients with chronic hepatitis C. Our results might have implications in cases of re-infection with HCV and for HCV vaccine development. Hepatitis C virus (HCV) persists and leads to chronicity in the majority of infected patients. Chronic hepatitis C is mainly characterized by functional impairment of virus-specific CD8+ T cells.1,2 Several mechanisms have been suggested to lead to functional impairment of HCV-specific CD8+ T cells. However, the phenomena that have gained most attention for their contribution to failure of virus-specific CD8+ T cell responses are viral escape variants and CD8+ T cell exhaustion.3–5 Persistent antigen stimulation during chronic HCV infection is suggested to lead to exhaustion of virus-specific CD8+ T cells. Exhausted virus-specific CD8+ T cells in-turn are characterized by the expression of multiple co-regulatory molecules,3 limited proliferative capacity,6 impaired cytokine production3,6 as well as impaired metabolism.7–9 
Eight-week glecaprevir/pibrentasvir leads to high rates of sustained virological response at post-treatment week 12 (SVR12) across HCV genotypes (GT) 1–6 in treatment-naïve patients without cirrhosis. We evaluated glecaprevir/pibrentasvir once daily for 8 weeks in treatment-naïve patients with compensated cirrhosis. EXPEDITION-8 was a single-arm, multicenter, phase IIIb trial. The primary and key secondary efficacy analyses were to compare the lower bound of the 95% CI of the SVR12 rate in i) patients with GT1,2,4–6 in the per protocol (PP) population, ii) patients with GT1,2,4–6 in the intention-to-treat (ITT) population, iii) patients with GT1–6 in the PP population, and iv) patients with GT1–6 in the ITT population, to pre-defined efficacy thresholds based on historical SVR12 rates for 12 weeks of glecaprevir/pibrentasvir in the same populations. Safety was also assessed. A total of 343 patients were enrolled. Most patients were male (63%), white (83%), and had GT1 (67%). The SVR12 rate in patients with GT1–6 was 99.7% (n/N = 334/335; 95% CI 98.3–99.9) in the PP population and 97.7% (n/N = 335/343; 95% CI 96.1–99.3) in the ITT population. All primary and key secondary efficacy analyses were achieved. One patient (GT3a) experienced relapse (0.3%) at post-treatment week 4. Common adverse events (≥5%) were fatigue (9%), pruritus (8%), headache (8%), and nausea (6%). Serious adverse events (none related) occurred in 2% of patients. No adverse event led to study drug discontinuation. Clinically significant laboratory abnormalities were infrequent. Eight-week glecaprevir/pibrentasvir was well tolerated and led to a similarly high SVR12 rate as the 12-week regimen in treatment-naïve patients with chronic HCV GT1–6 infection and compensated cirrhosis. Trial registration: ClinicalTrials.gov, NCT03089944. Chronic HCV infection is a major public health threat, with an estimated 71 million individuals affected worldwide.1 In 2015, it was estimated that only 20% of these individuals were aware of their HCV infection.1 In 2016, approximately 13% of those aware of having chronic HCV infection were being treated.2 Approximately 15–30% of patients with chronic HCV infection will develop cirrhosis within 20 years3 and, if left untreated, these patients are at risk of developing hepatic decompensation and hepatocellular carcinoma, ultimately leading to increased liver-related mortality.4 Successful treatment of chronic HCV infection can significantly reduce disease progression, as well as rates of HCV transmission.5 
Diabetes occurring as a direct consequence of loss of liver function is usually characterized by non-diabetic fasting plasma glucose (FPG) and haemoglobin A1c (HbA1c) levels and should regress after orthotopic liver transplantation (OLT). This observational, longitudinal study investigated the relationship between the time-courses of changes in all 3 direct determinants of glucose regulation, i.e., β-cell function, insulin clearance and insulin sensitivity, and diabetes regression after OLT. Eighty cirrhotic patients with non-diabetic FPG and HbA1c levels underwent an extended oral glucose tolerance test (OGTT) before and 3, 6, 12 and 24 months after OLT. The OGTT data were analysed with a mathematical model to estimate derivative control (DC) and proportional control (PC) of β-cell function and insulin clearance (which determine insulin bioavailability), and with the Oral Glucose Insulin Sensitivity (OGIS)-2 h index to estimate insulin sensitivity. At baseline, 36 patients were diabetic (45%) and 44 were non-diabetic (55%). Over the 2-year follow-up, 23 diabetic patients (63.9%) regressed to non-diabetic glucose regulation, whereas 13 did not (36.1%); moreover, 4 non-diabetic individuals progressed to diabetes (9.1%), whereas 40 did not (90.9%). Both DC and PC increased in regressors (from month 3 and 24, respectively) and decreased in progressors, whereas they remained stable in non-regressors and only PC decreased in non-progressors. Insulin clearance increased in all groups, apart from progressors. Likewise, OGIS-2 h improved at month 3 in all groups, but thereafter it continued to improve only in regressors, whereas it returned to baseline values in the other groups. Increased insulin bioavailability driven by improved β-cell function plays a central role in favouring diabetes regression after OLT, in the presence of a sustained improvement of insulin sensitivity. Diabetes mellitus (DM) is a common feature in cirrhotic individuals, due to the bidirectional relationship between impaired glucose metabolism and chronic liver disease.1 On the one hand, type 2 DM is a risk factor for non-alcoholic fatty liver disease (NAFLD)2 and, though not included in the most widely used prognostic tools,3 is a major predictor of adverse outcomes in cirrhotic individuals both before4 and after5 orthotopic liver transplantation (OLT). On the other hand, certain aetiological agents of liver disease, including HCV and NAFLD, may cause β-cell dysfunction and/or insulin resistance, thus favouring development of DM even prior to the onset of cirrhosis.1 Moreover, DM may be a direct consequence of loss of liver function, which impairs insulin secretion and sensitivity via several, partly unrecognized, mechanisms.6 This is the so-called hepatogenous DM, which is not considered a separate clinical entity, despite distinguishing pathophysiological and clinical features.7 We have previously shown that, compared with non-DM cirrhotic individuals, those with hepatogenous DM are characterized by worse β-cell function, which deteriorates in parallel with severity of liver disease.8 In addition, they present with fasting plasma glucose (FPG) and haemoglobin A1c (HbA1c) levels not in the DM range, due to impaired glucose metabolism and reduced lifespan of erythrocytes, respectively.6,7 This “subclinical” presentation implies that, in cirrhotic patients, an oral glucose tolerance test (OGTT) is required for DM diagnosis6,9 and explains the differences in prevalence estimates of DM according to the method(s) of assessment.9 
The RESORCE trial showed that regorafenib improves overall survival (OS) in patients with hepatocellular carcinoma progressing during sorafenib treatment (hazard ratio [HR] 0.62, 95% confidence interval [CI] 0.50–0.78; p <0.0001). This exploratory analysis describes outcomes of sequential treatment with sorafenib followed by regorafenib. In RESORCE, 573 patients were randomized 2:1 to regorafenib 160 mg/day or placebo for 3 weeks on/1 week off. Efficacy and safety were evaluated by last sorafenib dose. The time from the start of sorafenib to death was assessed. Time to progression (TTP) in RESORCE was analyzed by TTP during prior sorafenib treatment. HRs (regorafenib/placebo) for OS by last sorafenib dose were similar (0.67 for 800 mg/day; 0.68 for <800 mg/day). Rates of grade 3, 4, and 5 adverse events with regorafenib by last sorafenib dose (800 mg/day vs. <800 mg/day) were 52%, 11%, and 15% vs. 60%, 10%, and 12%, respectively. Median times (95% CI) from the start of sorafenib to death were 26.0 months (22.6–28.1) for regorafenib and 19.2 months (16.3–22.8) for placebo. Median time from the start of sorafenib to progression on sorafenib was 7.2 months for the regorafenib arm and 7.1 months for the placebo arm. An analysis of TTP in RESORCE in subgroups defined by TTP during prior sorafenib in quartiles (Q) showed HRs (regorafenib/placebo; 95% CI) of 0.66 (0.45–0.96; Q1); 0.26 (0.17–0.40; Q2); 0.40 (0.27–0.60; Q3); and 0.54 (0.36–0.81; Q4). These exploratory analyses show that regorafenib conferred a clinical benefit regardless of the last sorafenib dose or TTP on prior sorafenib. Rates of adverse events were generally similar regardless of the last sorafenib dose. For patients with unresectable hepatocellular carcinoma (HCC) who cannot benefit from resection, transplantation, or ablation, the oral multikinase inhibitor sorafenib at the approved dose of 800 mg/day is the standard first-line systemic treatment.1–3 Recently, lenvatinib was shown to be non-inferior to sorafenib for overall survival (OS) for first-line systemic treatment of HCC.4 Since the approval of sorafenib in 2008, one of the most important unmet needs in the treatment of HCC has been the development of agents that improve outcomes after disease progression during sorafenib treatment. To that end, several phase III trials evaluating novel drugs in this population were conducted, but until recently all failed to meet their primary endpoint of improving OS for these patients.5–8 
Recently, Baveno VI guidelines suggested that esophagogastroduodenoscopy (EGD) can be avoided in patients with compensated advanced chronic liver disease (cACLD) who have a liver stiffness measurement (LSM) <20 kPa and platelet count >150,000/mm3. We aimed to: assess the performance of spleen stiffness measurement (SSM) in ruling out patients with high-risk varices (HRV); validate Baveno VI criteria in a large population and assess how the sequential use of Baveno VI criteria and SSM could safely avoid the need for endoscopy. We retrospectively analyzed 498 patients with cACLD who had undergone LSM/SSM by transient elastography (TE) (FibroScan®), platelet count and EGDs from 2012 to 2016 referred to our tertiary centre. The new combined model was validated internally by a split-validation method, and externally in a prospective multicentre cohort of 115 patients. SSM, LSM, platelet count and Child-Pugh-B were independent predictors of HRV. Applying the newly identified SSM cut-off (≤46 kPa) or Baveno VI criteria, 35.8% and 21.7% of patients in the internal validation cohort could have avoided EGD, with only 2% of HRVs being missed with either model. The combination of SSM with Baveno VI criteria would have avoided an additional 22.5% of EGDs, reaching a final value of 43.8% spared EGDs, with <5% missed HRVs. Results were confirmed in the prospective external validation cohort, as the combined Baveno VI/SSM ≤46 model would have safely spared (0 HRV missed) 37.4% of EGDs, compared to 16.5% when using the Baveno VI criteria alone. A non-invasive prediction model combining SSM with Baveno VI criteria may be useful to rule out HRV and could make it possible to avoid a significantly larger number of unnecessary EGDs compared to Baveno VI criteria only. The increase of portal pressure above the threshold of clinically significant portal hypertension (CSPH, hepatic venous pressure gradient [HVPG] ≥10 mmHg) is a landmark in the natural history of compensated advanced chronic liver disease (cACLD). Patients with CSPH are at risk of developing esophageal varices (EV) and clinical decompensation (variceal bleeding, ascites, jaundice, encephalopathy), which mark the transition from a compensated stage to a stage of the disease (decompensated) characterized by much higher mortality.1 In particular, variceal bleeding is still associated with 10–15% mortality despite the advances in its treatment.2 
Hepatitis C virus (HCV) has emerged as a sexually transmitted infection (STI) among HIV-positive men who have sex with men (MSM). We evaluated HCV-incidence and its risk-factors among HIV-negative MSM using HIV pre-exposure prophylaxis (PrEP). Participants of the Amsterdam PrEP project were tested for HCV antibodies or HCV-RNA every 6 months. Participants used daily or event-driven PrEP and could switch regimens during follow-up. We calculated incidence rates (IRs) for overall HCV-infection and separately for primary and re-infection. A univariable Bayesian exponential survival model was used to identify risk-factors associated with incident HCV-infection. The HCV NS5B gene fragment (709 bp) was sequenced and compared to HCV isolates from HIV-positive MSM and other risk groups (n=419) using phylogenetic analysis. Among 350 participants contributing 653.6 person-years (PY), we detected 15 HCV infections in 14 participants (IR=2.30/100PY). There were eight primary infections (IR=1.27/100PY) and seven re-infections (IR=27.8/100PY). IR was 2.71/100PY in daily and 1.15/100PY in event-driven PrEP-users. Factors associated with incident HCV-infection were higher number of receptive condomless anal sex acts with casual partners (posterior Hazards Rate (HR)=1.57 per ln increase, 95% Credibility Interval (CrI)=1.09-2.20), anal STI (posterior HR=2.93, 95%CrI=1.24-7.13), injecting drug use (posterior HR=4.69, 95%CrI=1.61-12.09) and sharing straws when snorting drugs (posterior HR=2.62, 95%CrI=1.09-6.02). We identified robust MSM-specific HCV clusters of subtypes 1a, 4d, 2b and 3a, which included MSM with and without HIV. HIV-negative MSM on PrEP are at risk for incident HCV-infection, while identified risk-factors are similar to those in HIV-positive MSM. Regular HCV-testing is needed, especially for those with a previous HCV-infection and those reporting risk-factors.  
Several steps in the HBV life cycle remain obscure because of a lack of robust in vitro infection models. These steps include particle entry, formation and maintenance of covalently closed circular (ccc) DNA, kinetics of gene expression and viral transmission routes. This study aimed to investigate infection kinetics and cccDNA dynamics during long-term culture. We selected a highly permissive HepG2-NTCP-K7 cell clone engineered to express sodium taurocholate co-transporting polypeptide (NTCP) that supports the full HBV life cycle. We characterized the replication kinetics and dynamics of HBV over six weeks of infection. HBV infection kinetics showed a slow infection process. Nuclear cccDNA was only detected 24 h post-infection and increased until 3 days post-infection (dpi). Viral RNAs increased from 3 dpi reaching a plateau at 6 dpi. HBV protein levels followed similar kinetics with HBx levels reaching a plateau first. cccDNA levels modestly increased throughout the 45-day study period with 5–12 copies per infected cell. Newly produced relaxed circular DNA within capsids was reimported into the nucleus and replenished the cccDNA pool. In addition to intracellular recycling of HBV genomes, secondary de novo infection events resulted in cccDNA formation. Inhibition of relaxed circular DNA formation by nucleoside analogue treatment of infected cells enabled us to measure cccDNA dynamics. HBV cccDNA decayed slowly with a half-life of about 40 days. After a slow infection process, HBV maintains a stable cccDNA pool by intracellular recycling of HBV genomes and via secondary infection. Our results provide important insights into the dynamics of HBV infection and support the future design and evaluation of new antiviral agents. Hepatitis B virus (HBV) chronically infects 257 million individuals worldwide and is a major driver of end-stage liver diseases such as cirrhosis and hepatocellular carcinoma [WHO 2017]. The immense death toll of 887,000 individuals/year has driven an intensive search for curative treatment approaches. However, a more detailed understanding of infection kinetics and the genesis and maintenance of episomal nuclear DNA pools, so called covalently closed circular (ccc) DNA, is needed to guide the development of an HBV cure. 
Under the regulation of various oncogenic pathways, cancer cells undergo adaptive metabolic programming to maintain specific metabolic states that support their uncontrolled proliferation. As it has been difficult to directly and effectively inhibit oncogenic signaling cascades with pharmaceutical compounds, focusing on the downstream metabolic pathways that enable indefinite growth may provide therapeutic opportunities. Thus, we sought to characterize metabolic changes in hepatocellular carcinoma (HCC) development and identify metabolic targets required for tumorigenesis. We compared gene expression profiles of Morris Hepatoma (MH3924a) and DEN (diethylnitrosamine)-induced HCC models to those of liver tissues from normal and rapidly regenerating liver models, and performed gain- and loss-of-function studies of the identified gene targets for their roles in cancer cell proliferation in vitro and in vivo. The proline biosynthetic enzyme PYCR1 (pyrroline-5-carboxylate reductase 1) was identified as one of the most upregulated genes in the HCC models. Knockdown of PYCR1 potently reduced cell proliferation of multiple HCC cell lines in vitro and tumor growth in vivo. Conversely, overexpression of PYCR1 enhanced the proliferation of the HCC cell lines. Importantly, PYCR1 expression was not elevated in the regenerating liver, and KD or overexpression of PYCR1 had no effect on proliferation of non-cancerous cells. Besides PYCR1, we found that additional proline biosynthetic enzymes, such as ALDH18A1, were upregulated in HCC models and also regulated HCC cell proliferation. Clinical data demonstrated that PYCR1 expression was increased in HCC, correlated with tumor grade, and was an independent predictor of clinical outcome. Enhanced expression of proline biosynthetic enzymes promotes HCC cell proliferation. Inhibition of PYCR1 or ALDH18A1 may be a novel therapeutic strategy to target HCC. With an estimated 700,000 new cases per year, hepatocellular carcinoma (HCC) is the fifth commonest cancer and third leading cause of cancer-related death worldwide. Currently, HCC cases occur predominantly in East and Southeast Asia, and sub-Saharan Africa due to hepatitis B/C viral infection. However, the incidence of HCC is rising in Western nations, largely associated with metabolic complications, such as non-alcoholic steatohepatitis related to obesity and type II diabetes. The overall 5-year survival rate remains below 12% due to limited treatment options.1 So far, 4 oral multikinase inhibitors (sorafenib, regorafenib, lenvatinib and cabozantinib) and 2 immunotherapeutic agents (nivolumab and pembrolizumab) have been FDA-approved for the treatment of HCC.2–7 However, due to treatment eligibility criteria, side effects and modest clinical benefit, it remains a top priority to identify additional effective therapeutic targets. 
Although there is increasing interest in its use, definitive evidence demonstrating a benefit for postmortem normothermic regional perfusion (NRP) in controlled donation after circulatory death (cDCD) liver transplantation is lacking. The aim of this study was to compare results of cDCD liver transplants performed with postmortem NRP vs. super-rapid recovery (SRR), the current standard for cDCD. This was an observational cohort study including all cDCD liver transplants performed in Spain between June 2012 and December 2016, with follow-up ending in December 2017. Each donor hospital determined whether organ recovery was performed using NRP or SRR. The propensity scores technique based on the inverse probability of treatment weighting (IPTW) was used to balance covariates across study groups; logistic and Cox regression models were used for binary and time-to-event outcomes. During the study period, there were 95 cDCD liver transplants performed with postmortem NRP and 117 with SRR. The median donor age was 56 years (interquartile range 45–65 years). After IPTW analysis, baseline covariates were balanced, with all absolute standardised differences <0.15. IPTW-adjusted risks were significantly improved among NRP livers for overall biliary complications (odds ratio 0.14; 95% CI 0.06–0.35, p <0.001), ischaemic type biliary lesions (odds ratio 0.11; 95% CI 0.02–0.57; p = 0.008), and graft loss (hazard ratio 0.39; 95% CI 0.20–0.78; p = 0.008). The use of postmortem NRP in cDCD liver transplantation appears to reduce postoperative biliary complications, ischaemic type biliary lesions and graft loss, and allows for the transplantation of livers even from cDCD donors of advanced age. Donation after circulatory death (DCD) donors, who are declared dead following cardiorespiratory arrest, are an increasingly common source of organs. The period of donor warm ischaemia surrounding arrest can damage the quality of organs in general and the liver in particular, because biliary cells are exquisitely susceptible to warm ischaemia.1 Thus, initial experiences with DCD liver transplantation described high rates of graft dysfunction and non-function and ischaemic type biliary lesions (ITBL). Although complication rates have improved with experience, the rate of post-transplant ITBL remains higher among recipients of DCD grafts vs. those receiving donation after brain death (DBD) grafts (16% vs. 3%, according to 2 meta-analyses2,3). Development of ITBL leads to repeat biliary procedures and hospitalisations; up to 70% of patients with ITBL either require retransplantation or die.4 
NGM282, an engineered analogue of the gut hormone FGF19, improves hepatic steatosis and fibrosis biomarkers in patients with non-alcoholic steatohepatitis (NASH). However, NGM282 increases serum cholesterol levels by inhibiting CYP7A1, which encodes the rate-limiting enzyme in the conversion of cholesterol to bile acids. Herein, we investigate whether administration of a statin can manage the cholesterol increase seen in patients with NASH receiving treatment with NGM282. In this phase II, open-label, multicenter study, patients with biopsy-confirmed NASH were treated with subcutaneous NGM282 once daily for 12 weeks. After 2 weeks, rosuvastatin was added in stepwise, biweekly incremental doses to a maximum of 40 mg daily. Both drugs were continued until the end of treatment at week 12. We evaluated plasma lipids, lipoprotein particles and liver fat content. In 66 patients who received NGM282 0.3 mg (n = 23), NGM282 1 mg (n = 21), or NGM282 3 mg (n = 22), circulating cholesterol increased from baseline at week 2. Initiation of rosuvastatin resulted in rapid decline in plasma levels of total cholesterol and low-density lipoprotein cholesterol. At week 12, reductions from baseline in total cholesterol levels of up to 18% (p <0.001), low-density lipoprotein cholesterol of up to 28% (p <0.001), triglycerides of up to 34% (p <0.001) and an increase in high-density lipoprotein cholesterol of up to 16% (p <0.001), with similar changes in lipoprotein particles, were observed in these patients. Robust decreases from baseline in 7alpha-hydroxy-4-cholesten-3-one (p <0.001) and liver fat content (p <0.001) were also observed. Rosuvastatin was safe and well-tolerated when co-administered with NGM282 in patients with NASH. In this multicenter study, NGM282-associated elevation of cholesterol was effectively managed with rosuvastatin. Co-administration of rosuvastatin with NGM282 may be a reasonable strategy to optimize the cardiovascular risk profile in patients with NASH. Non-alcoholic steatohepatitis (NASH), a severe form of non-alcoholic fatty liver disease (NAFLD), represents a large and growing public health concern that is increasingly contributing to the rising prevalence of cirrhosis and hepatocellular carcinoma globally.1,2 Currently, there is no approved drug for NASH, which is projected to be the leading indication for liver transplantation in the next decade.3 The pathogenesis of NASH is complex, and it is hypothesized that toxic lipid species or intermediates may inflict hepatocyte injury.4 In recent years, bile acids have emerged as important molecules that act at both hepatic and extrahepatic tissues to modulate metabolic, inflammation and fibrogenesis pathways.5 While crucial for the emulsification and absorption of dietary fat, bile acids can cause cell death and liver injury when amassed within hepatocytes. Indeed, patients with NASH have elevated hepatic and circulating concentrations of bile acids.6,7 The LDL-C elevation associated with NGM282 administration was effectively managed with statin therapy. Co-administration of NGM282 and rosuvastatin reduces plasma triglycerides, total cholesterol, LDL-C, and increases HDL-C in patients with NASH. SECTION Financial support
p38 mitogen-activated protein kinases are important inflammatory factors. p38α alteration has been implicated in both human and mouse inflammatory disease models. Therefore, we aimed to characterize the cell type-specific role of p38α in non-alcoholic steatohepatitis (NASH). Human liver tissues were obtained from 27 patients with non-alcoholic fatty liver disease (NAFLD) and 20 control individuals. NASH was established and compared between hepatocyte-specific p38α knockout (p38αΔHep), macrophage-specific p38α knockout (p38αΔMΦ) and wild-type (p38αfl/fl) mice fed with high-fat diet (HFD), high-fat/high-cholesterol diet (HFHC), or methionine-and choline-deficient diet (MCD). p38 inhibitors were administered to HFHC-fed wild-type mice for disease treatment. p38α was significantly upregulated in the liver tissues of patients with NAFLD. Compared to p38αfl/fl littermates, p38αΔHep mice developed significant nutritional steatohepatitis induced by HFD, HFHC or MCD. Meanwhile, p38αΔMΦ mice exhibited less severe steatohepatitis and insulin resistance than p38αfl/fl mice in response to a HFHC or MCD. The effect of macrophage p38α in promoting steatohepatitis was mediated by the induction of pro-inflammatory factors (CXCL2, IL-1β, CXCL10 and IL-6) secreted by M1 macrophages and associated signaling pathways. p38αΔMΦ mice exhibited M2 anti-inflammatory polarization as demonstrated by increased CD45+F4/80+CD11b+CD206+ M2 macrophages and enhanced arginase activity in liver tissues. Primary hepatocytes from p38αΔMΦ mice showed decreased steatosis and inflammatory damage. In a co-culture system, p38α deleted macrophages attenuated steatohepatitic changes in hepatocytes through decreased secretion of pro-inflammatory cytokines (TNF-α, CXCL10 and IL-6), which mediate M1 macrophage polarization in p38αΔMΦ mice. Restoration of TNF-α, CXCL10 or IL‐6 induced lipid accumulation and inflammatory responses in p38αfl/fl hepatocytes co-cultured with p38αΔMΦ macrophages. Moreover, pharmacological p38 inhibitors suppressed HFHC-induced steatohepatitis. Macrophage p38α promotes the progression of steatohepatitis by inducing pro-inflammatory cytokine secretion and M1 polarization. p38 inhibition protects against steatohepatitis. Non-alcoholic fatty liver disease (NAFLD) is the hepatic manifestation of metabolic syndrome and a major healthcare burden worldwide.1 The clinicopathological spectrum of NAFLD ranges from hepatic steatosis to non-alcoholic steatohepatitis (NASH), a more aggressive form which can progress to cirrhosis and/or hepatocellular carcinoma.2 However, there is currently no effective pharmacological therapy approved for NASH, and efforts to control complications arising from the condition are far from satisfactory.3 A better understanding of the molecular mechanisms of NASH is essential for developing promising treatment strategies for this highly prevalent disease. 
Primary biliary cholangitis (PBC) is an autoimmune-associated chronic liver disease triggered by environmental factors, such as exposure to xenobiotics, which leads to a loss of tolerance to the lipoic acid-conjugated regions of the mitochondrial pyruvate dehydrogenase complex, typically to the E2 component. We aimed to identify xenobiotics that might be involved in the environmental triggering of PBC. Urban landfill and control soil samples from a region with high PBC incidence were screened for xenobiotic activities using analytical, cell-based xenobiotic receptor activation assays and toxicity screens. A variety of potential xenobiotic classes were ubiquitously present, as identified by their interaction with xenobiotic receptors – aryl hydrocarbon receptor, androgen receptor and peroxisome proliferator activated receptor alpha – in cell-based screens. In contrast, xenoestrogens were present at higher levels in soil extracts from around an urban landfill. Furthermore, two landfill sampling sites contained a chemical(s) that inhibited mitochondrial oxidative phosphorylation and induced the apoptosis of a hepatic progenitor cell. The mitochondrial effect was also demonstrated in human liver cholangiocytes from three separate donors. The chemical was identified as the ionic liquid [3-methyl-1-octyl-1H-imidazol-3-ium]+ (M8OI) and the toxic effects were recapitulated using authentic pure chemical. A carboxylate-containing human hepatocyte metabolite of M8OI, bearing structural similarity to lipoic acid, was also enzymatically incorporated into the E2 component of the pyruvate dehydrogenase complex via the exogenous lipoylation pathway in vitro. These results identify, for the first time, a xenobiotic in the environment that may be related to and/or be a component of an environmental trigger for PBC. Therefore, further study in experimental animal models is warranted, to determine the risk of exposure to these ionic liquids. Liver disease constitutes the third most common cause of premature death in the UK, with an upward trend in mortality.1,2 The major causes of liver disease such as obesity, viral infection and chronic alcohol consumption are preventable. However, the causes of rarer types of liver disease – such as primary biliary cholangitis (PBC) and primary sclerosing cholangitis (PSC) are unknown and as a consequence, prevention and/or treatments are limited.3 A variety of factors have been linked to increased incidence of PBC and PSC in populations. In both cases, although there is a clear genetic pre-disposition,4,5 there is also evidence that environmental factors – such as exposure to foreign compounds (xenobiotics) – determine the likelihood of developing disease.6–9 
The hepatic injury caused by ischemia/reperfusion (I/R) insult is predominantly determined by the complex interplay of sterile inflammation and liver cell death. Caspase recruitment domain family member 6 (CARD6) was initially shown to play important roles in NF-κB activation. In our preliminary studies, CARD6 downregulation was closely related to hepatic I/R injury in liver transplantation patients and mouse models. Thus, we hypothesized that CARD6 protects against hepatic I/R injury and investigated the underlying molecular mechanisms. A partial hepatic I/R operation was performed in hepatocyte-specific Card6 knockout mice (HKO), Card6 transgenic mice with CARD6 overexpression specifically in hepatocytes (HTG), and the corresponding control mice. Hepatic histology, serum aminotransferases, inflammatory cytokines/chemokines, cell death, and inflammatory signaling were examined to assess liver damage. The molecular mechanisms of CARD6 function were explored in vivo and in vitro. Liver injury was alleviated in Card6-HTG mice compared with control mice as shown by decreased cell death, lower serum aminotransferase levels, and reduced inflammation and infiltration, whereas Card6-HKO mice had the opposite phenotype. Mechanistically, phosphorylation of ASK1 and its downstream effectors JNK and p38 were increased in the livers of Card6-HKO mice but repressed in those of Card6-HTG mice. Furthermore, ASK1 knockdown normalized the effect of CARD6 deficiency on the activation of NF-κB, JNK and p38, while ASK1 overexpression abrogated the suppressive effect of CARD6. CARD6 was also shown to interact with ASK1. Mutant CARD6 that lacked the ability to interact with ASK1 could not inhibit ASK1 and failed to protect against hepatic I/R injury. CARD6 is a novel protective factor against hepatic I/R injury that suppresses inflammation and liver cell death by inhibiting the ASK1 signaling pathway. Ischemia/reperfusion (I/R) injury is an important cause of liver damage that occurs during surgical procedures, including liver tumor resection and liver transplantation.1 During this process, the initial hypoxic insult causes direct cellular damage, while the subsequent return of blood flow further aggravates liver dysfunction and injury due to the propagation of inflammation. The mechanism underlying the propagation of the inflammatory response and further tissue damage involves a complex interplay of cytokines/chemokines, multiple cell types, and various signaling pathways.2,3 Identification of pivotal regulators controlling this process and elucidation of the underlying mechanisms are essential for the development of novel strategies for clinical intervention of hepatic I/R injury. 
Subclinical inflammatory changes are commonly described in long-term transplant recipients undergoing protocol liver biopsies. The pathogenesis of these lesions remains unclear. The aim of this study was to identify the key molecular pathways driving progressive subclinical inflammatory liver allograft damage. All liver recipients followed at Hospital Clínic Barcelona who were >10 years post-transplant were screened for participation in the study. Patients with recurrence of underlying liver disease, biliary or vascular complications, chronic rejection, and abnormal liver function tests were excluded. Sixty-seven patients agreed to participate and underwent blood and serological tests, transient elastography and a liver biopsy. Transcriptome profiling was performed on RNA extracted from 49 out of the 67 biopsies employing a whole genome next generation sequencing platform. Patients were followed for a median of 6.8 years following the index liver biopsy. Median time since transplantation to liver biopsy was 13 years (10–22). The most frequently observed histological abnormality was portal inflammation with different degrees of fibrosis, present in 45 biopsies (67%). Two modules of 102 and 425 co-expressed genes were significantly correlated with portal inflammation, interface hepatitis and portal fibrosis. These modules were enriched in molecular pathways known to be associated with T cell mediated rejection. Liver allografts showing the highest expression levels for the two modules recapitulated the transcriptional profile of biopsies with clinically apparent rejection and developed progressive damage over time, as assessed by non-invasive markers of fibrosis. A large proportion of adult liver transplant recipients who survive long-term exhibit subclinical histological abnormalities. The transcriptomic profile of these patients’ liver tissue closely resembles that of T cell mediated rejection and may result in progressive allograft damage. Routine serum markers of liver injury such as aspartate aminotransferase (AST), alanine aminotransferases (ALT), gamma-glutamyl-transpeptidase (GGT) or alkaline phosphatase (AP) are known to be insensitive and nonspecific indicators of allograft rejection in liver transplantation (LT).1,2 Despite this, the long-term management of LT recipients continues to rely on a combination of serum liver biochemistry tests and calcineurin inhibitor pharmacokinetic markers. The performance of protocol, or surveillance, liver biopsies has been proposed as a more accurate strategy to assess graft function and potentially to personalize the use of immunosuppression.3–5 This is based on a multiplicity of studies showing that a very large proportion of patients with normal liver biochemistry tests exhibit clinically significant histological lesions, with chronic hepatitis not attributable to recognizable causes, such as viral infection or autoimmune hepatitis, being the most frequently described abnormality.1,6–8 However, the clinical utility of protocol liver biopsies remains contentious,5 and as a result they are not performed in the vast majority of adult liver transplant programs. The controversy stems from an incomplete understanding of the natural history and pathogenesis of the so-called idiopathic inflammatory lesions.2,9 This is due to the paucity of prospective clinical studies and the lack of in-depth studies comparing the molecular signatures of these lesions with those of well-characterized histological phenotypes. 
Whether non-selective beta blockers (NSBBs) are deleterious in patients with end-stage cirrhosis and refractory ascites has been widely debated. We hypothesized that only the subset of patients on the liver transplant waiting list who had impaired cardiac performance would be at increased risk of mortality if receiving NSBBs. This study included 584 consecutive patients with cirrhosis evaluated for transplantation between 1999 and 2014. All patients had right heart catheterization with hemodynamic measurements at evaluation. Fifty percent received NSBBs. Refractory ascites was present in 33%. Cardiac performance was assessed by left ventricular stroke work index (LVSWI). Waiting list mortality without liver transplantation was explored using competing risk analysis. LVSWI was significantly lower in patients with refractory ascites. In multivariate analysis using competing risk, refractory ascites, NSBBs and LVSWI were associated with waiting list mortality in the whole population, with a statistically significant interaction between NSBBs and LVSWI. The most discriminant value of LVSWI was 64.1 g-m/m2. In the final model, refractory ascites (subdistribution hazard ratio 1.52; 95% CI 1.01–2.28; p = 0.0083) and treatment by NSBBs with LVSWI <64.1 g-m/m2 (subdistribution hazard ratio 1.96; 95% CI 1.32–2.90; p = 0.0009) were significantly associated with waiting list mortality, taking into account serum sodium and the model for end-stage liver disease score. This study suggests that compromised cardiac performance is more common in patients with refractory ascites and that NSBBs are deleterious in cirrhotic patients with compromised cardiac performance. These results highlight the prognostic value of cardiac function in patients with end-stage cirrhosis. Complex interactions between splanchnic and systemic hemodynamic changes are characteristic features of advanced cirrhosis.1 Splanchnic and systemic vasodilation associated with decreased systemic vascular resistance (SVR) and increased cardiac output (CO) result in a hyperkinetic state.2–4 However, the imbalance between systemic and splanchnic circulation is associated with a state of effective hypovolemia. Besides hyperkinetic state and effective hypovolemia, so-called cirrhotic cardiomyopathy was described several years ago.5,6 Typically, cirrhotic cardiomyopathy is characterized by (i) a blunted contractile responsiveness to stress, (ii) diastolic dysfunction and (iii) electromechanical abnormalities,5,7 in the absence of other known causes of heart disease. Such cardiac changes may be present in up to 60% of patients with cirrhosis, especially in those with ascites.8 However, the prevalence of cirrhotic cardiomyopathy has not been extensively investigated. In addition, some series suggest that cirrhotic cardiomyopathy is not related to the severity of cirrhosis,9 while others suggest a correlation between cardiac changes and disease severity.10,11 Moreover, the impact of cirrhotic cardiomyopathy on the outcome of patients with advanced cirrhosis and on post-transplant survival is poorly understood.11–15 
It has been proposed that serum hepatitis B core-related antigen (HBcrAg) reflects intrahepatic covalently closed circular (ccc)DNA levels. However, the correlation of HBcrAg with serum and intrahepatic viral markers and liver histology has not been comprehensively investigated in a large sample. We aimed to determine if HBcrAg could be a useful therapeutic marker in patients with chronic hepatitis B. HBcrAg was measured by chemiluminescent enzyme immunoassay in 130 (36 hepatitis B e antigen [HBeAg]+ and 94 HBeAg−) biopsy proven, untreated, patients with chronic hepatitis B. HBcrAg levels were correlated with: a) serum hepatitis B virus (HBV)-DNA, quantitative hepatitis B surface antigen and alanine aminotransferase levels; b) intrahepatic total (t)HBV-DNA, cccDNA, pregenomic (pg)RNA and cccDNA transcriptional activity (defined as pgRNA/cccDNA ratio); c) fibrosis and necroinflammatory activity scores. HBcrAg levels were significantly higher in HBeAg+ vs. HBeAg− patients and correlated with serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA and cccDNA levels, and transcriptional activity. Patients who were negative for HBcrAg (<3 LogU/ml) had less liver cccDNA and lower cccDNA activity than the HBcrAg+ group. Principal component analysis coupled with unsupervised clustering identified that in a subgroup of HBeAg− patients, higher HBcrAg levels were associated with higher serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA, cccDNA transcriptional activity and with higher fibrosis and necroinflammatory activity scores. Our results indicate that HBcrAg is a surrogate marker of both intrahepatic cccDNA and its transcriptional activity. HBcrAg could be useful in the evaluation of new antiviral therapies aiming at a functional cure of HBV infection either by directly or indirectly targeting the intrahepatic cccDNA pool. The development of novel antiviral agents and immunomodulatory approaches to cure hepatitis B virus (HBV) infection requires new biomarkers capable of reflecting the intrahepatic activity of the virus and chronic HBV (CHB) stages and defining new meaningful treatment endpoints. Indeed, there is an unmet need for standardized assays able to provide mechanistic insights into the effects of the novel antiviral and immunomodulatory agents and to assess treatment efficacy.1 
In order to design an effective vaccine against hepatitis C virus (HCV) infection, it is necessary to understand immune protection. A number of broadly reactive neutralizing antibodies have been isolated from B cells of HCV-infected patients. However, it remains unclear whether B cells producing such antibodies contribute to HCV clearance and long-term immune protection against HCV. We analysed the B cell repertoire of 13 injecting drug users from the Amsterdam Cohort Study, who were followed up for a median of 17.5 years after primary infection. Individuals were classified into 2 groups based on the outcome of HCV infection: 5 who became chronically infected either after primary infection or after reinfection, and 8 who were HCV RNA negative following spontaneous clearance of ≥1 HCV infection(s). From each individual, 10,000 CD27+IgG+B cells, collected 0.75 year after HCV infection, were cultured to characterize the antibody repertoire. Using a multiplex flow cytometry-based assay to study the antibody binding to E1E2 from genotype 1 to 6, we found that a high frequency of cross-genotype antibodies was associated with spontaneous clearance of 1 or multiple infections (p = 0.03). Epitope specificity of these cross-genotype antibodies was determined by alanine mutant scanning in 4 individuals who were HCV RNA negative following spontaneous clearance of 1 or multiple infections. Interestingly, the cross-genotype antibodies were mainly antigenic region 3 (AR3)-specific and showed cross-neutralizing activity against HCV. In addition to AR3 antibodies, 3 individuals developed antibodies recognizing antigenic region 4, of which 1 monoclonal antibody showed cross-neutralizing capacity. Together, these data suggest that a strong B cell response producing cross-genotype and neutralizing antibodies, especially targeting AR3, contributes to HCV clearance and long-term immune protection against HCV. Hepatitis C virus (HCV) is one of the major global public health problems, with 71 million chronically infected people worldwide, which results in 350,000 to 500,000 liver-related deaths per year.1 Current direct-acting antiviral (DAA) treatment is very effective in clearing infection.2 However, despite high DAA efficacy, treatment alone is unlikely to eliminate HCV by the year 2030 as envisioned by the World Health Organization (WHO),1 since treated individuals may become reinfected if exposure continues. Moreover, worldwide, most of the HCV-infected individuals are unaware of their HCV status because of the prolonged asymptomatic nature of HCV infection and limited access to diagnostic tests.3 In addition, as a result of the high cost of treatment, a large proportion of the HCV-infected individuals are left untreated.3 Therefore, for the global elimination of HCV, a preventive vaccine is urgently needed. 
Advanced liver fibrosis is an important diagnostic target in non-alcoholic fatty liver disease (NAFLD) as it defines the subgroup of patients with impaired prognosis. The non-invasive diagnosis of advanced fibrosis is currently limited by the suboptimal positive predictive value and the grey zone (representing indeterminate diagnosis) of fibrosis tests. Here, we aimed to determine the best combination of non-invasive tests for the diagnosis of advanced fibrosis in NAFLD. A total of 938 patients with biopsy-proven NAFLD were randomized 2:1 into derivation and validation sets. All patients underwent liver stiffness measurement with vibration controlled transient elastography (VCTE) and blood fibrosis tests (NAFLD fibrosis score, Fibrosis-4 [FIB4], Fibrotest, Hepascore, FibroMeter). FibroMeterVCTE, which combines VCTE results and FibroMeter markers in a single test, was also calculated in all patients. For the diagnosis of advanced fibrosis, VCTE was significantly more accurate than the blood tests (area under the receiver operating characteristic curve [AUROC]: 0.840 ± 0.013, p ≤0.005). FibroMeter was the most accurate blood test (AUROC: 0.793 ± 0.015, p ≤0.017). The combinatory test FibroMeterVCTE outperformed VCTE and blood tests (AUROC: 0.866 ± 0.012, p ≤0.005). The sequential combination of FIB4 then FibroMeterVCTE (FIB4-FMVCTE algorithm) or VCTE then FibroMeterVCTE (VCTE-FMVCTE algorithm) provided an excellent diagnostic accuracy of 90% for advanced fibrosis, with liver biopsy only required to confirm the diagnosis in 20% of cases. The FIB4-FMVCTE and VCTE-FMVCTE algorithms were significantly more accurate than the pragmatic algorithms currently proposed. The sequential combination of fibrosis tests in the FIB4-FMVCTE and VCTE-FMVCTE algorithms provides a highly accurate solution for the diagnosis of advanced fibrosis in NAFLD. These algorithms should now be validated for the diagnosis of advanced liver fibrosis in diabetology or primary care settings. Non-alcoholic fatty liver disease (NAFLD), the liver manifestation of the metabolic syndrome linked to obesity and insulin resistance, affects 25% of the general population both in western and developing countries.1 As in the other causes of chronic liver disease, liver fibrosis is the main determinant of prognosis in NAFLD.2 The risk of liver-related mortality increases from fibrosis stage 2 and is exponentially higher when transitioning to stage F3 (bridging fibrosis) then F4 (cirrhosis).2 Therefore, as recommended by international guidelines, patients with NAFLD should be assessed for the presence of advanced F3/4 fibrosis, because of its prognostic implications.3,4 
Early recurrence of hepatocellular carcinoma (HCC) after curative resection is common. However, the association between genetic mechanisms and early HCC recurrence, especially in Chinese patients, remains largely unknown. We performed whole-genome sequencing (49 cases), whole-exome sequencing (18 cases), and deep targeted sequencing (115 cases) on 182 primary HCC samples. Focusing on WNK2, we used Sanger sequencing and qPCR to evaluate all the coding exons and copy numbers of that gene in an additional 554 HCC samples. We also explored the functional effect and mechanism of WNK2 on tumor growth and metastasis. We identified 5 genes (WNK2, RUNX1T1, CTNNB1, TSC1, and TP53) harboring somatic mutations that correlated with early tumor recurrence after curative resection in 182 primary HCC samples. Focusing on WNK2, the overall somatic mutation and copy number loss occurred in 5.3% (39/736) and 27.2% (200/736), respectively, of the total 736 HCC samples. Both types of variation were associated with lower WNK2 protein levels, higher rates of early tumor recurrence, and shorter overall survival. Biofunctional investigations revealed a tumor-suppressor role of WNK2: its inactivation led to ERK1/2 signaling activation in HCC cells, tumor-associated macrophage infiltration, and tumor growth and metastasis. Our results delineate genomic events that characterize Chinese HCCs and identify WNK2 as a driver of early HCC recurrence after curative resection. Hepatocellular carcinoma (HCC) is a relatively common type of cancer with rising incidence and mortality rate.1 Approximately half of the 782,500 new cases of liver cancer and the 745,500 deaths due to liver cancer that occur worldwide each year occur in China.1 In China, most HCCs arise from hepatitis B,2 other risk factors such as exposures to aflatoxin3 and aristolochic acids4 can also be important in HCC development. Those etiological factors may result in different genetic alterations, as well as different therapeutic targets, in Chinese patients compared with patients from other countries and regions. 
There are limited data on the association between non-alcoholic fatty liver disease (NAFLD) and subclinical coronary atherosclerosis. This study investigated the influence of NAFLD on subclinical coronary atherosclerosis as detected by coronary computed tomography angiography (CCTA) in an asymptomatic population. A total of 5,121 consecutive asymptomatic individuals with no prior history of coronary artery disease or significant alcohol intake voluntarily underwent abdominal ultrasonography and CCTA as part of a general health examination. Fatty liver was assessed by ultrasonography examination. The fatty liver index and NAFLD fibrosis score were also calculated. Coronary atherosclerotic plaques on CCTA were evaluated. The association between NAFLD and subclinical coronary atherosclerosis was determined by logistic regression analysis. Of the study participants, 1,979 (38.6%) had ultrasonography-diagnosed NAFLD. After adjustment for cardiovascular risk factors, there were no statistically significant differences in the adjusted odds ratios of NAFLD for calcified plaque (1.03; 95% CI 0.89–1.20; p = 0.673) and mixed plaque (1.15; 95% CI 0.93–1.42; p = 0.214). However, adjusted odds ratios for any atherosclerotic plaque (1.18; 95% CI 1.03–1.35; p = 0.016) and non-calcified plaque (1.27; 95% CI 1.08–1.48; p = 0.003) were significantly higher in NAFLD. In addition, there was a significant association of fatty liver index ≥30 with non-calcified plaque (1.37; 95% CI 1.14–1.65; p = 0.001) and NAFLD fibrosis score ≥−1.455 with non-calcified plaque (1.20; 95% CI 1.08–1.42; p = 0.030). In this large cross-sectional study of asymptomatic individuals undergoing CCTA, NAFLD was consistently associated with non-calcified plaque, suggesting an increased cardiovascular risk. Non-alcoholic fatty liver disease (NAFLD) is a prevalent liver disease in the general population, affecting up to 30% of the adult population in the United States and Europe.1,2 NAFLD is also considered as a hepatic manifestation of metabolic syndrome, with insulin resistance a common pathophysiology.3,4 The presence of NAFLD has been known to be associated with an increased prevalence and incidence of cardiovascular disease, independently of other well-known cardiovascular risk factors.5–9 However, there are limited data regarding the association between NAFLD and subclinical coronary atherosclerosis in asymptomatic individuals. Recently, with the advent of multidetector row computed tomography, coronary computed tomography angiography (CCTA) has proven to be effective in providing a comprehensive evaluation of coronary atherosclerosis, including lesion location, severity, and plaque characteristics.10 A recent meta-analysis with CCTA showed that the specific characteristics of atherosclerotic plaques determined the different risk of future cardiovascular events.11 Therefore, this study sought to evaluate the relationship between NAFLD and characteristics of atherosclerotic plaques through a large cohort of asymptomatic Korean individuals who voluntarily underwent CCTA for early detection of coronary artery disease (CAD). SECTION Patients and methods SECTION Study population 
No-touch multibipolar radiofrequency ablation (NTM-RFA) represents a novel therapy that surpasses standard RFA for hepatocellular carcinoma (HCC), but it has not been compared to surgical resection (SR). We aimed to compare the outcomes of NTM-RFA and SR for intermediate-sized HCC. Between 2012 and 2016, 141 patients with solitary HCC ranging from 2 to 5 cm were treated by NTM-RFA or SR at a single-center. The outcomes of 128 patients were compared after using inverse probability of treatment weighting (IPTW). Seventy-nine patients had NTM-RFA and 62 had SR. After IPTW, the two groups were well-balanced for most baseline characteristics including tumor size, location, etiology, severity of underlying liver disease and alpha-fetoprotein level. Morbidity was higher (67.9% vs. 50.0%, p = 0.042) and hospital stay was longer (12 [IQR 8–13] vs. 7 [IQR 5–9] days, p <0.001) after SR. Local recurrence rates at one and three years were 5.5% and 10.0% after NTM-RFA and 1.9% and 1.9% after SR, respectively (p = 0.065). The rates of systematized recurrence (within the treated segment or in an adjacent segment within a 2 cm distance from treatment site) were higher after NTM-RFA (7.4% vs. 1.9% at one year, 27.8% vs. 3.3% at three years, p = 0.008). Most patients with recurrence were eligible for rescue treatment, resulting in similar overall survival (86.7% after NTM-RFA, 91.4% after SR at three years, p = 0.954) and disease-free survival (40.8% after NTM-RFA, 56.4% after SR at three years, p = 0.119). Compared to SR, NTM-RFA for solitary intermediate-sized HCC was associated with less morbidity and more systematized recurrence, while the rate of local recurrence was not significantly different. Most patients with intrahepatic recurrence remained eligible for rescue therapies, resulting in equivalent long-term oncological results after both treatments. Surgical resection (SR) and radiofrequency ablation (RFA) represent two treatments of choice for the curative management of early hepatocellular carcinoma (HCC) in patients not eligible for or awaiting liver transplantation.1–4 In the past, several studies mainly from Eastern centers suggested that RFA was as effective as SR for solitary HCCs of less than 2 to 3 cm. Furthermore, it is associated with less procedure-related morbidity,5–8 resulting in better cost-effectiveness9 and quality of life.10 However, for larger lesions, SR appears to be superior to the various existing modalities of thermoablation, such as radiofrequency or microwave ablation in terms of local control and disease-free survival.8,11–14 Therefore, SR is currently considered the best therapeutic option in patients with intermediate-sized HCC ranging from 2 to 5 cm, with no contraindication for surgery.1,3,4,15 
Liver macrophages can be involved in both pathogen clearance and/or pathogenesis. To get further insight on their role during chronic hepatitis B virus (HBV) infections, our aim was to phenotypically and functionally characterize in vivo and ex vivo the interplay between HBV, primary human liver macrophages (PLMs) and primary blood monocytes differentiated into pro-inflammatory or anti-inflammatory macrophages (M1-MDMs or M2-MDMs, respectively). PLMs or primary blood monocytes, either ex vivo differentiated into M1-MDMs or M2-MDMs, were exposed to HBV and their activation followed by ELISA or quantitative reverse transcription PCR (RT-qPCR). Liver biopsies from HBV-infected patients were analysed by RT-qPCR or immunohistochemistry. Viral parameters in HBV-infected primary human hepatocytes and differentiated HepaRG cells were followed by ELISA, qPCR and RT-qPCR analyses. HBc protein was present within the macrophages of liver biopsies taken from HBV-infected patients. Macrophages from HBV-infected patients also expressed higher levels of anti-inflammatory macrophage markers than those from non-infected patients. Ex vivo exposure of naive PLMs to HBV led to reduced secretion of pro-inflammatory cytokines. Upon exposure to HBV or HBV-producing cells during differentiation and activation, M1-MDMs secreted less IL-6 and IL-1β, whereas M2-MDMs secreted more IL-10 when exposed to HBV during activation. Finally, cytokines produced by M1-MDMs, but not those produced by HBV-exposed M1-MDMs, decreased HBV infection of hepatocytes. Altogether, our data strongly suggest that HBV modulates liver macrophage functions to favour the establishment of infection. Hepatitis B virus (HBV) chronically infects around 250 million people worldwide (World Health Organization data, 2016) and increases the risk of developing cirrhosis and hepatocellular carcinoma.1 Current treatments, mainly based on nucleos(t)ide analogues, reduce blood viremia to undetectable levels in the majority of patients, but do not achieve virus elimination from the liver.2 New treatments, including immune-therapeutic components, are therefore needed in order to progress toward a functional cure for HBV. 
Noncytolytic curing of hepatitis B virus (HBV) infected hepatocytes by cytokines including type I interferons (IFNs) is of importance for resolving acute and chronic infection. However, as IFNs stimulate hundreds of genes those most relevant for HBV suppression remain largely unknown. Amongst them are the large Mx GTPases. Human MX1 (or MxA) is active against many RNA viruses while MX2 (or MxB) was recently found to restrict human immunodeficiency virus 1, hepatitis C virus, and herpesviruses. Here we investigated the anti-HBV activity of MX2. Potential anti-HBV activity of MX2 and functional variants was assessed in transfected and HBV infected hepatoma cells and primary human hepatocytes, employing multiple assays to determine HBV nucleic acids as well as their synthesis and decay. The specific roles of MX2 in IFN-α inhibition of HBV transcription and replication were addressed by MX2-specific shRNA interference (RNAi). MX2 alone as well as IFN-α substantially inhibited HBV replication, due to significant deceleration of the synthesis and slight acceleration of the turnover of viral RNA. RNAi knock-down of MX2 significantly reduced the inhibitory effects of IFN-α. Strikingly, MX2 inhibited HBV infection by reducing covalently closed circular DNA (cccDNA), most likely by indirectly impairing relaxed circular DNA to cccDNA conversion rather than destabilizing existing cccDNA. Various mutations affecting the GTPase activity and oligomerization status reduced MX2’s anti-HBV activity. MX2 is an important IFN-α inducible effector that decreases HBV RNA levels but can also potently inhibit HBV infection by indirectly impairing cccDNA formation. MX2 likely has the potential of therapeutic application aimed at curing HBV infection by eliminating cccDNA. More than 250 million people are chronically infected with hepatitis B virus (HBV) and at a greatly increased risk to develop terminal liver disease. Currently approved treatments for chronic hepatitis B (CHB) are limited to type-I interferons (IFNs) and five nucleos(t)ide analogs (NAs) [1]. NAs inhibit the viral reverse transcriptase but rarely induce HBV surface antigen (HBsAg) seroconversion (an indicator of functional cure [2]), probably necessitating life-long treatment. IFNs, though applicable in only a fraction of patients, can lead to sustained suppression of HBV replication after a finite (usually 48 week) therapy [3, 4], possibly by their immunomodulatory activity, including restoration of innate responses [5, 6], and/or via one or more products of the many antiviral IFN stimulated genes (ISGs) [7]. 
A causal link has recently been established between epigenetic alterations and hepatocarcinogenesis, indicating that epigenetic inhibition may have therapeutic potential. We aimed to identify and target epigenetic modifiers that show molecular alterations in hepatocellular carcinoma (HCC). We studied the molecular-clinical correlations of epigenetic modifiers including bromodomains, histone acetyltransferases, lysine methyltransferases and lysine demethylases in HCC using The Cancer Genome Atlas (TCGA) data of 365 patients with HCC. The therapeutic potential of epigenetic inhibitors was evaluated in vitro and in vivo. RNA sequencing analysis and its correlation with expression and clinical data in the TCGA dataset were used to identify expression programs normalized by Jumonji lysine demethylase (JmjC) inhibitors. Genetic alterations, aberrant expression, and correlation between tumor expression and poor patient prognosis of epigenetic enzymes are common events in HCC. Epigenetic inhibitors that target bromodomain (JQ-1), lysine methyltransferases (BIX-1294 and LLY-507) and JmjC lysine demethylases (JIB-04, GSK-J4 and SD-70) reduce HCC aggressiveness. The pan-JmjC inhibitor JIB-04 had a potent antitumor effect in tumor bearing mice. HCC cells treated with JmjC inhibitors showed overlapping changes in expression programs related with inhibition of cell proliferation and induction of cell death. JmjC inhibition reverses an aggressive HCC gene expression program that is also altered in patients with HCC. Several genes downregulated by JmjC inhibitors are highly expressed in tumor vs. non-tumor parenchyma, and their high expression correlates with a poor prognosis. We identified and validated a 4-gene expression prognostic signature consisting of CENPA, KIF20A, PLK1, and NCAPG. The epigenetic alterations identified in HCC can be used to predict prognosis and to define a subgroup of high-risk patients that would potentially benefit from JmjC inhibitor therapy. Hepatocellular carcinoma (HCC) is the most common primary liver cancer and usually occurs in patients with cirrhosis.1 HCC is the sixth most frequent solid tumor and the second leading cause of cancer-related death worldwide and, unfortunately, its incidence and mortality are steadily increasing in Western countries.1 Liver resection, transplantation, and tumor ablation are considered curative options although they are applied in only 30–40% of patients. The multikinase inhibitors sorafenib, as first line therapy, and regorafenib, as second line therapy, have been approved for advanced HCC, yet they have modest impact on patient survival.1,2 Thus, there is an urgent need for new effective therapies. 
Interferon regulatory factor 3 (IRF3) is a transcription factor mediating antiviral responses, yet recent evidence indicates that IRF3 also has critical non-transcriptional functions, including activating RIG-I-like receptors-induced IRF-3-mediated pathway of apoptosis (RIPA) and restricting activity of NF-κB. Using a novel murine model expressing only non-transcriptional IRF3 activity (Irf3S1/S1), we tested the hypothesis that non-transcriptional functions of IRF3 modulate innate immune responses in the Gao-binge (acute-on-chronic) model of alcohol-related liver disease. IRF3 and IRF3-mediated signals were analysed in liver samples from 5 patients transplanted for alcoholic hepatitis and 5 healthy controls. C57BL/6, Irf3−/− and Irf3S1/S1 mice were exposed to Gao-binge ethanol-induced liver injury. IRF3-mediated RIPA was investigated in cultured macrophages. Phospho-IRF3 and IRF3-mediated signals were elevated in livers of patients with alcoholic hepatitis. In C57BL/6 mice, Gao-binge ethanol exposure activated IRF3 signaling and resulted in hepatocellular injury. Indicators of liver injury were differentially impacted by Irf3 genotype. Irf3−/−, but not Irf3S1/S1, mice were protected from steatosis, elevated alanine/aspartate aminotransferase levels and inflammatory cytokine expression. In contrast, neutrophil accumulation and endoplasmic reticulum stress were independent of genotype. Protection from Gao-binge injury in Irf3−/− mice was associated with an increased ratio of Ly6Clow (restorative) to Ly6Chigh (inflammatory) cells compared to C57BL/6 and Irf3S1/S1 mice. Reduced ratios of Ly6Clow/Ly6Chigh in C57BL/6 and Irf3S1/S1 mice were associated with increased apoptosis in the Ly6Clow population in response to Gao-binge. Activation of primary macrophage cultures with Poly (I:C) induced translocation of IRF3 to the mitochondria, where it associated with Bax and activated caspases 3 and 9, processes indicative of activation of the RIPA pathway. Taken together, these data identify that the non-transcriptional function of IRF3 plays an important role in modulating the innate immune environment in response to Gao-binge ethanol exposure, via regulation of immune cell apoptosis. Alcohol consumption is a leading cause of preventable morbidity and mortality worldwide.1 The pathogenesis of alcohol-related liver disease (ALD) is initially characterized by steatosis, progressing in some individuals to fibrosis and cirrhosis. Alcoholic hepatitis (AH), a severe inflammatory condition, with extensive infiltration of leukocytes and hepatocellular injury, can occur at any stage of disease progression; 28-day mortality rates range from 25–35%.1–2 The development of AH is a complex process involving both parenchymal and non-parenchymal cells resident in the liver, as well as the recruitment of immune cells to the liver in response to damage and inflammation.3 Current therapies, focusing on suppressing inflammation, are ineffective in many patients with severe AH and outcomes remain poor.4 
Sorafenib is first-line standard of care for patients with advanced hepatocellular carcinoma (HCC), yet it confers limited survival benefit. Therefore, we aimed to compare clinical outcomes of sorafenib combined with concurrent conventional transarterial chemoembolization (cTACE) vs. sorafenib alone in patients with advanced HCC. In this investigator-initiated, multicenter, phase III trial, patients were randomized to receive sorafenib alone (Arm S, n = 169) or in combination with cTACE on demand (Arm C, n = 170). Sorafenib was started within 3 days and cTACE within 7–21 days of randomization. The primary endpoint was overall survival (OS). For Arms C and S, the median OS was 12.8 vs. 10.8 months (hazard ratio [HR] 0.91; 90% CI 0.69–1.21; p = 0.290); median time to progression, 5.3 vs. 3.5 months (HR 0.67; 90% CI 0.53–0.85; p = 0.003); median progression-free survival, 5.2 vs. 3.6 months (HR 0.73; 90% CI 0.59–0.91; p = 0.01); and tumor response rate, 60.6% vs. 47.3% (p = 0.005). For Arms C and S, serious (grade ≥3) adverse events occurred in 33.3% vs. 19.8% (p = 0.006) of patients and included increased alanine aminotransferase levels (20.3% vs. 3.6%), hyperbilirubinemia (11.8% vs. 3.0%), ascites (11.8% vs. 4.2%), thrombocytopenia (7.2% vs. 1.2%), anorexia (7.2% vs. 1.2%), and hand-foot skin reaction (10.5% vs. 11.4%). A post hoc subgroup analysis compared OS in Arm C patients (46.4%) receiving ≥2 cTACE sessions to Arm S patients (18.6 vs. 10.8 months; HR 0.58; 95% CI 0.40–0.82; p = 0.006). Compared with sorafenib alone, sorafenib combined with cTACE did not improve OS in patients with advanced HCC. However, sorafenib combined with cTACE significantly improved time to progression, progression-free survival, and tumor response rate. Sorafenib alone remains the first-line standard of care for patients with advanced HCC. Worldwide, primary liver cancer, including hepatocellular carcinoma (HCC), is the fifth most common cancer and the second leading cause of cancer-related mortality.1 HCC prognosis remains poor because of the underlying chronic liver disease; late diagnosis, often at advanced stages of disease; and frequent recurrence/progression after treatment.2,3 
Genetic hemochromatosis is mainly related to the homozygous p.Cys282Tyr (C282Y) mutation in the HFE gene, which causes hepcidin deficiency. Its low penetrance suggests the involvement of cofactors that modulate its expression. We aimed to describe the evolution of disease presentation and of non-genetic factors liable to impact hepcidin production in the long term. Clinical symptoms, markers of iron load, and risk factors according to the year of diagnosis were recorded over 30 years in a cohort of adult C282Y homozygotes. A total of 2,050 patients (1,460 probands [804 males and 656 females] and 542 relatives [244 males and 346 females]) were studied. Over time: (i) the proband-to-relative ratio remained roughly stable; (ii) the gender ratio tended towards equilibrium among probands; (iii) age at diagnosis did not change among males and increased among females; (iv) the frequency of diabetes and hepatic fibrosis steadily decreased while that of chronic fatigue and distal joint symptoms remained stable; (v) transferrin saturation, serum ferritin and the amount of iron removed decreased; and (vi) the prevalence of excessive alcohol consumption decreased while that of patients who were overweight increased. Tobacco smoking was associated with increased transferrin saturation. Genetic testing did not alter the age at diagnosis, which contrasts with the dramatic decrease in iron load in both genders. Tobacco smoking could be involved in the extent of iron loading. Besides HFE testing, which enables the diagnosis of minor forms of the disease, the reduction of alcohol consumption and the increased frequency of overweight patients may have played a role in the decreased long-term iron load, as these factors are likely to improve hepcidin production. In Caucasians, most cases of genetic iron overload result from the homozygous genotype for the HFE p.Cys282Tyr (C282Y) mutation, which leads to impaired production of hepcidin, the key regulator of systemic iron.1 This results in increased iron efflux from cells, mainly from enterocytes and macrophages, and consequently in increased serum iron levels and transferrin saturation, leading to abnormal iron deposits in various parenchyma, especially the liver.1 The clinical penetrance of the HFE C282Y homozygous genotype is fairly low, estimated at 30% among males and 1% among females.2 Thus, C282Y homozygosity is a necessary, although insufficient, condition for developing clinical hemochromatosis. This suggests that genetic and environmental cofactors modulate its expression in terms of both iron load and organ damage. Genetic polymorphisms have been suggested as phenotypic modifiers but none has been found to be frequent enough to explain this low penetrance.3,4 This suggests that non-genetic factors are likely to play a key role in disease presentation, especially those liable to interfere with hepcidin production, i.e. alcohol consumption, being overweight and tobacco smoking. 
The World Health Organization (WHO) established targets to eliminate hepatitis C virus (HCV) infection as a public health threat by 2030. Evidence that HCV treatment can lower viraemic prevalence among people who inject drugs (PWID) is limited. Broad accessibility of direct-acting antiviral (DAA) therapy in Australia, since March 2016, provides an opportunity to assess the efficacy of these treatments at a population level in a real-world setting. Data from Australia’s annual bio-behavioural surveillance examined treatment uptake and estimated viraemic prevalence among PWID attending needle syringe programs nationally between 2015 and 2017. Multivariate logistic regression identified variables independently associated with HCV treatment among those considered eligible (anti-HCV positive excluding HCV RNA negative with no self-reported history of HCV treatment) in 2017. Annual samples ranged from 1,995–2,380 PWID. Anti-HCV prevalence declined from 57% (2015) to 49% (2017, χ2 p trend <0.001), with 40–56% of anti-HCV positive respondents providing sufficient sample for HCV RNA testing. Between 2015 and 2017, treatment uptake among those eligible increased from 10% to 41% (χ2 p trend <0.001) and viraemic prevalence among the overall sample declined from 43% to 25% (χ2 p trend <0.001). In multivariable analysis, older age (≥50 years adjusted odds ratio [aOR] 1.82; 95% CI 1.09–3.06;p = 0.023 and 44–49 years aOR 1.75; 95% CI 1.03–3.00;p = 0.038 vs. ≤37 years) and history of opioid substitution therapy (aOR 2.06; 95% CI 1.30–3.26; p = 0.002) were independently associated with treatment. This study confirms PWID are willing to initiate treatment when HCV DAA therapy is available and provides population-level evidence of a decline in viraemic prevalence among people most at risk of ongoing HCV transmission. Scaled up surveillance and monitoring are required to evaluate progress toward WHO HCV elimination goals. The treatment landscape for chronic hepatitis C virus (HCV) infection has transformed over the past five years, with the development of all-oral direct-acting antiviral (DAA) regimens with minimal toxicity and cure rates above 95%.1 In keeping with the optimism surrounding these new treatments, the World Health Organisation (WHO) recently developed targets with a goal to eliminate viral hepatitis as a public health threat by 2030, including 80% of the eligible chronic HCV population treated, 65% reduction in liver-related mortality and 80% reduction in HCV incidence by 2030.2 
Emricasan, an oral pan-caspase inhibitor, decreased portal pressure in experimental cirrhosis and in patients with cirrhosis and portal pressure (assessed by the hepatic venous pressure gradient [HVPG]) ≥12 mmHg. We aimed to confirm these results in a randomized, placebo-controlled, double blind study. Multicenter study including 263 patients with cirrhosis due to non-alcoholic steatohepatitis (NASH) and baseline HVPG ≥12 mmHg randomized 1:1:1:1 to emricasan 5 (n=65), 25 (n=65), 50 (n=66) mg or placebo (n=67) orally twice daily for up to 48 weeks. Primary endpoint was change in HVPG (ΔHVPG) at week 24. Secondary endpoints were changes in biomarkers (aminotransferases, caspases, cytokeratins) and development of liver-related outcomes. There were no significant differences in ΔHVPG for any emricasan dose vs. placebo (-0.21, -0.45, -0.58 mmHg, respectively) adjusted by baseline HVPG, compensation status, and non-selective beta-blocker use. Compensated subjects (n=201 [76%]) tended to have a greater decrease in HVPG (emricasan all vs. placebo, p=0.06), the decrease being greater in those with higher baseline HVPG (p=0.018), with a significant interaction between baseline HVPG (continuous, p=0.024; dichotomous at 16 mmHg [median], p=0.013) and treatment. Biomarkers decreased significantly with emricasan at week 24 but returned to baseline levels by week 48. New or worsening decompensating events (∼10% over median exposure of 337 days), progression in MELD and Child Pugh scores, and treatment-emergent adverse events were similar among treatment groups. Despite reduction in biomarkers indicating target engagement, emricasan was not associated with improvement in HVPG or clinical outcomes in patients with NASH cirrhosis and severe portal hypertension. Compensated subjects with higher baseline HVPG had evidence of a small treatment effect. Emricasan treatment appeared safe and well-tolerated. Cirrhosis due to non-alcoholic fatty liver disease is fast becoming a major disease burden worldwide (1). In fact, non-alcoholic steatohepatitis (NASH) is a leading indication for liver transplantation in the U.S. (2). Portal hypertension is a key driver of the major complications of cirrhosis that define decompensation, the latter being the main predictor of death in cirrhosis. (3, 4) A portal pressure (determined by the hepatic venous pressure gradient [HVPG]) ≥10 or ≥12 mmHg is the strongest predictor of decompensation in patients with mostly viral-induced cirrhosis (4), but also in patients with NASH cirrhosis (5). Importantly, decreases in HVPG are associated with lower rates of decompensation and death in compensated and decompensated patients (6). There are currently no approved therapies that can ameliorate the abnormalities leading to significant portal hypertension in NASH cirrhosis patients. 
To date, studies into the natural history of alcohol-related liver disease (ALD) have lacked long-term follow-up, large numbers of participants, or both. We performed a systematic review to summarise studies that describe the natural history of histologically proven ALD. PubMed and Medline were searched for relevant studies according to pre-specified criteria. Data were extracted to describe the prevalence of ALD, histological progression of disease and mortality. Single-proportion meta-analysis was used to combine data from studies regarding rates of progression or mortality. Thirty-seven studies were included, reporting data from 7,528 participants. Amongst cohorts of hazardous drinkers, on average 15% had normal histological appearance, 27% had hepatic steatosis, 24% had steatohepatitis and 26% had cirrhosis. The annualised rates of progression of pre-cirrhotic disease to cirrhosis were 1% (0–8%) for patients with normal histology, 3% (2–4%) for hepatic steatosis, 10% (6–17%) for steatohepatitis and 8% (3–19%) for fibrosis. Annualised mortality was 6% (4–7%) in patients with steatosis and 8% (5–13%) in cirrhosis. In patients with steatohepatitis on biopsy a marked difference was seen between inpatient cohorts (annual mortality 15%, 8–26%) and mixed cohorts of inpatients and outpatients (annual mortality 5%, 2–10%). Only in steatosis did non-liver-related mortality exceed liver-specific causes of mortality (5% per year vs. 1% per year). These data confirm the observation that alcohol-related hepatic steatohepatitis requiring admission to hospital is the most dangerous subtype of ALD. Alcohol-related steatosis is not a benign condition as it is associated with significant risk of mortality. Alcohol-related liver disease (ALD) is common throughout the world.1 ALD is a leading cause of liver-related morbidity and mortality,2 and a frequent cause of death amongst people of working age.3 Hazardous drinking – consumption of alcohol at levels that are likely to cause harm – is prevalent globally.4 This is a prerequisite for the development of ALD, which covers a spectrum of disease from steatosis, steatohepatitis to cirrhosis. Earlier stages of disease are considered reversible with abstinence from alcohol.5 Liver-specific morbidity and mortality is only considered relevant in patients with more advanced disease. 
Radiofrequency ablation (RFA) is an effective treatment for single hepatocellular carcinoma (HCC) ≤3 cm. Disease recurrence is common, and in some patients will occur outside transplant criteria. We aimed to assess the incidence and risk factors for recurrence beyond Milan criteria in potentially transplantable patients treated with RFA as first-line therapy. We performed a retrospective cohort study of potentially transplantable patients with new diagnoses of unifocal HCC ≤3 cm that underwent RFA as first-line therapy between 2000-2015. We defined potentially transplantable patients as those aged <70 years without any comorbidities that would preclude transplant surgery. Incidence of recurrence beyond Milan criteria was compared across 2 groups according to HCC diameter at the time of ablation: (HCC ≤2 cm vs. HCC >2 cm). Competing risks Cox regression was used to identify predictors of recurrence beyond Milan criteria. We included 301 patients (167 HCC ≤2 cm and 134 HCC >2 cm). Recurrence beyond Milan criteria occurred in 36 (21.6%) and 47 (35.1%) patients in the HCC ≤2 cm and the HCC >2 cm groups, respectively (p = 0.01). The 1-, 3- and 5-year actuarial survival rates after RFA were 98.2%, 86.2% and 79.0% in the HCC ≤2 cm group vs. 93.3%, 77.6% and 70.9% in the HCC >2 cm group (p = 0.01). Tumor size >2 cm (hazard ratio 1.94; 95% CI 1.25–3.02) and alpha-fetoprotein levels at the time of ablation (100–1,000 ng/ml: hazard ratio 2.05; 95% CI 1.10–3.83) were found to be predictors of post-RFA recurrence outside Milan criteria. RFA for single HCC ≤3 cm provides excellent short- to medium-term survival. However, we identified patients at higher risk of recurrence beyond Milan criteria. For these patients, liver transplantation should be considered immediately after the first HCC recurrence following RFA. The optimal approach for transplantable patients with early unifocal HCC is unclear. Small HCC (BCLC-0 or A) can be treated with ablation, liver resection, or liver transplantation (LT) as first-line therapies.1 Although the results of LT are excellent, most jurisdictions currently only assign model for end-stage liver disease (MELD) exception points to patients with larger HCC based on the belief that if resection or radiofrequency ablation (RFA) do not work, transplantation can be undertaken as a second curative procedure.2 
Accurate evaluation of renal function in patients with liver cirrhosis is critical for clinical management. However, there are still discrepancies between the measured glomerular filtration rate (mGFR) and creatinine-based estimated GFR (eGFR). In this study, we compared the performance of 2 common eGFR measurements with mGFR and evaluated the impact of low muscle mass on overestimation of renal function in patients with cirrhosis. This study included 779 consecutive cirrhotic patients who underwent 51Cr-ethylenediamine tetra acetic acid (EDTA) (as a mGFR) and abdominal computed tomography (CT). The eGFR was calculated using creatinine or cystatin C. Muscle mass was assessed in terms of the total skeletal muscle at L3 level using CT. Modification of diet in renal disease (MDRD)-eGFR was overestimated in 47% of patients. A multivariate analysis showed that female sex (adjusted odds ratio [aOR] 4.91), Child B and C vs. A (aOR 1.69 and 1.84) and skeletal muscle mass (aOR 0.89) were independent risk factors associated with overestimation. Interestingly, the effect of skeletal muscle mass on overestimation varied based on sex. Decreased muscle mass significantly enhanced the risk of overestimation of MDRD-eGFR in male patients, but not in female patients. Cystatin C-based eGFR showed a better correlation with mGFR than MDRD-eGFR; it was also better at predicting overall survival and the incidence of acute kidney injury than MDRD-eGFR. The risk factors associated with overestimation included female sex, impaired liver function, and decreased muscle mass in males. In particular, eGFR in male patients with sarcopenia should be carefully interpreted. Creatinine-based eGFR was overestimated more often than cystatin C-based eGFR, with overestimation of eGFR closely related to poor prognostic performance. Accurate evaluation of renal function is a prerequisite for the management of patients with liver cirrhosis.1 It facilitates diagnostic and therapeutic assessment, prognostic evaluation and indication for liver transplantation.2 It is well known that cirrhosis is often accompanied by decreased renal function, resulting in poor outcomes even in stage I acute kidney injury (AKI).3,4 
Natural killer (NK) cells are known to exert strong antiviral activity. Killer cell lectin-like receptor subfamily G member 1 (KLRG1) is expressed by terminally differentiated NK cells and KLRG1-expressing lymphocytes are known to expand following chronic viral infections. We aimed to elucidate the previously unknown role of KLRG1 in the pathogenesis of chronic hepatitis B (CHB). KLRG1+ NK cells were taken from the blood and liver of healthy individuals and patients with CHB. The phenotype and function of these cells was assessed using flow cytometry and in vitro stimulation. Patients with CHB had a higher frequency of KLRG1+ NK cells compared to healthy controls (blood 13.4 vs. 2.3%, p <0.0001 and liver 23.4 vs. 2.6%, p <0.01). KLRG1+ NK cells were less responsive to K562 and cytokine stimulation, but demonstrated enhanced cytotoxicity (9.0 vs. 4.8%, p <0.05) and IFN-γ release (8.0 vs. 1.5%, p <0.05) via antibody dependent cellular cytotoxicity compared to their KLRG1− counterparts. KLRG1+ NK cells possessed a mature phenotype, demonstrating stronger cytolytic activity and IFN-γ secretion against hepatic stellate cells (HSCs) than KLRG1− NK cells. Moreover, KLRG1+ NK cells more effectively induced primary HSC apoptosis in a TRAIL-dependent manner. Increased KLRG1+ NK cell frequency in the liver and blood was associated with lower fibrosis stage (F0/F1) in patients with CHB. Finally, the expression of CD44, degranulation and IFN-γ production were all increased in KLRG1+ NK cells following stimulation with osteopontin, the CD44 ligand, suggesting that HSC-derived osteopontin may cause KLRG1+ NK cell activation. KLRG1+ NK cells likely play an antifibrotic role during the natural course of CHB infection. Harnessing this antifibrotic function may provide a novel therapeutic approach to treat liver fibrosis in patients with CHB. Natural killer (NK) cells are a primary effector population responsible for the innate immune response to viral infection, representing approximately 5–15% of blood and 40–60% of hepatic lymphocytes.1,2 While NK cells play a role in controlling hepatitis B virus (HBV) infection,3–9 they act as a double-edged sword, contributing to liver injury through sustained activation and tissue damage.10–13 
As hepatitis B virus (HBV) spreads through the infected liver it is simultaneously secreted into the blood. HBV-susceptible in vitro infection models do not efficiently amplify viral progeny or support cell-to-cell spread. We sought to establish a cell culture system for the amplification of infectious HBV from clinical specimens. An HBV-susceptible sodium-taurocholate cotransporting polypeptide-overexpressing HepG2 cell clone (HepG2-NTCPsec+) producing high titers of infectious progeny was selected. Secreted HBV progeny were characterized by native gel electrophoresis and electron microscopy. Comparative RNA-seq transcriptomics was performed to quantify the expression of host proviral and restriction factors. Viral spread routes were evaluated using HBV entry- or replication inhibitors, visualization of viral cell-to-cell spread in reporter cells, and nearest neighbor infection determination. Amplification kinetics of HBV genotypes B-D were analyzed. Infected HepG2-NTCPsec+ secreted high levels of large HBV surface protein-enveloped infectious HBV progeny with typical appearance under electron microscopy. RNA-seq transcriptomics revealed that HBV does not induce significant gene expression changes in HepG2-NTCPsec+, however, transcription factors favoring HBV amplification were more strongly expressed than in less permissive HepG2-NTCPsec−. Upon inoculation with HBV-containing patient sera, rates of infected cells increased from 10% initially to 70% by viral spread to adjacent cells, and viral progeny and antigens were efficiently secreted. HepG2-NTCPsec+ supported up to 1,300-fold net amplification of HBV genomes depending on the source of virus. Viral spread and amplification were abolished by entry and replication inhibitors; viral rebound was observed after inhibitor discontinuation. The novel HepG2-NTCPsec+ cells efficiently support the complete HBV life cycle, long-term viral spread and amplification of HBV derived from patients or cell culture, resembling relevant features of HBV-infected patients. Despite vaccination, chronic hepatitis B (CHB) has remained among the most widespread, life-shortening infectious diseases. Two billion people worldwide have been infected with the hepatitis B virus (HBV), including 257 million chronic carriers. Up to 30% of chronically HBV-infected adults will develop liver cirrhosis or hepatocellular carcinoma, accounting for 887,000 deaths annually.1 Reverse transcriptase inhibitors and interferon-alpha can control viral replication and prevent CHB progression. However, these therapies do not act on HBV genomes in host cell nuclei that persist as covalently closed circular DNA (cccDNA). Thus, current treatment regimens are not curative, requiring lifelong therapy. 
Liver adenomatosis (LA) is characterized by the presence of at least 10 hepatocellular adenomas (HCAs), but the natural history of this rare liver disorder remains unclear. Thus, we aimed to reappraise the natural history and the risk of complications in a cohort of patients with at least 10 HCAs. We analyzed the natural history of 40 patients with LA, excluding glycogen storage disorders, in a monocentric cohort. Pathological examination was performed, with immunostaining and molecular biology carried out on surgical specimens or liver biopsies. Forty patients (36 female) were included with a median follow-up of 10.6 (1.9–26.1) years. Six (15%) patients had familial LA, all with germline HNF1A mutations. Median age at diagnosis was 39 (9–55) years. Thirty-three (94%) women had a history of oral contraception, and 29 (81%) women had a pregnancy before LA diagnosis. Overall, thirty-seven (93%) patients underwent surgery at diagnosis. Classification of HCAs showed 46% of patients with HNF1A-mutated HCA, 31% with inflammatory HCA, 3% with sonic hedgehog HCA, 8% with unclassified HCA. Only 15% of the patients demonstrated a “mixed LA” with different HCA subtypes. Hepatic complications were identified in 7 patients: 1 patient (3%) died from recurrent hepatocellular carcinoma after liver transplantation; 6 (15%) had hemorrhages, of which 5 occurred at diagnosis, with 1 fatal case during pregnancy, and 2 occurred in male patients with familial LA. Four patients (10%) had repeated liver resections. Finally, 4 (10%) patients developed extrahepatic malignancies during follow-up. The diversity in HCA subtypes, as well as the occurrence of bleeding and malignant transformation during long-term follow-up, underline the heterogeneous nature of LA, justifying close and specific management. In patients with germline HNF1A mutation, familial LA occurred equally frequently in males and females, with a higher rate of bleeding in male patients. Hepatocellular adenomas (HCAs) are rare benign liver tumors that increased in frequency after the wide prescription of oral contraceptive pills in the 1960s. Currently the prevalence of HCAs ranges between 1 and 4 per 100,000 individuals.1 Owing to its estrogen-dependence, the female to male ratio is 10:1 and HCAs predominantly affect young women around 30–40 years old.2,3 The 2 major risks of HCAs are symptomatic hemorrhage and malignant transformation, which are estimated to occur in 15% and 5% of cases in surgical series, respectively.3,4 Lesion size greater >5 cm is a risk factor for both complications, while male sex is a risk factor for malignant transformation.3,5 
Inherited abnormalities in apolipoprotein E (ApoE) or low-density lipoprotein receptor (LDLR) function result in early onset cardiovascular disease and death. Currently, the only curative therapy available is liver transplantation. Hepatocyte transplantation is a potential alternative; however, physiological levels of hepatocyte engraftment and repopulation require transplanted cells to have a competitive proliferative advantage of over host hepatocytes. Herein, we aimed to test the efficacy and safety of a novel preparative regimen for hepatocyte transplantation. Herein, we used an ApoE-deficient mouse model to test the efficacy of a new regimen for hepatocyte transplantation. We used image-guided external-beam hepatic irradiation targeting the median and right lobes of the liver to enhance cell transplant engraftment. This was combined with administration of the hepatic mitogen GC-1, a thyroid hormone receptor-β agonist mimetic, which was used to promote repopulation. The non-invasive preparative regimen of hepatic irradiation and GC-1 was well-tolerated in ApoE−/− mice. This regimen led to robust liver repopulation by transplanted hepatocytes, which was associated with significant reductions in serum cholesterol levels after transplantation. Additionally, in mice receiving this regimen, ApoE was detected in the circulation 4 weeks after treatment and did not induce an immunological response. Importantly, the normalization of serum cholesterol prevented the formation of atherosclerotic plaques in this model. Significant hepatic repopulation and the cure of dyslipidemia in this model, using a novel and well-tolerated preparative regimen, demonstrate the clinical potential of applying this method to the treatment of inherited metabolic diseases of the liver. Lipid homeostasis requires the uptake of chylomicron remnants and other lipoprotein particles by hepatocytes via binding of apolipoprotein E (ApoE) and apolipoprotein B-100 (ApoB-100) lipoprotein ligands to the low-density lipoprotein (LDL) receptor (LDLR) family.1 Inherited loss-of-function mutations in the LDLR, gain-of-function mutations in PCSK9 or ApoB-100 and/or ApoE deficiency all lead to familial hypercholesterolemia (FH) with elevated levels of plasma LDL cholesterol, the main cause of atherosclerotic disease.1–6 
Congenital hepatic fibrosis (CHF) is a genetic liver disease resulting in abnormal proliferation of cholangiocytes and progressive hepatic fibrosis. CHF is caused by mutations in the PKHD1 gene and the subsequent dysfunction of the protein it encodes, fibrocystin. However, the underlying molecular mechanism of CHF, which is quite different from liver cirrhosis, remains unclear. This study investigated the molecular mechanism of CHF pathophysiology using a genetically engineered human induced pluripotent stem (iPS) cell model to aid the discovery of novel therapeutic agents for CHF. PKHD1-knockout (PKHD1-KO) and heterozygously mutated PKHD1 iPS clones were established by RNA-guided genome editing using the CRISPR/Cas9 system. The iPS clones were differentiated into cholangiocyte-like cells in cysts (cholangiocytic cysts [CCs]) in a 3D-culture system. The CCs were composed of a monolayer of cholangiocyte-like cells. The proliferation of PKHD1-KO CCs was significantly increased by interleukin-8 (IL-8) secreted in an autocrine manner. IL-8 production was significantly elevated in PKHD1-KO CCs due to mitogen-activated protein kinase pathway activation caused by fibrocystin deficiency. The production of connective tissue growth factor (CTGF) was also increased in PKHD1-KO CCs in an IL-8-dependent manner. Furthermore, validation analysis demonstrated that both the serum IL-8 level and the expression of IL-8 and CTGF in the liver samples were significantly increased in patients with CHF, consistent with our in vitro human iPS-disease model of CHF. Loss of fibrocystin function promotes IL-8-dependent proliferation of, and CTGF production by, human cholangiocytes, suggesting that IL-8 and CTGF are essential for the pathogenesis of CHF. IL-8 and CTGF are candidate molecular targets for the treatment of CHF. Congenital hepatic fibrosis (CHF) is a rare genetic liver disease (1/20,000 births) characterized by ductal plate malformation during bile duct development and progressive hepatic fibrosis. CHF is also frequently associated with autosomal recessive polycystic kidney disease.1 Liver transplantation is necessary for the treatment of patients with progressive CHF and a severe phenotype. There is lots of pathological evidence to indicate that the mechanism of fibrosis in CHF is quite different from liver cirrhosis due to chronic hepatitis: in patients with CHF, neither necroinflammatory changes in hepatocytes nor hepatic stellate cell activation are observed. Fibrotic change in the CHF liver is limited to periportal areas of the hepatic lobes and is not observed around the central vein.2 The pathophysiology of ductal plate malformation and progressive fibrosis in the CHF liver remains unclear. 
Several studies have shown that chronic hepatitis C (CHC) infection has a negative impact on kidney function, as well as survival, in patients with chronic kidney disease (CKD) or on hemodialysis. The aim of this nationwide registry study was to describe renal disease in Swedish patients with CHC. In the present study, patients were identified for CHC (B18.2) and CKD (N18) according to the International Classification of Diseases (ICD)-10 in the nationwide Swedish inpatient care day surgery (1997–2013) and non-primary outpatient care (2001–2013) patient registries. Hemodialysis was defined using the procedure code in the non-primary outpatient care. For each patient, up to five non-CHC diagnosed age/sex/place of residency-matched comparators were drawn from the general population at the time of diagnosis. Follow-up started at the date of CHC diagnosis and patients accrued person-time until, whichever came first, death, emigration or December 31st, 2013. Between 2001 and 2013, 42,522 patients received a CHC diagnosis. Of these patients, 2.5% (1,077/45,222) were diagnosed with CKD during 280,123 person-years, compared with 0.7% (1,454/202,694) in the matched general population comparators (1,504,765 person-years), resulting in a standardized incidence ratio (SIR) of 4.0. There was a 3.3–7.0-fold risk of patients with CHC requiring hemodialysis. Overall, 17% of patients with CHC receiving hemodialysis were treated for CHC; 24% in the treated cohort died compared with 56% of the untreated cohort (p <0.0001), with antiviral treatment improving survival with an odds ratio of 3.901 (p = 0.001). The results from this nationwide registry study showed that patients with CHC are at a higher risk of developing CKD. Furthermore, hepatitis C treatment seemed to improve survival for patients with CHC on hemodialysis compared with untreated patients. Hepatitis C virus (HCV) infection is a major cause of viral hepatitis with a global seroprevalence estimated to be greater than 185 million.1 Approximately 75% to 85% of patients with HCV infection develop a chronic hepatitis C (CHC) infection.2 In addition to the direct negative impact of the virus on the liver, where more than 40% of CHC infections lead to liver cirrhosis after 30 years,3 CHC infection is also associated with extrahepatic manifestations including kidney disease, most commonly membranoproliferative glomerulonephritis with or without cryoglobulinemia.4 
A better identification of factors predicting death is needed in alcoholic hepatitis (AH). Acute-on-chronic liver failure (ACLF) occurs during the course of liver disease and can be identified when AH is diagnosed (prevalent ACLF [pACLF]) or during follow-up (incidental ACLF [iACLF]). This study analyzed the impact of ACLF on outcomes in AH and the role of infection on the onset of ACLF and death. Patients admitted from July 2006 to July 2015 suffering from biopsy-proven severe (s)AH with a Maddrey discriminant function (mDF) ≥32 were included. Infectious episodes, ACLF, and mortality were assessed during a 168-day follow-up period. Results were validated on an independent cohort. One hundred sixty-five patients were included. Mean mDF was 66.3 ± 20.7 and mean model for end-stage liver disease score was 26.8 ± 7.4. The 28-day cumulative incidence of death (CID) was 31% (95% CI 24–39%). Seventy-nine patients (47.9%) had pACLF. The 28-day CID without pACLF and with pACLF-1, pACLF-2, and pACLF-3 were 10.4% (95% CI 5.1–18.0), 30.8% (95% CI 14.3–49.0), 58.3% (95% CI 35.6–75.5), and 72.4% (95% CI 51.3–85.5), respectively, p <0.0001. Twenty-nine patients (17.5%) developed iACLF. The 28-day relative risk of death in patients developing iACLF was 41.87 (95% CI 5.2–335.1; p <0.001). A previous infection was the only independent risk factor for developing iACLF during the follow-up. Prevalence, incidence, and impact on prognosis of ACLF were confirmed in a validation cohort of 97 patients with probable sAH. ACLF is frequent during the course of sAH and is associated with high mortality. Infection strongly predicts the development of ACLF in this setting. Severe alcoholic hepatitis (sAH) is a complication of alcoholic liver disease associated with high mortality.1–4 Several studies have shown that in patients suffering from sAH, with a Maddrey discriminant function (mDF) score of more than 32, the use of corticosteroids (CS) significantly improves survival.2,5–8 Nevertheless, the long-term benefit of CS on survival of patients with sAH has not been clearly established. A recent large randomized controlled study (the STOPAH study) provided evidence that 90-day mortality for sAH was about 30% with or without this treatment.9 A more accurate stratification system is needed to help clinicians identify patients at high risk of death in sAH.10 
Genetic variability in the hepatitis B virus X gene (HBx) is frequently observed and is associated with hepatocellular carcinoma (HCC) progression. However, a genotype classification based on the full-length HBx sequence and the impact of genotypes on hepatitis B virus (HBV)-related HCC prognosis remain unclear. We therefore aimed to perform this genotype classification and assess its clinical impact. We classified the genotypes of the full-length HBx gene through sequencing and a cluster analysis of HBx DNA from a cohort of patients with HBV-related HCC, which served as the primary cohort (n = 284). Two independent HBV-related HCC cohorts, a validation cohort (n = 171) and a serum cohort (n = 168), were used to verify the results. Protein microarray assay analysis was performed to explore the underlying mechanism. In the primary cohort, the HBx DNA was classified into 3 genotypes: HBx-EHBH1, HBx-EHBH2, and HBx-EHBH3. HBx-EHBH2 (HBx-E2) indicated better recurrence-free survival and overall survival for patients with HCC. HBx-E2 was significantly correlated with the absence of liver cirrhosis, a small tumor size, a solitary tumor, complete encapsulation and Barcelona Clinic Liver Cancer (BCLC) stage A-0 tumors. Additionally, HBx-E2 served as a significant prognostic factor for patients with BCLC stage B HCC after hepatectomy. Mechanistically, HBx-E2 is unable to promote proliferation in HCC cells and normal hepatocytes. It also fails to activate the Janus kinase 1 (JAK1)/signal transducer and activator of transcription 3 (STAT3)/STAT5 pathway. Our study identifies a novel HBx genotype that is unable to promote the proliferation of HCC cells and suggests a potential marker to preoperatively predict the prognosis of patients with BCLC stage B, HBV-associated, HCC. Chronic hepatitis B virus (HBV) infection is the dominant risk factor for the development of hepatocellular carcinoma (HCC) in the Asia-Pacific region due to inflammation, cirrhosis and direct viral oncogenic factors.1 The hepatitis B virus X gene (HBx) gene, 1 of 4 open reading frames of HBV, encodes a 17 kDa protein and has been shown to be strongly linked to the development of HCC.2 As a transactivator, the HBx protein activates various viral and cellular promoters and enhancers via protein–protein interactions and affects various signal-transduction pathways, such as the Janus kinase (JAK)/signal transducer and activator of transcription (STAT), Wnt/β-catenin, nuclear factor-κB (NF-κB) and protein kinase B/Akt pathways.3,4 Furthermore, HBx has been shown to modulate a wide range of cellular functions, including proliferation, the cell cycle, apoptosis, autophagy, metastasis, and metabolism, which lead to the development of HCC.5 
Heat shock protein (Hsp) 72 is a molecular chaperone that has broad cytoprotective functions and is upregulated in response to stress. To determine its hepatic functions, we studied its expression in human liver disorders and its biological significance in newly generated transgenic animals. Double transgenic mice overexpressing Hsp72 (gene Hspa1a) under the control of a tissue-specific tetracycline-inducible system (Hsp72-LAP mice) were produced. Acute liver injury was induced by a single injection of acetaminophen (APAP). Feeding with either a methionine choline-deficient (MCD; 8 weeks) or a 3,5-diethoxycarbonyl-1,4-dihydrocollidine-supplemented diet (DDC; 12 weeks) was used to induce lipotoxic injury and Mallory–Denk body (MDB) formation, respectively. Primary hepatocytes were treated with palmitic acid. Patients with non-alcoholic steatohepatitis and chronic hepatitis C infection displayed elevated HSP72 levels. These levels increased with the extent of hepatic inflammation and HSP72 expression was induced after treatment with either interleukin (IL)-1β or IL-6. Hsp72-LAP mice exhibited robust, hepatocyte-specific Hsp72 overexpression. Primary hepatocytes from these animals were more resistant to isolation-induced stress and Hsp72-LAP mice displayed lower levels of hepatic injury in vivo. Mice overexpressing Hsp72 had fewer APAP protein adducts and were protected from oxidative stress and APAP-/MCD-induced cell death. Hsp72-LAP mice and/or hepatocytes displayed significantly attenuated Jnk activation. Overexpression of Hsp72 did not affect steatosis or the extent of MDB formation. Our results demonstrate that HSP72 induction occurs in human liver disease, thus, HSP72 represents an attractive therapeutic target owing to its broad hepatoprotective functions. The liver has an important role in protein and lipid metabolism. It is a key hub for fatty acid synthesis and lipid circulation, while also producing most plasma proteins.1 This tremendous metabolic activity results in severe oxidative stress that becomes apparent in multiple liver disorders. For example, protein misfolding with the formation of cytoplasmic aggregates, also referred to as Mallory–Denk bodies (MDBs), is a characteristic feature of specific liver diseases, such as alcoholic and non-alcoholic steatohepatitis (NASH).2 Although the exact pathogenesis of MDB formation remains unknown, lipotoxicity, which is a key feature of both alcoholic liver disease and NASH, appears to have an important role.1,3 Another source of proteotoxic stress stems from the fact that most xenobiotics are detoxified by the liver. As a prime example, overdose of the common analgesic drug acetaminophen (APAP) leads to depletion of the endogenous antioxidant glutathione and to the formation of APAP-protein adducts, which are key mediators of APAP hepatotoxicity.4 
Treatment of liver cancer remains challenging because of a paucity of drugs that target critical dependencies. Sorafenib is a multikinase inhibitor that is approved as the standard therapy for patients with advanced hepatocellular carcinoma, but it only provides limited survival benefit. In this study we aimed to identify potential combination therapies to improve the clinical response to sorafenib. To investigate the cause of the limited therapeutic effect of sorafenib, we performed a CRISPR-Cas9 based synthetic lethality screen to search for kinases whose knockout synergizes with sorafenib. Synergistic effects of sorafenib and selumetinib on cell apoptosis and phospho-ERK (p-ERK) were analyzed by caspase-3/7 apoptosis assay and western blot, respectively. p-ERK was measured by immunochemical analysis using a tissue microarray containing 78 liver cancer specimens. The in vivo effects of the combination were also measured in two xenograft models. We found that suppression of ERK2 (MAPK1) sensitizes several liver cancer cell lines to sorafenib. Drugs inhibiting the MEK (MEK1/2 [MAP2K1/2]) or ERK (ERK1/2 [MAPK1/3]) kinases reverse unresponsiveness to sorafenib in vitro and in vivo in a subset of liver cancer cell lines characterized by high levels of active p-ERK, through synergistic inhibition of ERK kinase activity. Our data provide a combination strategy for treating liver cancer and suggest that tumors with high basal p-ERK levels, which are seen in approximately 30% of liver cancers, are most likely to benefit from such combinatorial treatment. Liver cancer is one of the most frequent malignancies and the second leading cause of cancer-related deaths worldwide.1 In the last decade, our understanding of the genetic landscape of hepatocellular carcinoma (HCC) has improved significantly through large-scale sequencing studies. These studies have indicated that several signaling pathways are involved in HCC initiation and progression, including telomere maintenance, WNT-β-catenin pathway, cell cycle regulators, epigenetic regulators, AKT/mTOR and mitogen-activated protein kinase (MAPK) pathways.2 However, the most frequent mutations in HCC are currently undruggable. Sorafenib is a multikinase inhibitor that is approved as the standard therapy for advanced HCC patients, but only provides 2.8 months survival benefit.3 The survival advantage is even more limited for Asia-Pacific patients (2.3 months).4 Regorafenib is a treatment strategy shown to provide overall survival benefit in patients with HCC that has progressed on sorafenib treatment.5 However, chemically, regorafenib and sorafenib differ by just one atom.6 The regorafenib registration trial design involved a high degree of patient selection. Even in such optimal settings, only a modest clinical benefit was achieved. Lenvatinib is another multikinase inhibitor, which provided impressive antitumor activity in phase II trials in patients with advanced unresectable HCC.7 However, there is only 1.3 months additional survival benefit for patients compared to sorafenib in the phase III trials. These clinical data point to the need to reconsider the current therapeutic strategy. A recurrent problem in clinical studies in liver cancer is the paucity of biomarkers linked to drug response.8 
Routine HEV testing of blood products has recently been implemented in Great Britain and the Netherlands. The relevance of transfusion-transmitted HEV infections is still controversially discussed in Europe. All blood donations at the University Medical Center Hamburg-Eppendorf were prospectively tested for HEV RNA by pooled PCR from October 2016 to May 2017. Reactive samples were individually retested. Additionally, stored samples from previous donations of positive donors were tested to determine the duration of HEV viraemia. HEV RNA-positive donors and a control cohort were asked to answer a questionnaire. Twenty-three out of 18,737 HEV RNA-positive donors were identified (0.12%). Only two of the positive donors (8.7%) presented with elevated aminotransferases at time of donation (alanine aminotransferase: 192 and 101 U/L). The retrospective analysis of all positive donors revealed that four asymptomatic donors had been HEV viraemic for up to three months with the longest duration of HEV viraemia exceeding four months. Despite the HEV-testing efforts, 14 HEV RNA-positive blood products were transfused into 12 immunocompromised and two immunocompetent patients. One recipient of these products developed fatal acute-on-chronic liver failure complicated by Pseudomonas septicemia. The questionnaire revealed that HEV RNA-positive donors significantly more often consumed raw pork meat (12 out of 18; 67%) than controls (89 out of 256; 35%; p = 0.01). In two donors, undercooked pork liver dishes were identified as the source of infection. HEV genotyping was possible in 7 out of 23 of HEV viraemic donors and six out of seven isolates belonged to HEV Genotype 3, Group 2. Prolonged HEV viraemia can be detected at a relatively high rate in Northern German blood donors, leading to transfusion-transmitted HEV infections in several patients with the risk of severe and fatal complications. Eating raw pork tartare represented a relevant risk for the acquisition of HEV infection. HEV infections are present worldwide.1 Consumption of pork meat has been considered to be the major source of HEV Genotype (GT) 3 infections in Europe. In addition to zoonotic transmission, blood products were shown to be a potential source of acute and chronic HEV infection in industrialised countries.2,3 The anti-HEV seroprevalence rate in Europe, depicting the number of people previously exposed to HEV, varies largely depending on the region and the assay used.4 A high seroprevalence rate of 30% was found in healthy German individuals using the sensitive Wantai anti-HEV IgG assay.5 In addition to serological studies, blood donors were tested for HEV viraemia by PCR in various European studies, and rates of HEV positivity from 1:726 to 1:3,333 have been determined.6–11 The largest of these studies was conducted in the UK. In this study, 225,000 blood donations were tested in pools of 24 samples by an in-house assay, and 79 specimens tested positive for HEV PCR (1 out of 2,850). The likelihood of developing clinically relevant hepatitis E after transfusion of an HEV-contaminated blood product was 42%,10 while in a Japanese study, 50% of recipients of HEV-contaminated blood products developed HEV infection.12 Since it became apparent how frequently HEV was found in blood products, routine testing of blood products has been discussed controversially in many European countries. In the vast majority of these countries, HEV screening has not been implemented, while authorities in the UK, Ireland, and the Netherlands decided to test all blood products for HEV.13 The aim of the present study was to determine the prevalence of blood-borne HEV infections at our academic tertiary care centre in Northern Germany and to evaluate whether routine HEV testing of blood products should be performed. SECTION Materials and methods SECTION Routine screening 
Liver function tests (LFTs) are frequently requested blood tests which may indicate liver disease. LFTs are commonly abnormal, the causes of which can be complex and are frequently under investigated. This can lead to missed opportunities to diagnose and treat liver disease at an early stage. We developed an automated investigation algorithm, intelligent liver function testing (iLFT), with the aim of increasing the early diagnosis of liver disease in a cost-effective manner. We developed an automated system that further investigated abnormal LFTs on initial testing samples to generate a probable diagnosis and management plan. We integrated this automated investigation algorithm into the laboratory management system, based on minimal diagnostic criteria, liver fibrosis estimation, and reflex testing for causes of liver disease. This algorithm then generated a diagnosis and/or management plan. A stepped-wedged trial design was utilised to compare LFT outcomes in general practices in the 6 months before and after introduction of the iLFT system. Diagnostic outcomes were collated and compared. Of eligible patients with abnormal LFTs, 490 were recruited to the control group and 64 were recruited to the intervention group. The primary diagnostic outcome was based on the general practitioner diagnosis, which agreed with the iLFT diagnosis in 67% of cases. In the iLFT group, the diagnosis of liver disease was increased by 43%. Additionally, there were significant increases in the rates of GP visits after diagnosis and the number of referrals to secondary care in the iLFT group. iLFT was cost-effective with a low initial incremental cost-effectiveness ratio of £284 per correct diagnosis, and a saving to the NHS of £3,216 per patient lifetime. iLFT increases liver disease diagnoses, improves quality of care, and is highly cost-effective. This can be achieved with minor changes to working practices and exploitation of functionality existing within modern laboratory diagnostics systems. There has been an exponential increase in the number of liver function tests (LFTs) requested in general practice.1,2 A proportion are checked for the investigation of liver disease but most are for investigating undifferentiated illness, or monitoring non-hepatic long-term health conditions.3–5 It is unknown how significant a solitary abnormal LFT result is;1 does it signify current or future liver disease, disease in other organs, or is it a temporary phenomenon of little clinical relevance?6 Liver disease is increasing in incidence in contrast to many other conditions, predominantly driven by NAFLD. It disproportionately affects people under 65 leading to substantially increased morbidity and mortality. It is clear that interventions that lead to early diagnosis and the opportunity to intervene and abate disease progression are needed. iLFT delivers this opportunity in primary care to the general population at a minimal intervention cost, using existing infrastructure and utilising existing clinical pathways. It is designed for immediate implementation and could have impacts in the short term. The iLFT system works, it increases liver diagnosis, is cost-effective, and is clearly more effective at diagnosing liver disease than the standard of care. SECTION Financial support
The role of 18F-fluorodeoxyglucose positron emission tomography (18FDG-PET) in the diagnosis and staging of patients with biliary tract cancers (BTCs) remains controversial, so we aimed to provide robust information on the utility of 18FDG-PET in the diagnosis and management of BTC. This systematic review and meta-analysis explored the diagnostic test accuracy of 18FDG-PET as a diagnostic tool for diagnosis of primary tumour, lymph node invasion, distant metastases and relapsed disease. Subgroup analysis by study quality and BTC subtype were performed. Changes in management based on 18FDG-PET and impact of maximum standardised uptake values (SUVmax) on prognosis were also assessed. A random effects model was used for meta-analyses. A total of 2,125 patients were included from 47 eligible studies. The sensitivity (Se) and specificity (Sp) of 18FDG-PET for the diagnosis of primary tumour were 91.7% (95% CI 89.8–93.2) and 51.3% (95% CI 46.4–56.2), respectively, with an area under the curve (AUC) of 0.8668. For lymph node invasion, Se was 88.4% (95% CI 82.6–92.8) and Sp was 69.1% (95% CI 63.8–74.1); AUC 0.8519. For distant metastases, Se was 85.4% (95% CI 79.5–90.2) and Sp was 89.7% (95% CI 86.0–92.7); AUC 0.9253. For relapse, Se was 90.1% (95% CI 84.4–94.3) and Sp was 83.5% (95% CI 74.4–90.4); AUC 0.9592. No diagnostic threshold effect was identified. Meta-regression did not identify significant sources of heterogeneity. Sensitivity analysis revealed no change in results when analyses were limited to studies with low risk of bias/concern. The pooled proportion of change in management was 15% (95% CI 11–20); the majority (78%) due to disease upstaging. Baseline high SUVmax was associated with worse survival (pooled hazard ratio of 1.79; 95% CI 1.37–2.33; p <0.001). There is evidence to support the incorporation of 18FDG-PET into the current standard of care for the staging (lymph node and distant metastases) and identification of relapse in patients with BTC to guide treatment selection; especially if the identification of occult sites of disease would change management, or if diagnosis of relapse remains unclear following standard of care imaging. The role for diagnosis of the primary tumour remains controversial due to low sensitivity and 18FDG-PET should not be considered as a replacement for pathological confirmation in this setting. Introduction to biliary tract cancer 
The effects of long-term antiviral therapy on survival have not been adequately assessed in chronic hepatitis B (CHB). In this 10-centre, ongoing cohort study, we evaluated the probability of survival and factors affecting survival in Caucasian CHB patients who received long-term entecavir/tenofovir therapy. We included 1,951 adult Caucasians with CHB, with or without compensated cirrhosis and without hepatocellular carcinoma (HCC) at baseline, who received entecavir/tenofovir for ≥12 months (median, six years). Kaplan–Meier estimates of cumulative survival over time were obtained. Standardized mortality ratios (SMRs) were calculated by comparing death rates with those in the Human Mortality Database. The one-, five-, and eight-year cumulative probabilities were 99.7, 95.9, and 94.1% for overall survival, 99.9, 98.3, and 97.4% for liver-related survival, and 99.9, 97.8, and 95.8% for transplantation-free liver-related survival, respectively. Overall mortality was independently associated with older age and HCC development, liver-related mortality was associated with HCC development only, and transplantation-free liver-related mortality was independently associated with HCC development and lower platelet levels at baseline. Baseline cirrhosis was not independently associated with any type of mortality. Compared with the general population, in all CHB patients mortality was not significantly different (SMR 0.82), whereas it was lower in patients without HCC regardless of baseline cirrhosis (SMR 0.58) and was higher in patients who developed HCC (SMR 3.09). Caucasian patients with CHB and compensated liver disease who receive long-term entecavir/tenofovir therapy have excellent overall and liver-related eight-year survival, which is similar to that of the general population. HCC is the main factor affecting their overall mortality, and is the only factor affecting their liver-related mortality. Chronic infection with hepatitis B virus (HBV) is one of the most common causes of chronic liver disease worldwide.1–3 Patients with chronic HBV infection may have low viral replication and no significant histological lesions, but a substantial proportion of them develop chronic hepatitis B (CHB) with high viral replication and active histological lesions.1,4,5 If left untreated, CHB leads to accumulation of liver fibrosis and eventually progresses to cirrhosis and liver decompensation, and is thus associated with high morbidity and mortality.1,4,5 It is estimated that 2–10% of untreated CHB patients develop cirrhosis every year,1,4,6 and that only 55–85% of untreated patients with active HBV cirrhosis are alive five years later.4,6,7 In addition, all patients with chronic HBV infection are at higher risk of hepatocellular carcinoma (HCC) when compared with the general population, but the risk is highest when cirrhosis is present.1,5,6 Thus, more than 750,000 people die every year of HBV-related causes.8 
Hepatitis delta virus (HDV) infection is the most severe form of viral hepatitis. Although HDV-associated liver disease is considered immune-mediated, adaptive immune responses against HDV are weak. Thus, the role of several other cell-mediated mechanisms such as those driven by mucosa-associated invariant T (MAIT) cells, a group of innate-like T cells highly enriched in the human liver, has not been extensively studied in clinical HDV infection. MAIT cells from a sizeable cohort of patients with chronic HDV were analyzed ex vivo and in vitro after stimulation. Results were compared with MAIT cells from hepatitis B virus (HBV) monoinfected patients and healthy controls. Circulating MAIT cells were dramatically decreased in the peripheral blood of HDV-infected patients. Signs of decline were also observed in the liver. In contrast, only a modest decrease of circulating MAIT cells was noted in HBV monoinfection. Unsupervised high-dimensional analysis of residual circulating MAIT cells in chronic HDV infection revealed the appearance of a compound phenotype of CD38hiPD-1hiCD28loCD127loPLZFloEomesloHelioslo cells indicative of activation. Corroborating these results, MAIT cells exhibited a functionally impaired responsiveness. In parallel to MAIT cell loss, HDV-infected patients exhibited signs of monocyte activation and increased levels of proinflammatory cytokines IL-12 and IL-18. In vitro, IL-12 and IL-18 induced an activated MAIT cell phenotype similar to the one observed ex vivo in HDV-infected patients. These cytokines also promoted MAIT cell death, suggesting that they may contribute to MAIT cell activation and subsequent loss during HDV infection. These results suggest that chronic HDV infection engages the MAIT cell compartment causing activation, functional impairment, and subsequent progressive loss of MAIT cells as the HDV-associated liver disease progresses. Hepatitis delta virus (HDV), a small, defective RNA virus, causes the most severe form of viral hepatitis.1,2 For infection with HDV, coinfection with hepatitis B virus (HBV) is required. Up to 70 million individuals worldwide are chronically infected with HDV in conjunction with HBV infection.1–3 Compared to other chronic viral hepatitis patients, HDV-infected patients experience an accelerated progression to liver fibrosis, increased risk of hepatocellular carcinoma, and earlier decompensation during liver cirrhosis.1,2 Furthermore, treatment options against HDV infections are limited. At best, approximately 25% of infected patients respond to pegylated interferon-treatment with a measurable decline in HDV RNA viral load.4 Alternative new treatment strategies are only in the very early stages of clinical development.5 
Cholestasis often occurs after burn injuries. However, the prevalence of cholestasis and its effect on outcomes in patients with severe burn injuries are unknown. The aim of this study was to describe the course and the burden of cholestasis in a cohort of severely burned adult patients. We investigated the relationship between burn-associated cholestasis (BAC) and clinical outcomes in a retrospective cohort of patients admitted to our unit for severe burn injuries between 2012 and 2015. BAC was defined as an increased level of serum alkaline phosphatase (ALP) ≥1.5x the upper limit of normal (ULN) with an increased level of gamma-glutamyltransferase (GGT) ≥3x ULN, or as an increased level of total bilirubin ≥2x ULN. A total of 214 patients were included: 111 (52%) patients developed BAC after a median (IQR) stay of 9 (5–16) days. At 90 days, the mortality rate was 20%, including 34 and 9 patients with and without BAC (p <0.001), respectively, which corresponded to a 2.5-fold higher (95% CI 1.2–5.2, p = 0.012) risk of 90-day mortality for patients with BAC. After being adjusted for severity of illness, patients with BAC, hyperbilirubinemia and without elevated ALP and GGT levels had a hazard ratio of 4.51 (95% CI 1.87–10.87) for 90-day mortality. BAC was associated with the severity of the burn injury, shock and bacteraemia. BAC was present in 38 (51%) patients at discharge, and 7 (18%) patients had secondary sclerosing cholangitis. These patients maintained elevated levels of ALP and GGT that were 5.8x (1.7–15) the ULN and 11x the ULN (4.5–22), respectively, 20 months (3.5–35) after discharge. BAC is prevalent among patients with severe burn injuries and is associated with worse short-term outcomes, especially when total bilirubin levels were increased without elevated ALP and GGT levels. BAC survivors are at risk of developing sclerosing cholangitis. Although the outcomes of severe burn injuries have improved over recent decades, morbidity and mortality remain high: almost 40% of patients develop acute respiratory distress syndrome,1,2 up to 50% of patients develop acute kidney injury3,4 and the reported mortality rates range from 15%1,3,5 to 25–30%.2,4 Liver necrosis and hepatic dysfunction after burn injuries have been reported since the late 1930s.6 An autopsy series from the 1980s described increased liver size and lipid infiltrations after burn injuries.7 Later studies showed that these infiltrations were prevalent among burn patients and correlated with the total body surface area (TBSA) of the burn8 and were associated with liver dysfunction.9 In a paediatric series, increases in alkaline phosphatase (ALP) and gamma glutamyltransferase (GGT) levels were reported 10 days after sustaining a burn injury.10 Furthermore, cases of sclerosing cholangitis have been reported after burn injuries.11 However, the prevalence of cholestasis and the relationship between cholestasis and outcomes in patients with burns is unknown. The purpose of this study was to describe the course and the burden of cholestasis in a cohort of severely burned adult patients. According to the standard definitions of cholestasis12,13 based on ALP, GGT, and bilirubin levels, we propose the term/entity of burn-associated cholestasis (BAC) with a subtype classification according to the patterns of ALP, GGT, and total bilirubin levels and their associations with clinical outcomes. SECTION Patients and methods SECTION Study design 
Severe acute liver injury is a grave complication of exertional heatstroke. Liver transplantation (LT) may be a therapeutic option, but the criteria for LT and the optimal timing of LT have not been clearly established. The aim of this study was to define the profile of patients who require transplantation in this context. This was a multicentre, retrospective study of patients admitted with a diagnosis of exertional heatstroke-related severe acute liver injury with a prothrombin time (PT) of less than 50%. A total of 24 male patients were studied. Fifteen of the 24 patients (median nadir PT: 35% [29.5–40.5]) improved under medical therapy alone and survived. Nine of the 24 were listed for emergency LT. At the time of registration, the median PT was 10% (5–12) and all had numerous dysfunctional organs. Five patients (nadir PT: 12% [9–12]) were withdrawn from the list because of an elevation of PT values that mainly occurred between day 2 and day 3. Ultimately, 4 patients underwent transplantation as their PT persisted at <10%, 3 days (2.75–3.25) after the onset of exertional heatstroke, and they had more than 3 organ dysfunctions. Of these 4 patients, 3 were still alive 1 year later. Histological analysis of the 4 explanted livers demonstrated massive or sub-massive necrosis, and little potential for effective mitoses, characterised by a “mitonecrotic” appearance. The first-line treatment for exertional heatstroke-related severe acute liver injury is medical therapy. LT is only a rare alternative and such a decision should not be taken too hastily. A persistence of PT <10%, without any signs of elevation after a median period of 3  days following the onset of heatstroke, was the trigger that prompted LT, was the trigger adopted in order to decide upon LT. Exertional heatstroke is a rare but serious complication that can occur during intense and prolonged physical exercise in a hot and humid climate. It is mainly characterised by hyperthermia higher than 40 °C associated with central neurological disorders. It occurs in individuals who were previously in good health.1 
Intrahepatic cholangiocarcinoma (ICC) is the second-most lethal primary liver cancer. Little is known about intratumoral heterogeneity (ITH) and its impact on ICC progression. We aimed to investigate the ITH of ICC in the hope of helping to develop new therapeutic strategies. We obtained 69 spatially distinct regions from six operable ICCs. Patient-derived primary cancer cells (PDPCs) were established for each region, followed by whole-exome sequencing (WES) and multi-level validation. We observed widespread ITH for both somatic mutations and clonal architecture, shaped by multiple mechanisms, like clonal “illusion”, parallel evolution and chromosome instability. A median of 60.3% of mutations were heterogeneous, among which 85% of the driver mutations were located on the branches of tumor phylogenetic trees. Many truncal and clonal driver mutations occurred in tumor suppressor genes, such as TP53, SMARCB1 and PBRM1 that are involved in DNA repair and chromatin-remodeling. Genome doubling occurred in most cases (5/6) after the accumulation of truncal mutations and was shared by all intratumoral sub-regions. In all cases, ongoing chromosomal instability is evident throughout the evolutionary trajectory of ICC. The recurrence of ICC1239 provided evidence to support the polyclonal metastatic seeding in ICC. The change of mutation landscape and internal diversity among subclones during metastasis, such as the loss of chemoresistance mediator, can be used for new treatment strategies. Targeted therapy against truncal alterations, such as IDH1, JAK1, and KRAS mutations and EGFR amplification, was developed in 5/6 patients. Integrated investigations of spatial ITH and clonal evolution may provide an important molecular foundation for enhanced understanding of tumorigenesis and progression in ICC. Intrahepatic cholangiocarcinoma (ICC) is the second-most frequent type of primary liver cancer with an increasing global incidence during the past decades. Although hepatectomy is a curative treatment method for early ICC, five-year survival rates after resection varies around 30–35%.1 For those in the late stage that only receive cisplatin-based chemotherapy, the median overall survival is as low as 11.7 months.2 The lack of commonly accepted effective medical treatments highlights the importance of understanding ICC pathogenesis, to develop new treatment strategies.3 
The development of non-invasive liver fibrosis tests may enable earlier identification of patients with non-alcoholic fatty liver disease (NAFLD) requiring referral to secondary care. We developed and evaluated a pathway for the management of patients with NAFLD, aimed at improving the detection of cases of advanced fibrosis and cirrhosis, and avoiding unnecessary referrals. This was a prospective longitudinal cohort study, with analyses performed before and after introduction of the pathway, and comparisons made to unexposed controls. We used a 2-step algorithm combining the use of Fibrosis-4 score followed by the ELF™ test if required. In total, 3,012 patients were analysed. Use of the pathway detected 5 times more cases of advanced fibrosis (Kleiner F3) and cirrhosis (odds ratio [OR] 5.18; 95% CI 2.97–9.04; p <0.0001), while reducing unnecessary referrals from primary care to secondary care by 81% (OR 0.193; 95% CI 0.111–0.337; p <0.0001). Although it was used for only 48% of referrals, significant benefits were observed in practices exposed to the pathway compared to those which were not, with unnecessary referrals falling by 77% (OR 0.23; 95% CI 0.658–0.082; p = 0.006) and a 4-fold improvement in detection of cases of advanced fibrosis and cirrhosis (OR 4.32; 95% CI 1.52–12.25; p = 0.006). Compared to referrals made before the introduction of the pathway, unnecessary referrals fell from 79/83 referrals (95.2%) to 107/152 (70.4%), representing an 88% reduction in unnecessary referrals when the pathway was followed (OR 0.12; 95% CI 0.042–0.349; p <0.0001). The use of non-invasive blood tests for liver fibrosis improves the detection of advanced fibrosis and cirrhosis, while reducing unnecessary referrals in patients with NAFLD. This strategy improves resource use and benefits patients. Non-alcoholic fatty liver disease (NAFLD) is the commonest cause of deranged liver blood tests (LFTs) in primary care in Europe and North America,1 with an estimated prevalence of 25–30% in the adult population.2 Only a minority of people with NAFLD (5%) develop clinically significant liver disease,2 but the burden is such that NAFLD is predicted to be the leading indication for liver transplantation within a decade.3 The C&I NAFLD pathway improved the selection of patients with advanced fibrosis and cirrhosis for referral to secondary care, reducing unnecessary referrals. This in turn delivers improvements in the detection of serious liver damage, better use of healthcare resources and immediate cost savings. The reduction in referrals to secondary care reduces strain on services that are confronting a rising prevalence of obesity and NAFLD as well as improving patient experiences by avoiding unnecessary clinic appointments and investigations. This is the first study to incorporate the British Society for Gastroenterology guidance on the management of NAFLD and validates the recommendation to use FIB-4 and ELF for 2-stage stratification. The NAFLD pathway is highly generalizable, as GPs will have access to both FIB-4 and ELF tests through most biochemistry laboratories.
The lysyl oxidase-like protein 2 (LOXL2) promotes stabilization of the extracellular matrix, chemotaxis, cell growth and cell mobility. We aimed to (i) identify stimuli of LOXL2 in cholangiopathies, (ii) characterize the effects of LOXL2 on biliary epithelial cells’ (BECs) barrier function, (iii) compare LOXL2 expression in primary sclerosing cholangitis (PSC), primary biliary cholangitis, and disease controls, and (iv) to determine LOXL2 expression and its cellular sources in four mouse models of cholangiopathies. Cultured murine BECs were challenged with well-known triggers of cellular senescence, hypoxia, phospholipid-deficient Abcb4−/− mouse bile and chenodeoxycholic acid and investigated for LOXL2, SNAIL1 and E-cadherin expression and transepithelial electrical resistance with and without LOX-inhibition. In vivo, LOXL2 expression was studied in PSC livers, and controls and mouse models. We compared LOXL2 serum levels in patients with PSC, secondary SC, primary biliary cholangitis, and controls. Cellular senescence, hypoxia, Abcb4−/− bile and chenodeoxycholic acid induced LOXL2 and SNAIL1 expression, repressed E-cadherin expression, and significantly reduced transepithelial electrical resistance in BECs. Notably, all of the pathological changes could be recovered via pharmacological LOX-inhibition. Mouse models showed induced LOXL2 expression in the portal region and in association with ductular reaction. LOXL2 serum levels were significantly elevated in patients with cholangiopathies. In PSC, LOXL2 expression was located to characteristic periductal onion skin-type fibrosis, ductular reaction, Kupffer cells, and fibrotic septa. Importantly, in PSC, LOXL2 overexpression was paralleled by E-cadherin loss in BECs from medium-sized bile ducts. Reactive BECs produce LOXL2, resulting in increased tight junction permeability, which can be ameliorated by pharmacological LOX-inhibition in vitro. Reactive BECs, portal myofibroblasts, and Kupffer cells are the main sources of LOXL2 in cholangiopathies. Bile ducts play a pivotal and active role in bile formation and excretion.1–4 Bile duct integrity is therefore a prerequisite for normal liver function.5 Normal bile duct secretory function critically depends on regular epithelial barrier function including selective permeability for specific molecules.5 This requires numerous specific proteins forming a tightly regulated tight junction protein complex between bile duct epithelial cells (BECs).6 Cholangiopathies are frequently associated with alterations of BECs’ tight junctions.5,7,8 Numerous exogenous and endogenous toxins, cytokines and chemokines, and pathogens may negatively impact on tight junction function studied primarily in vitro and in animal models (e.g. lithocholic acid [LCA]-fed mice, Mdr2−/− mice, alpha-naphthylisothiocyanate).9–11 Increasing evidence from mouse models suggests that impaired tight junction integrity may play an important role in the pathogenesis of cholangiopathies and cholestasis: (i) E-cadherin (Cdh1−/−) knockout mice spontaneously develop sclerosing cholangitis,12 (ii) α-catenin (Ctnna1−/−) knockout mice develop cholestasis with reduced bile flow and increased susceptibility to cholic acid feeding injury,13 and (iii) claudin-2 deficiency in mice significantly reduced bile flow.14 Consequently, altered bile composition could alter tight junction structure and function9,10,15,16 and conversely, tight junction alterations may affect bile formation and render bile ducts more susceptible to cholangitis.5,13,14,12,17,18 
Long noncoding RNAs (lncRNAs) play important roles in various biological processes, regulating gene expression by diverse mechanisms. However, how lncRNAs regulate liver repopulation is unknown. Herein, we aimed to identify lncRNAs that regulate liver repopulation and elucidate the signaling pathways involved. Herein, we performed 70% partial hepatectomy in wild-type and gene knockout mice. We then performed transcriptomic analyses to identify a divergent lncRNA termed lncHand2 that is highly expressed during liver regeneration. LncHand2 is constitutively expressed in the nuclei of pericentral hepatocytes in mouse and human livers. LncHand2 knockout abrogates liver regeneration and repopulation capacity. Mechanistically, lncHand2 recruits the Ino80 remodeling complex to initiate expression of Nkx1-2 in trans, which triggers c-Met (Met) expression in hepatocytes. Finally, knockout of both Nkx1-2 and c-Met causes more severe liver injury and poorer repopulation ability. Thus, lncHand2 promotes liver repopulation via initiating Nkx1-2-induced c-Met signaling. Our findings reveal that lncHand2 acts as a critical mediator regulating liver repopulation. It does this by inducing Nkx1-2 expression, which in turn triggers c-Met signaling. The liver is a central organ for homeostasis with considerable regenerative capacity.1–3 Liver transplantation is the only effective therapy for advanced liver disease. However, a shortage of donor organs is a real problem across the world. Hepatocyte cell therapy is thus an attractive alternative to liver transplantation. Mature hepatocytes harbor a remarkable capacity to proliferate upon liver injury.4 Liver regeneration occurs after the loss of hepatic tissue as a fundamental parameter. However, the molecular regulatory mechanisms of liver regeneration are still elusive. 
Resection is the most widely used potentially curative treatment for patients with early hepatocellular carcinoma (HCC). However, recurrence within 2 years occurs in 30–50% of patients, being the major cause of mortality. Herein, we describe 2 models, both based on widely available clinical data, which permit risk of early recurrence to be assessed before and after resection. A total of 3,903 patients undergoing surgical resection with curative intent were recruited from 6 different centres. We built 2 models for early recurrence, 1 using preoperative and 1 using pre and post-operative data, which were internally validated in the Hong Kong cohort. The models were then externally validated in European, Chinese and US cohorts. We developed 2 online calculators to permit easy clinical application. Multivariable analysis identified male gender, large tumour size, multinodular tumour, high albumin-bilirubin (ALBI) grade and high serum alpha-fetoprotein as the key parameters related to early recurrence. Using these variables, a preoperative model (ERASL-pre) gave 3 risk strata for recurrence-free survival (RFS) in the entire cohort – low risk: 2-year RFS 64.8%, intermediate risk: 2-year RFS 42.5% and high risk: 2-year RFS 20.7%. Median survival in each stratum was similar between centres and the discrimination between the 3 strata was enhanced in the post-operative model (ERASL-post) which included ‘microvascular invasion’. Statistical models that can predict the risk of early HCC recurrence after resection have been developed, extensively validated and shown to be applicable in the international setting. Such models will be valuable in guiding surveillance follow-up and in the design of post-resection adjuvant therapy trials. Worldwide, hepatocellular carcinoma (HCC) is the sixth most frequent malignancy and the second most common cause of cancer-related death.1 There is a wide variety of therapeutic options for patients with HCC, depending on tumour burden, liver function and performance status.2 Potentially curative therapy recommended for those patients with very early/early stage tumour (Barcelona Clinic Liver Cancer [BCLC] 0/A) consists of surgical resection, liver transplantation or local ablation. Because of the scarcity of donor organs, surgical resection and ablation are the mainstay of curative treatment options in Asian-Pacific countries, which account for three-quarters of all new patients globally.1 Surgical resection provides better clinical outcome than local ablation particularly among patients with well-preserved hepatic function.3,4 
Non-alcoholic steatohepatitis (NASH) is characterized by hepatocyte steatosis, ballooning, and lobular inflammation which may lead to fibrosis. Lipotoxicity activates caspases, which cause apoptosis and inflammatory cytokine (IL-1β and IL-18) production. Emricasan is a pan-caspase inhibitor that decreases serum aminotransferases and caspase activation in NASH patients. This study postulated that 72 weeks of emricasan treatment would improve liver fibrosis without worsening of NASH. This double-blind, placebo-controlled study randomized 318 subjects 1:1:1 to twice-daily treatment with emricasan (5 or 50 mg) or matching placebo for 72 weeks. Subjects had definite NASH and NASH CRN fibrosis stage F1-F3, as determined by a central reader, on a liver biopsy obtained within 6 months of randomization. Emricasan treatment did not achieve the primary objective of fibrosis improvement without worsening of NASH (emricasan 5 mg: 11.2%; emricasan 50 mg: 12.3%; placebo: 19.0%; odds ratios vs. placebo 0.530 and 0.588, with p=0.972 and 0.972, respectively) or the secondary objective of NASH resolution without worsening of fibrosis (emricasan 5 mg: 3.7%; emricasan 50 mg: 6.6%; placebo: 10.5%; odds ratios vs. placebo 0.334 and 0.613, with p=0.070 and 0.335, respectively). In the small subset of subjects with consistent normalization of serum ALT over 72 weeks, emricasan may have improved histologic outcomes. Emricasan treatment did not improve liver histology in subjects with NASH fibrosis despite target engagement and may have worsened fibrosis and ballooning. Caspase inhibition lowered serum ALT in the short-term but may have directed cells to alternative mechanisms of cell death, resulting in more liver fibrosis and hepatocyte ballooning. Non-alcoholic fatty liver disease (NAFLD) is becoming a major epidemic. Up to 30% of Western populations have NAFLD, 10-20% of those patients will eventually develop non-alcoholic steatohepatitis (NASH) and fibrosis, and 10-20% of those NASH patients will develop cirrhosis [1, 2]. NASH with fibrosis is the most important risk factor for developing cirrhosis and liver-related morbidity [3]. Unlike hepatitis C infection, where the paradigm of hepatocellular carcinoma occurs typically via a cirrhotic pathway, newer data suggest that a significant number of new cases of hepatocellular carcinoma in NAFLD may arise outside of a cirrhotic phenotype [4, 5]. Consequently, NASH cirrhosis and hepatocellular carcinoma due to NASH are a leading indication for liver transplantation in the U.S. [6]. 
Idarubicin shows high cytotoxicity against hepatocellular carcinoma (HCC) cells, a high hepatic extraction ratio, and high lipophilicity leading to stable emulsions with lipiodol. A dose-escalation phase I trial of idarubicin_lipiodol (without embolisation) was conducted in patients with cirrhotic HCC to estimate the maximum-tolerated dose (MTD) and to assess the safety, efficacy, and pharmacokinetics of the drug, and the health-related quality of life achieved by patients. Patients underwent two sessions of treatment with a transarterial idarubicin_lipiodol emulsion without embolisation. The idarubicin dose was escalated according to a modified continuous reassessment method. The MTD was defined as the dose closest to that causing dose-limiting toxicity (DLT) in 20% of patients. A group of 15 patients were enrolled, including one patient at 10 mg, four patients at 15 mg, seven patients at 20 mg, and three patients at 25 mg. Only two patients experienced DLT: oedematous ascitic decompensation and abdominal pain at 20 and 25 mg, respectively. The calculated MTD of idarubicin was 20 mg. The most frequent grade ≥3 adverse events were biological. One month after the second session, the objective response rate was 29% (complete response, 0%; partial response, 29%) based on modified Response Evaluation Criteria In Solid Tumours. The median time to progression was 5.4 months [95% confidence limit (CI) 3.0–14.6 months] and median overall survival was 20.6 months (95% CI 5.7–28.7 months). Pharmacokinetic analysis of idarubicin showed that the mean Cmax of idarubicin after intra-arterial injection of the idarubicin-lipiodol emulsion is approximately half the Cmax after intravenous administration. Health-related quality of life results confirmed the good safety results associated with use of the drug. The MTD of idarubicin was 20 mg after two chemolipiodolisation sessions. Encouraging safety results, and patient responses and survival were observed. A phase II trial has been scheduled. Transarterial chemoembolisation (TACE) is the standard of care for unresectable intermediate-stage hepatocellular carcinoma (HCC).1 Although TACE has been widely used for several years, the procedure varies considerably across centres and interventional radiologists, especially regarding chemotherapeutic agents, doses, drug-releasing vectors, and embolisation agents.2 This heterogeneity is explained by the absence of proof to clarify the mechanism by which TACE improves patient survival. Some clinicians believe that embolisation has a major role and that adding a chemotherapeutic agent does not improve efficacy. Indeed, six randomised trials failed to demonstrate the superiority of TACE over bland embolisation.3–8 Others believe that the locally delivered chemotherapeutic agent has a major role and that embolisation is associated with greater harm than benefit. Three randomised trials failed to demonstrate the superiority of TACE over chemolipiodolisation (transarterial injection of a chemotherapeutic agent emulsified with lipiodol) in patients with unresectable HCC.9–11 A large randomised study comparing three groups of patients with unresectable HCC receiving triple-drug chemolipiodolisation with (arm 1, n = 122) or without embolisation (arm 2, n = 121) or single-drug chemolipiodolisation with embolisation (arm 3, n = 122) showed significantly better survival in patients receiving triple-drug chemolipiodolisation,12 thereby highlighting the major role of the drugs in the efficacy of the procedure. 
Advancing liver disease results in deleterious changes in a number of critical organs. The ability to measure structure, blood flow and tissue perfusion within multiple organs in a single scan has implications for determining the balance of benefit vs. harm for therapies. Our aim was to establish the feasibility of magnetic resonance imaging (MRI) to assess changes in Compensated Cirrhosis (CC), and relate this to disease severity and future liver-related outcomes (LROs). A total of 60 patients with CC, 40 healthy volunteers and 7 patients with decompensated cirrhosis were recruited. In a single scan session, MRI measures comprised phase-contrast MRI vessel blood flow, arterial spin labelling tissue perfusion, T1 longitudinal relaxation time, heart rate, cardiac index, and volume assessment of the liver, spleen and kidneys. We explored the association between MRI parameters and disease severity, analysing differences in baseline MRI parameters in the 11 (18%) patients with CC who experienced future LROs. In the liver, compositional changes were reflected by increased T1 in progressive disease (p <0.001) and an increase in liver volume in CC (p = 0.006), with associated progressive reduction in liver (p <0.001) and splenic (p <0.001) perfusion. A significant reduction in renal cortex T1 and increase in cardiac index and superior mesenteric arterial blood flow was seen with increasing disease severity. Baseline liver T1 (p = 0.01), liver perfusion (p <0.01), and renal cortex T1 (p <0.01) were significantly different in patients with CC who subsequently developed negative LROs. MRI enables the contemporaneous assessment of organs in liver cirrhosis in a single scan without the requirement for a contrast agent. MRI parameters of liver T1, renal T1, hepatic and splenic perfusion, and superior mesenteric arterial blood flow were related to the risk of LROs. The evolution of liver cirrhosis to clinical liver-related outcomes resulting from portal hypertension is not simply dictated by architectural and haemodynamic changes within the liver. Rather, advancing liver disease results in deleterious changes in a number of critical organs and the understanding of this process is a central aspect in the clinical management of cirrhotic patients. 
Over the last decade, liver transplantation of sicker, older non-hepatitis C cirrhotics with multiple co-morbidities has increased in the United States. We sought to identify an easily applicable set of recipient factors among HCV negative adult transplant recipients associated with significant morbidity and mortality within five years after liver transplantation. We collected national (n = 31,829, 2002–2015) and center-specific data. Coefficients of relevant recipient factors were converted to weighted points and scaled from 0–5. Recipient factors associated with graft failure included: ventilator support (five patients; hazard ratio [HR] 1.59; 95% CI 1.48–1.72); recipient age >60 years (three patients; HR 1.29; 95% CI 1.23–1.36); hemodialysis (three patients; HR 1.26; 95% CI 1.16–1.37); diabetes (two patients; HR 1.20; 95% CI 1.14–1.27); or serum creatinine ≥1.5 mg/dl without hemodialysis (two patients; HR 1.15; 95% CI 1.09–1.22). Graft survival within five years based on points (any combination) was 77.2% (0–4), 69.1% (5–8) and 57.9% (>8). In recipients with >8 points, graft survival was 42% (model for end-stage liver disease [MELD] score <25) and 50% (MELD score 25–35) in recipients receiving grafts from donors with a donor risk index >1.7. In center-specific data within the first year, subjects with ≥5 points (vs. 0–4) had longer hospitalization (11 vs. 8 days, p <0.01), higher admissions for rehabilitation (12.3% vs. 2.7%, p <0.01), and higher incidence of cardiac disease (14.2% vs. 5.3%, p <0.01) and stage 3 chronic kidney disease (78.6% vs. 39.5%, p = 0.03) within five years. The impact of co-morbidities in an MELD-based organ allocation system need to be reassessed. The proposed clinical tool may be helpful for center-specific assessment of risk of graft failure in non-HCV patients and for discussion regarding relevant morbidity in selected subsets. Liver transplant (LT) recipients in the United States are globally sicker in the current era.1,2 The number of candidates delisted for being too sick has nearly tripled over the last few years.3 Indications for LT have also evolved with a noticeable increase in transplantation of candidates with non-alcoholic fatty liver disease (NAFLD) and cryptogenic cirrhosis, as well as patients with significant co-morbidities.2,4,5 In the current organ allocation policy based on urgency, people at the highest risk of death are offered liver transplantation (LT) first. However, several recipient characteristics may diminish the guiding principle of an MELD-based system.6–8 
Alpha-1 antitrypsin deficiency (AATD) is an uncommonly recognized cause of liver disease in adults, with descriptions of its natural history limited to case series and patient-reported data from disease registries. Liver pathology is limited to selected patients or unavailable. Therefore, we aimed to determine the prevalence and severity of liver fibrosis in an adult AATD population who were not known to have cirrhosis, while defining risk factors for fibrosis and testing non-invasive markers of disease. A total of 94 adults with classic genotype ‘PI*ZZ’ AATD were recruited from North America and prospectively enrolled in the study. Liver aminotransferases and markers of synthetic function, transient elastography, and liver biopsy were performed. The prevalence of clinically significant liver fibrosis (F ≥ 2) was 35.1%. Alanine aminotransferase, aspartate aminotransferase and gamma-glutamyltransferase values were higher in the F ≥ 2 group. Metabolic syndrome was associated with the presence of clinically significant fibrosis (OR 14.2; 95% CI 3.7–55; p <0.001). Additionally, the presence of accumulated abnormal AAT in hepatocytes, portal inflammation, and hepatocellular degeneration were associated with clinically significant fibrosis. The accuracy of transient elastography to detect F ≥ 2 fibrosis was fair, with an AUC of 0.70 (95% CI 0.58–0.82). Over one-third of asymptomatic and lung affected adults with ‘PI*ZZ’ AATD have significant underlying liver fibrosis. Liver biopsies demonstrated variable amounts of accumulated Z AAT. The risk of liver fibrosis increases in the presence of metabolic syndrome, accumulation of AAT in hepatocytes, and portal inflammation on baseline biopsy. The results support the hypothesis that liver disease in this genetic condition may be related to a “toxic gain of function” from accumulation of AAT in hepatocytes. Alpha-1 antitrypsin deficiency (AATD) is an inherited cause of liver disease in adults and children. The underlying genetic change is a point mutation in the serine protease inhibitor alpha-1-antitrypsin.1 In the classic form of AATD, affected individuals are homozygous for the Z allele (ZZ), where a Glu342Lys substitution leads to a misfolded protein. The resulting protein conformation alpha-1 antitrypsin Z (ATZ) favors retention and subsequent polymerization within the hepatocyte endoplasmic reticulum (ER).2 These polymers form the PAS-positive inclusions that are the hallmark of AATD liver disease on biopsy. Accumulation of ATZ within the ER leads to activation of intracellular disposal mechanisms for misfolded proteins and clearance from hepatocytes.3 Insufficient clearance of ATZ leads to further polymerization and retention, which is widely accepted as the cause of liver disease. Experimentally, hepatocytes with the highest burden of ATZ retention and accumulation are the most susceptible to injury and cell death.4 Conversely, blocking production and/or buildup of ATZ leads to reversal of liver injury in disease models.5,6 
Fibrosis, a cardinal feature of a dysfunctional liver, significantly contributes to the ever-increasing mortality due to end-stage chronic liver diseases. The crosstalk between hepatocytes and hepatic stellate cells (HSCs) plays a key role in the progression of fibrosis. Although ample efforts have been devoted to elucidate the functions of HSCs during liver fibrosis, the regulatory functions of hepatocytes remain elusive. Using an unbiased functional microRNA (miRNA) screening, we investigated the ability of hepatocytes to regulate fibrosis by fine-tuning gene expression via miRNA modulation. The in vivo functional analyses were performed by inhibiting miRNA in hepatocytes using adeno-associated virus in carbon-tetrachloride- and 3,5-di-diethoxycarbonyl-1,4-dihydrocollidine-induced liver fibrosis. Blocking miRNA-221-3p function in hepatocytes during chronic liver injury facilitated recovery of the liver and faster resolution of the deposited extracellular matrix. Furthermore, we demonstrate that reduced secretion of C–C motif chemokine ligand 2, as a result of post-transcriptional regulation of GNAI2 (G protein alpha inhibiting activity polypeptide 2) by miRNA-221-3p, mitigates liver fibrosis. Collectively, miRNA modulation in hepatocytes, an easy-to-target cell type in the liver, may serve as a potential therapeutic approach for liver fibrosis. Liver fibrosis and cirrhosis contribute to more than 1 million deaths per year worldwide1,2 (particularly, 170,000 per year in Europe3 and 33,539 per year in the United States4). The underlying pathologies leading to fibrosis and subsequently cirrhosis are chronic virus infection, alcoholic steatohepatitis, and non-alcoholic steatohepatitis.5 The manifestation of fibrosis is accompanied by the activation of quiescent HSCs, accumulation of excessive extracellular matrix, and hepatocyte dysfunction leading to liver failure.6 While the majority of efforts have been concentrated on the elucidation of HSCs function during fibrosis, the regulatory functions of hepatocyte, the main parenchymal cells of the liver, remain to be understood further. Specifically, how gene expression alterations, particularly at the post-transcriptional level in hepatocytes, regulate fibrosis remains to be investigated. MiRNAs, one of the post-transcriptional regulators of gene expression,7,8 have been reported to be deregulated in liver fibrosis.7,9 The majority of studies have examined the functions of miRNAs in HSCs; however, identification and functional analysis of hepatocyte miRNAs, which are capable of regulating fibrosis, remain to be investigated. Elucidation of such key fibrosis-regulating miRNAs in hepatocytes, an easy-to-target cell type in the liver, would allow development of successful therapeutics for liver fibrosis. SECTION Materials and methods SECTION Animals 
Little is known about outcomes of liver transplantation for patients with non-alcoholic steatohepatitis (NASH). We aimed to determine the frequency and outcomes of liver transplantation for patients with NASH in Europe and identify prognostic factors. We analysed data from patients transplanted for end-stage liver disease between January 2002 and December 2016 using the European Liver Transplant Registry database. We compared data between patients with NASH versus other aetiologies. The principle endpoints were patient and overall allograft survival. Among 68,950 adults undergoing first liver transplantation, 4.0% were transplanted for NASH – an increase from 1.2% in 2002 to 8.4% in 2016. A greater proportion of patients transplanted for NASH (39.1%) had hepatocellular carcinoma (HCC) than non-NASH patients (28.9%, p <0.001). NASH was not significantly associated with survival of patients (hazard ratio [HR] 1.02, p = 0.713) or grafts (HR 0.99; p = 0.815) after accounting for available recipient and donor variables. Infection (24.0%) and cardio/cerebrovascular complications (5.3%) were the commonest causes of death in patients with NASH without HCC. Increasing recipient age (61–65 years: HR 2.07, p <0.001; >65: HR 1.72, p = 0.017), elevated model for end-stage liver disease score (>23: HR 1.48, p = 0.048) and low (<18.5 kg/m2: HR 4.29, p = 0.048) or high (>40 kg/m2: HR 1.96, p = 0.012) recipient body mass index independently predicted death in patients transplanted for NASH without HCC. Data must be interpreted in the context of absent recognised confounders, such as pre-morbid metabolic risk factors. The number and proportion of liver transplants performed for NASH in Europe has increased from 2002 through 2016. HCC was more common in patients transplanted with NASH. Survival of patients and grafts in patients with NASH is comparable to that of other disease indications. The prevalence of non-alcoholic fatty liver disease (NAFLD) has increased dramatically, in parallel with the worldwide increase in obesity and diabetes.1,2 Approximately a quarter of the European adult population have NAFLD, representing an increase of 10% since 2005.3 
The immunogenomic characteristics of hepatocellular carcinomas (HCCs) with immune cell stroma (HCC-IS), defined histologically, have not been clarified. We investigated the clinical and molecular features of HCC-IS and the prognostic impact of Epstein-Barr virus (EBV) infection. We evaluated 219 patients with conventional HCC (C-HCC) and 47 with HCC-IS using in situ hybridization for EBV, immunohistochemistry, multiplex immunofluorescence staining, and whole exome and transcriptome sequencing. Human leukocyte antigen types were also extracted from the sequencing data. Genomic and prognostic parameters were compared between HCC-IS and C-HCC. Results: CD8 T cell infiltration was more frequent in HCC-IS than C-HCC (mean fraction/sample, 22.6% vs. 8.9%, false discovery rate q <0.001), as was EBV positivity in CD20-positive tumor-infiltrating lymphocytes (TILs) (74.5% vs. 4.6%, p <0.001). CTNNB1 mutations were not identified in any HCC-IS, while they were present in 24.1% of C-HCC (p = 0.016). Inhibitory and stimulatory immune modulators were expressed at similar levels in HCC-IS and EBV-positive C-HCC. Global hypermethylation, and expression of PD-1 and PD-L1 in TILs, and PD-L1 in tumors, were also associated with HCC-IS (p <0.001), whereas human leukocyte antigen type did not differ according to HCC type or EBV positivity. HCC-IS was an independent factor for favorable recurrence-free survival (adjusted hazard ratio [aHR] 0.23; p = 0.002). However, a subgroup of tumors with a high density of EBV-positive TILs had poorer recurrence-free (aHR 25.48; p <0.001) and overall (aHR 9.6; p = 0.003) survival, and significant enrichment of CD8 T cell exhaustion signatures (q = 0.0296). HCC-IS is a distinct HCC subtype associated with a good prognosis and frequent EBV-positive TILs. However, paradoxically, a high density of EBV-positive TILs in tumors is associated with inferior prognostic outcomes. Patients with HCC-IS could be candidates for immunotherapy. Hepatocellular carcinomas (HCCs) are biologically heterogeneous at the molecular level, and thus diverse molecular subgroups based on large-scale exome or transcriptome profiling have been identified.1–4 This oncologic background may prevent novel molecular-targeted agents from significantly improving survival rates in patients with HCC.5 Immune checkpoint inhibitors, including anti-programmed cell death 1 (PD-1) and anti-programmed cell death 1 ligand 1 (PD-L1), are emerging as potential anti-HCC agents,6,7 and a unique immune class specific for HCC was recently defined based on gene expression profiling of tumor tissues.8–10 
GS-9620, an oral agonist of toll-like receptor 7 (TLR7), is in clinical development for the treatment of chronic hepatitis B (CHB). GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the woodchuck and chimpanzee models of CHB. Herein, we investigated the molecular mechanisms that contribute to the antiviral response to GS-9620 using in vitro models of hepatitis B virus (HBV) infection. Cryopreserved primary human hepatocytes (PHH) and differentiated HepaRG (dHepaRG) cells were infected with HBV and treated with GS-9620, conditioned media from human peripheral blood mononuclear cells treated with GS-9620 (GS-9620 conditioned media [GS-9620-CM]), or other innate immune stimuli. The antiviral and transcriptional response to these agents was determined. GS-9620 had no antiviral activity in HBV-infected PHH, consistent with low level TLR7 mRNA expression in human hepatocytes. In contrast, GS-9620-CM induced prolonged reduction of HBV DNA, RNA, and antigen levels in PHH and dHepaRG cells via a type I interferon (IFN)-dependent mechanism. GS-9620-CM did not reduce covalently closed circular DNA (cccDNA) levels in either cell type. Transcriptional profiling demonstrated that GS-9620-CM strongly induced various HBV restriction factors – although not APOBEC3A or the Smc5/6 complex – and indicated that established HBV infection does not modulate innate immune sensing or signaling in cryopreserved PHH. GS-9620-CM also induced expression of immunoproteasome subunits and enhanced presentation of an immunodominant viral peptide in HBV-infected PHH. Type I IFN induced by GS-9620 durably suppressed HBV in human hepatocytes without reducing cccDNA levels. Moreover, HBV antigen presentation was enhanced, suggesting additional components of the TLR7-induced immune response played a role in the antiviral response to GS-9620 in animal models of CHB. Approximately 240 million individuals are chronically infected with hepatitis B virus and over 650,000 people die each year because of HBV-associated liver diseases, such as cirrhosis and hepatocellular carcinoma. Immunologic control of chronic hepatitis B (CHB), recognized as a “functional cure”, is defined as sustained loss of HBV surface antigen (HBsAg) off therapy with or without seroconversion to anti-HBs antibody. Current therapies for CHB are limited to nucleos(t)ides inhibitors and interferon-alpha (IFN-α). These agents reduce viral load and improve long-term outcome, but they rarely lead to cure. Therefore, there is an urgent need for new therapies that induce durable immune control of chronic HBV infection. 
In non-alcoholic fatty liver disease (NAFLD), hepatocytes can undergo necroptosis, a regulated form of necrotic cell death mediated by the receptor-interacting protein kinase (RIPK) 1. We herein assessed the potential of RIPK1 and its downstream effector mixed lineage kinase domain-like protein (MLKL), as therapeutic targets and markers of activity in NAFLD. C57/BL6J-mice were fed a normal chow diet (NCD) or high fat diet (HFD). The effect of RIPA-56, a highly specific inhibitor of RIPK1, was evaluated in either a prophylactic or a curative treatment of HFD-fed mice, and in primary human steatotic hepatocytes. RIPK1 and MLKL concentrations were measured in the serum of patients with NAFLD. Both prophylactic and curative treatments of HFD-fed mice with RIPA-56, caused a down-regulation of MLKL and a reduction of liver injury, inflammation and fibrosis, characteristic of non-alcoholic steatohepatitis (NASH), as well as of steatosis. This latter effect was reproduced by treating primary human steatotic hepatocytes with RIPA-56 or necrosulfonamide (NSA), a specific inhibitor of human MLKL, and by knocking out (KO) MLKL in fat-loaded AML-12 mouse hepatocytes. MLKL KO in steatotic hepatocytes, caused an activation of the mitochondrial respiration, and an increase in β-oxidation. Along with MLKL decreased activation, RIPK3-KO mice exhibited increased activities of the liver mitochondrial respiratory chain complexes in experimental NASH. In patients with NAFLD, serum concentrations of RIPK1 and MLKL increased in correlation with the activity. The inhibition of RIPK1 improves NASH features in HFD-fed mice and reverses steatosis by an MLKL-dependent mechanism that involves at least partly an increase in mitochondrial respiration. RIPK1 and MLKL are potential serum markers of activity and promising therapeutic targets in NAFLD. Non-alcoholic fatty liver disease (NAFLD) has been paralleling the worldwide increase in obesity for the last decades. It has become the most common chronic liver disease, and now affects up to one third of the adult population in Western countries [1]. NAFLD encompasses a continuum of entities that span from plain steatosis to non-alcoholic steatohepatitis (NASH), cirrhosis and ultimately end-stage liver disease [2]. Hepatocyte cell death is a critical event in the progression of all chronic inflammatory liver diseases including NAFLD [3]. Until recently, two main forms of cell death were recognized: apoptosis, which occurs in a highly controlled manner, and necrosis that is accidentally triggered. However, during the past few years, it became clear that programmed cell death was not restricted to apoptosis, and comprised other forms of regulated cell death [4]. Necroptosis is one of them, combining the molecular machinery of the extrinsic apoptotic pathways with an execution similar to necrosis [4]. Unlike apoptosis that requires the activation of aspartate-specific proteases known as caspases [5], necroptosis is driven by the activation of the receptor-interacting protein kinase (RIPK) 1 and 3, and the pseudo kinase mixed lineage kinase domain-like (MLKL) [4]. Previous studies have shown that ablation of RIPK3 protected mice from the development of steatohepatitis and fibrosis in a methionine and choline-deficient (MCD) diet mouse model [6, 7]. In addition, it has been demonstrated that necroptosis was activated in the hepatocytes of patients with NASH [6-8]. Overall, compelling studies have shown that RIP kinases were pleiotropic modulators of cell death and participated in the pathogenesis of many chronic diseases [9]. Therefore, there is evidence to suggest that by preventing the formation and/or activation of the necrosome, one could limit cell death and possibly disease progression in NASH. 
DNAJB1-PRKACA fusion is a specific driver event in fibrolamellar carcinoma (FLC), a rare subtype of hepatocellular carcinoma (HCC) occurring in adolescents and young adults. In older patients, molecular determinants of HCC with mixed histological features of HCC and FLC (mixed-FLC/HCC) remain to be discovered. A series of 151 liver tumors including 126 HCC, 15 FLC, and 10 mixed-FLC/HCC were analyzed by RNAseq and whole-genome- or whole-exome-sequencing. Western-blots were performed to validate genomics discoveries. Results were validated using the TCGA database. Most of the mixed-FLC/HCC RNAseq clustered in a robust subgroup of 17 tumors all showing mutation or translocation inactivating BAP1 that codes for the BRCA1 associated protein-1. Similar to FLC, BAP1-HCC were significantly enriched in female, tumor fibrosis and the lack of chronic liver disease when compared to non-BAP1-HCC. However, patients were older and with a poorer prognosis than FLC patients. BAP1 tumors were immune hot, showed progenitor features, did not show DNAJB1-PRKACA fusion and were almost all non-mutated for CTNNB1, TP53 and TERT promoter. In contrast, 80% of the BAP1 tumors showed a chromosome gain of PRKACA at 19p13, combined with a loss of PRKAR2A (coding for the inhibitory regulatory subunit of PKA) at 3p21, leading to a high PRKACA/PRKAR2A ratio at the mRNA and protein levels. We have characterized a subgroup of BAP1-driven HCC bearing fibrolamellar-like features and a dysregulation of the PKA pathway, which could be at the root of the clinical and histological similarities between BAP1 tumors and DNAJB1-PRKACA FLCs. Fibrolamellar carcinoma (FLC) is a rare subtype of hepatocellular carcinoma (HCC) mostly diagnosed in adolescents and young adults. It was originally defined by specific histological features of both the tumor cells and their stroma, with the presence of abundant fibrosis arranged in a lamellar fashion around deeply eosinophilic large neoplastic hepatocytes, frequent central scar and calcifications1,2. FLC defines a specific subgroup of HCC since they have peculiar clinical features compared to classical HCC, such as a young age at onset between 10 to 35 years old, a balanced sex ratio, an absence of underlying liver disease or risk factors and a better prognosis with 80% of survival at 5-years after resection3,4. Biologically, FLCs show a high number of mitochondria and progenitor features, suggesting that the tumor cells are blocked at a specific stage of differentiation with hepatocellular (HEPAR1), biliary (CK7) and CD68 co-expressed markers1,5. In 2014, Honeyman and colleagues discovered a specific 400 kb chromosome deletion at chromosome 19 in FLC leading to recurrent chimeric DNAJB1-PRKACA gene fusion6. DNAJB1 encodes HSP40, a member of the heat shock protein family, while PRKACA codes for the cAMP-dependent protein kinase (PKA) catalytic subunit alpha; the chimeric gene results in PRKACA catalytic domain overexpression and subsequent PKA activation. In addition, rare FLC without DNAJB1-PRKACA fusion were identified in patients with Carney disease due to PRKAR1A germline mutations leading also to PKA activation and similar phenotype7. In contrast, PKA activation is only rarely identified in HCC (<1% GNAS mutations) or in cholangiocarcinoma (around 6% of GNAS mutations and rare fusions of PRKACA, PRKACAB and PRKAR1B)8,9. 
We undertook a cross-sectional study of children/adolescents with and without non-alcoholic fatty liver disease (NAFLD) to compare the prevalence of prediabetes and diabetes, and to examine the role of abnormal glucose tolerance as a predictor of liver disease severity. We recruited a cohort of 599 Caucasian children/adolescents with biopsy-proven NAFLD, and 118 children/adolescents without NAFLD, who were selected to be similar for age, sex, body mass index and waist circumference to those with NAFLD. The diagnosis of prediabetes and diabetes was based on either hemoglobin A1c, fasting plasma glucose or 2 h post-load glucose concentrations. Children/adolescents with NAFLD had a significantly higher prevalence of abnormal glucose tolerance (prediabetes or diabetes) than those without NAFLD (20.6% vs. 11%, p = 0.02). In particular, 124 (20.6%) children/adolescents with NAFLD had abnormal glucose tolerance, with 19.8% (n = 119) satisfying the diagnostic criteria for prediabetes and 0.8% (n = 5) satisfying the criteria for diabetes. The combined presence of prediabetes and diabetes was associated with a nearly 2.2-fold increased risk of non-alcoholic steatohepatitis (NASH; unadjusted odds ratio 2.19; 95% CI 1.47–3.29; p <0.001). However, this association was attenuated (but remained significant) after adjustment for age, sex, waist circumference (adjusted odds ratio 1.69, 95% CI 1.06–2.69, p = 0.032), and the PNPLA3 rs738409 polymorphism. Both this PNPLA3 polymorphism and waist circumference were strongly associated with NASH. Abnormal glucose tolerance (especially prediabetes) is highly prevalent among children/adolescents with biopsy-proven NAFLD. These children also have a higher risk of NASH, though central adiposity is the factor that is most strongly associated with NASH. With the growing prevalence of childhood obesity, pediatric non-alcoholic fatty liver disease (NAFLD) has emerged as the most common chronic liver disease in children and adolescents in Western countries.1–3 NAFLD is a spectrum of progressive liver disease that encompasses simple steatosis, non-alcoholic steatohepatitis (NASH), advanced fibrosis and, ultimately, cirrhosis.3,4 Compelling evidence indicates that NAFLD also has serious health consequences outside of the liver and is strongly associated with an increased risk of cardiovascular disease and abnormal glucose tolerance (prediabetes and type 2 diabetes).5–7 
Cholangiocarcinoma (CCA) carries a poor prognosis, is increasing in incidence and its causes are poorly understood. Although some risk factors are known, they vary globally and collectively account for a minority of cases. The aim of this study was to perform a comprehensive meta-analysis of risk factors for intrahepatic (iCCA) and extrahepatic cholangiocarcinoma (eCCA), from Eastern and Western world studies. A literature search of case-control studies was performed to identify potential risk factors for iCCA and eCCA. Pooled odds ratios (ORs) with 95% CIs and heterogeneity were calculated. Funnel plots were used to assess publication bias, and meta-regression was used to select risk factors for comparison between Eastern and Western studies. A total of 13 risk factors were selected from 25 case-control studies in 7 geographically diverse countries. The strongest risk factors for both iCCA and eCCA were biliary cysts and stones, cirrhosis, hepatitis B and hepatitis C. Choledochal cysts conferred the greatest risk of both iCCA and eCCA with pooled ORs of 26.71 (95% CI 15.80–45.16) and 34.94 (24.36–50.12), respectively. No significant associations were found between hypertension and obesity for either iCCA or eCCA. Comparing Eastern and Western populations, there was a difference for the association of hepatitis B with iCCA (coefficient = −0.15195; 95% CI −0.278 to −0.025; p = 0.022). This is the most comprehensive meta-analysis of CCA risk factors to date. Some risk factors, such as diabetes, although less strong, are increasing globally and may be contributing to rising rates of this cancer. Cholangiocarcinoma (CCA) is an exceptionally aggressive cancer arising from the biliary duct epithelium. CCAs represent approximately 3 to 5% of all malignancies of the gastrointestinal system. CCAs are classically sub-divided into 3 groups depending on the anatomical site of origin: intrahepatic CCA (iCCA), perihilar CCA (pCCA) and distal CCA (dCCA). iCCAs classically arise above the second-order bile ducts, whereas the anatomical point of distinction between perihilar cholangiocarcinomas (pCCAs) and distal cholangiocarcinomas (dCCAs) is the cystic duct.1–5 pCCA account for approximately 50–60% of all CCAs, dCCA 20–30%; and iCCA approximately 10–20%. iCCAs comprise about 10% of all primary liver cancers, making them the second most common primary hepatic malignancy after hepatocellular carcinoma.4,6 CCA typically presents late with non-specific symptoms. This is compounded by the lack of knowledge of risk factors in most cases and inaccurate screening tools, making the diagnosis of early, resectable disease uncommon. Beyond this stage, CCA is one of the most fatal cancers with a 5-year survival of approximately 5%.6 
Non-alcoholic fatty liver disease and alcohol-related liver disease pose an important challenge to current clinical healthcare pathways because of the large number of at-risk patients. Therefore, we aimed to explore the cost-effectiveness of transient elastography (TE) as a screening method to detect liver fibrosis in a primary care pathway. Cost-effectiveness analysis was performed using real-life individual patient data from 6 independent prospective cohorts (5 from Europe and 1 from Asia). A diagnostic algorithm with conditional inference trees was developed to explore the relationships between liver stiffness, socio-demographics, comorbidities, and hepatic fibrosis, the latter assessed by fibrosis scores (FIB-4, NFS) and liver biopsies in a subset of 352 patients. We compared the incremental cost-effectiveness of a screening strategy against standard of care alongside the numbers needed to screen to diagnose a patient with fibrosis stage ≥F2. The data set encompassed 6,295 participants (mean age 55 ± 12 years, BMI 27 ± 5 kg/m2, liver stiffness 5.6 ± 5.0 kPa). A 9.1 kPa TE cut-off provided the best accuracy for the diagnosis of significant fibrosis (≥F2) in general population settings, whereas a threshold of 9.5 kPa was optimal for populations at-risk of alcohol-related liver disease. TE with the proposed cut-offs outperformed fibrosis scores in terms of accuracy. Screening with TE was cost-effective with mean incremental cost-effectiveness ratios ranging from 2,570 €/QALY (95% CI 2,456–2,683) for a population at-risk of alcohol-related liver disease (age ≥45 years) to 6,217 €/QALY (95% CI 5,832–6,601) in the general population. Overall, there was a 12% chance of TE screening being cost saving across countries and populations. Screening for liver fibrosis with TE in primary care is a cost-effective intervention for European and Asian populations and may even be cost saving. Alcohol-related liver disease (ALD) and non-alcoholic fatty liver disease (NAFLD) are leading causes of chronic liver diseases, hepatocellular carcinoma (HCC) and liver-related deaths worldwide.1,2 While the causes, consequences and treatment strategies for ALD and NAFLD are being studied and developed,3–5 the majority of patients are still diagnosed at an advanced stage of disease.6 Consequently, the course of action towards early disease detection from a public health perspective remains a grey area in hepatology.7 
Most cholesterol gallstones have a core consisting of inorganic and/or organic calcium salts, although the mechanisms of core formation are poorly understood. We examined whether the paracellular permeability of ions at hepatic tight junctions is involved in the core formation of cholesterol gallstones, with particular interest in the role of phosphate ion, a common food additive and preservative. We focused on claudin-3 (Cldn3), a paracellular barrier-forming tight junction protein whose expression in mouse liver decreases with age. Since Cldn3-knockout mice exhibited gallstone diseases, we used them to assess the causal relationship between paracellular phosphate ion permeability and the core formation of cholesterol gallstones. In the liver of Cldn3-knockout mice, the paracellular phosphate ion permeability through hepatic tight junctions was significantly increased, resulting in calcium phosphate core formation. Cholesterol overdose caused cholesterol gallstone disease in these mice. We revealed that in the hepatobiliary system, Cldn3 functions as a paracellular barrier for phosphate ions, to help maintain biliary ion homeostasis. We provide in vivo evidence that elevated phosphate ion concentrations play a major role in the lifestyle- and age-related risks of developing cholesterol gallstone disease under cholesterol overdose. Gallstone diseases have various symptoms, including pain and fever, and can lead to life-threatening conditions such as acute cholangitis, acute cholecystitis, biliary pancreatitis, and cancer of the biliary tract.1,2 Given that gallstone diseases have a high incidence rate, are costly to treat surgically, and are responsible for lost work hours due to symptoms, preventing these diseases should have deep impacts on health and society.1 
Acute-on-chronic liver failure (ACLF) is characterized by acute decompensation of cirrhosis, development of organ failure and high short-term mortality. Whether the outcome in patients admitted to the intensive care unit (ICU) with ACLF differs from other ICU populations is unknown. We compared the clinical course and host response in ICU patients with or without ACLF, matched for baseline severity of illness scores and characteristics. From the large prospective EPaNIC randomized control trial database (n = 4,640), 133 patients were identified with cirrhosis of whom 71 fulfilled the Chronic Liver Failure Consortium criteria for ACLF. These patients were matched for type and severity of illness and demographics to 71 septic and 71 medical ICU patients from the same database without chronic liver disease. Clinical, biochemical and outcome parameters were compared in this cohort study of 213 patients. In a subset of 100 patients, day 1 serum cytokines were quantified. The outcome of ACLF, when compared to septic or medical ICU patients, matched for baseline parameters of illness severity, was similar regarding length of ICU stay, development of new infections, organ failure and septic shock. ICU, hospital and 90-day mortality were similar between the groups. C-reactive protein and platelet levels were lower in patients with ACLF throughout the first week. Cytokines, including IL-10, IL-1β, IL-6, and IL-8, were similarly elevated in ACLF and septic ICU patients on day 1. However, TNF-α levels were higher in patients with ACLF. Patients with ACLF admitted to the ICU showed comparable clinical and ICU outcomes as ICU patients without chronic liver disease, but with similar baseline severity of illness characteristics. This suggests that ICU admission criteria should not be different in ACLF populations. Acute-on-chronic liver failure (ACLF) is a recently defined clinical entity characterized by acute deterioration of compensated or stable decompensated cirrhosis associated with organ failure and a high 28-day mortality.1–6 ACLF as an entity differs in its clinical presentation and prognosis from alcoholic hepatitis and decompensated cirrhosis without organ failure.4,7 Also, the inflammatory response is more pronounced in ACLF compared to compensated or decompensated cirrhosis with preserved organ function.4 White blood cell counts, plasma pro-inflammatory cytokines and C-reactive protein (CRP) levels are higher in patients with ACLF than in cirrhotic patients without ACLF.8–10 This excessive inflammatory response may be an important contributing factor in the development of organ failure and immune exhaustion which may predispose to the development of new infections especially in hospitalized individuals.1,11 
Sorafenib is associated with multiple adverse events (AEs), potentially causing its permanent interruption. It is unknown how physicians’ experience has impacted on the management of these AEs and consequently on clinical outcomes. We aimed to assess whether AE management changed over time and if these modifications impacted on treatment duration and overall survival (OS). We analysed the prospectively collected data of 338 consecutive patients who started sorafenib between January 2008 and December 2017 in 3 tertiary care centres in Italy. Patients were divided according to the starting date: Group A (2008–2012; n = 154), and Group B (2013–2017, n = 184). Baseline and follow-up data were compared. In the OS analysis, patients who received second-line treatments were censored when starting the new therapy. Baseline characteristics, AEs, and radiological response were consistent across groups. Patients in Group B received a lower median daily dose (425 vs. 568 mg/day, p <0.001) due to more frequent dose modifications. However, treatment duration was longer (5.8 vs. 4.1 months, p = 0.021) with a trend toward a higher cumulative dose in Group B. Notably, the OS was also higher (12.0 vs. 11.0 months, p = 0.003) with a sharp increase in the 2-year survival rate (28.1 vs. 18.4%, p = 0.003) in Group B. Multivariate time-dependent Cox regression analysis confirmed later period of treatment (2013–2017) as an independent predictor of survival (HR 0.728; 95% CI 0.581–0.937; p = 0.013). Unconsidered confounders were unlikely to affect these results at the sensitivity analysis. Experience in the management of sorafenib-related AEs prolongs treatment duration and survival. This factor should be considered in the design of future randomised clinical trials including a sorafenib treatment arm, as an underestimate of sample size may derive. Sorafenib is a multitarget tyrosine kinase inhibitor (TKI) currently used for the treatment of hepatocellular carcinoma (HCC) not amenable to surgery or locoregional treatments.1 Sorafenib significantly prolongs patients’ overall survival (OS), but its use is associated with different adverse events (AEs), mainly dermatological, gastrointestinal and cardiovascular.2,3 The management of these AEs can require dose reductions and temporary interruptions. In a sizeable proportion of patients, however, these modifications are not able to avoid intolerable or severe AEs, resulting in permanent drug discontinuation.1 It might seem common sense that the experience accumulated in the prescription of sorafenib could lead to improved management of its related AEs. However, the exact impact of the operators' experience on the prescribing patterns of sorafenib has rarely been investigated.4 Similarly, it is not known whether this phenomenon can lead to an increase in OS. The latter point is of crucial importance, as dermatological AEs have been demonstrated to have a favourable prognostic impact.5,6 Thus, avoiding a definitive suspension of sorafenib in these patients would be extremely beneficial. Moreover, data from observational studies7,8 and randomised clinical trials (RCTs)9 indirectly seem to suggest that the OS of patients treated with sorafenib is progressively increasing. However, the reasons for this phenomenon have not been fully elucidated. A very recent monocentric study10 suggested that the management of sorafenib AEs has improved over time, but rigorous multicentric studies that consider time-dependent variables, addressing the possible confounding factor of second-line treatments, and providing a confirmation of the survival benefit through multivariable regressions models are still lacking. 
The extrahepatic bile duct is the primary tissue initially affected by biliary atresia. Biliary atresia is a cholangiopathy which exclusively affects neonates. Current animal models suggest that the developing bile duct is uniquely susceptible to damage. In this study, we aimed to define the anatomical and functional differences between the neonatal and adult mouse extrahepatic bile ducts. We studied mouse passaged cholangiocytes, mouse BALB/c neonatal and adult primary cholangiocytes, as well as isolated extrahepatic bile ducts, and a collagen reporter mouse. The methods used included transmission electron microscopy, lectin staining, immunostaining, rhodamine uptake assays, bile acid toxicity assays, and in vitro modeling of the matrix. The cholangiocyte monolayer of the neonatal extrahepatic bile duct was immature, lacking the uniform apical glycocalyx and mature cell-cell junctions typical of adult cholangiocytes. Functional studies showed that the glycocalyx protected against bile acid injury and that neonatal cholangiocyte monolayers were more permeable than adult monolayers. In adult ducts, the submucosal space was filled with collagen I, elastin, hyaluronic acid, and proteoglycans. In contrast, the neonatal submucosa had little collagen I and elastin, although both increased rapidly after birth. In vitro modeling of the matrix suggested that the composition of the neonatal submucosa relative to the adult submucosa led to increased diffusion of bile. A Col-GFP reporter mouse showed that cells in the neonatal but not adult submucosa were actively producing collagen. We identified 4 key differences between the neonatal and adult extrahepatic bile duct. We showed that these features may have functional implications, suggesting the neonatal extrahepatic bile ducts are particularly susceptible to injury and fibrosis. Mammalian bile travels through an arborizing network, encompassing bile canaliculi, the canals of Hering and ductules, and cholangiocyte-lined intrahepatic (IHBD) and extrahepatic (EHBD) bile ducts. The IHBD and EHBD arise from different primordia: the cranial portion of the endodermal diverticulum gives rise to the liver and the IHBD, while the caudal portion develops into the EHBD.1–3 The EHBD and IHBD, in addition to differing in development, size, and surrounding tissue, are also notable for differential susceptibility to certain cholangiopathies including primary biliary cholangitis, primary sclerosing cholangitis, and the neonatal fibro-obliterative disease biliary atresia (BA). BA, which at the time of presentation primarily affects the EHBD, is specific to neonates, who are born apparently healthy but progress remarkably quickly (within several months) to liver fibrosis and cirrhosis. Recent human and animal model data suggest that the initial insult is prenatal, affecting the neonate but sparing the mother.4–8 This suggests a paradigm whereby the fetal and neonatal ducts, especially the EHBD, are uniquely susceptible to initial and progressive damage from injurious agents. 
Donation after circulatory death (DCD) in the UK has tripled in the last decade. However, outcomes following DCD liver transplantation are worse than for donation after brainstem death (DBD) liver transplants. This study examines whether a recipient should accept a “poorer quality” DCD organ or wait longer for a “better” DBD organ. Data were collected on 5,825 patients who were registered on the elective waiting list for a first adult liver-only transplant and 3,949 patients who received a liver-only transplant in the UK between 1 January 2008 and 31 December 2015. Survival following deceased donor liver transplantation performed between 2008 and 2015 was compared by Cox regression modelling to assess the impact on patient survival of accepting a DCD liver compared to deferring for a potential DBD transplant. A total of 953 (23%) of the 3,949 liver transplantations performed utilised DCD donors. Five-year post-transplant survival was worse following DCD than DBD transplantation (69.1% [DCD] vs. 78.3% [DBD]; p <0.0001: adjusted hazard ratio [HR] 1.65; 95% CI 1.40–1.94). Of the 5,798 patients registered on the transplant list, 1,325 (23%) died or were removed from the list without receiving a transplant. Patients who received DCD livers had a lower risk-adjusted hazard of death than those who remained on the waiting list for a potential DBD organ (adjusted HR 0.55; 95% CI 0.47–0.65). The greatest survival benefit was in those with the most advanced liver disease (adjusted HR 0.19; 95% CI 0.07–0.50). Although DCD liver transplantation leads to worse transplant outcomes than DBD transplantation, the individual’s survival is enhanced by accepting a DCD offer, particularly for patients with more severe liver disease. DCD liver transplantation improves overall survival for UK listed patients and should be encouraged. Rates of liver failure are increasing dramatically in the UK1 and over a million people worldwide die of cirrhosis every year.2 Liver transplantation is the only effective treatment for end-stage liver disease and provides an average of 17–22 years of additional life.1,3,4 Access to liver transplantation is limited by donor organ availability, and over the last decade, as the incidence of liver disease has increased,1 the number of patients on the liver transplant waiting list in the UK has roughly doubled. Consequently, after 2 years, about 13% of listed patients will no longer be eligible for liver transplantation because of death or deterioration in their condition.5,6 These waiting list pressures have prompted a focus on the use of organs from donation after circulatory death (DCD) donors, with numbers of DCD donors increasing markedly over the last decade, such that they now almost match annual numbers of donation after brain death (DBD) donors in the UK.5 Worldwide, only the Netherlands achieves similar numbers of DCD donors per million population.7 
Carcinogen-induced mouse models of liver cancer are used extensively to study the pathogenesis of the disease and are critical for validating candidate therapeutics. These models can recapitulate molecular and histological features of human disease. However, it is not known if the genomic alterations driving these mouse tumour genomes are comparable to those found in human tumours. Herein, we provide a detailed genomic characterisation of tumours from a commonly used mouse model of hepatocellular carcinoma (HCC). We analysed whole exome sequences of liver tumours arising in mice exposed to diethylnitrosamine (DEN). Mutational signatures were compared between liver tumours from DEN-treated and untreated mice, and human HCCs. DEN-initiated tumours had a high, uniform number of somatic single nucleotide variants (SNVs), with few insertions, deletions or copy number alterations, consistent with the known genotoxic action of DEN. Exposure of hepatocytes to DEN left a reproducible mutational imprint in resulting tumour exomes which we could computationally reconstruct using six known COSMIC mutational signatures. The tumours carried a high diversity of low-incidence, non-synonymous point mutations in many oncogenes and tumour suppressors, reflecting the stochastic introduction of SNVs into the hepatocyte genome by the carcinogen. We identified four recurrently mutated genes that were putative oncogenic drivers of HCC in this model. Every neoplasm carried activating hotspot mutations either in codon 61 of Hras, in codon 584 of Braf or in codon 254 of Egfr. Truncating mutations of Apc occurred in 21% of neoplasms, which were exclusively carcinomas supporting a role for deregulation of Wnt/β-catenin signalling in cancer progression. Our study provides detailed insight into the mutational landscape of tumours arising in a commonly used carcinogen model of HCC, facilitating the future use of this model to better understand the human disease. Hepatocellular carcinoma (HCC) is the predominant form of primary liver cancer, which is currently the sixth most frequently diagnosed human cancer. Liver cancer is the second most common cause of cancer death globally and its incidence is increasing in countries with historically low rates.1,2 HCC typically develops in the context of end-stage liver disease, resulting from chronic inflammation, fibrosis and cirrhosis, and is almost exclusively caused by environmental risk factors, such as chronic hepatitis virus infection, aflatoxin B exposure, chronic alcohol consumption, and metabolic syndrome.3 This diversity of aetiologies appears to be reflected in the molecular heterogeneity of the disease. Over the last few years, next generation sequencing analyses of hundreds of human liver tumours have identified several oncogenic pathways and a wide range of putative driver gene mutations underlying hepatocarcinogenesis.4–9 
Chronic hepatitis B (CHB) affects over 2 million people in the US, with little reported on healthcare utilization and cost. We aimed to quantify annual CHB utilization and costs by disease severity and payer type. Using Commercial, Medicare, and Medicaid databases from 2004 to 2015 and ICD9 codes, we retrospectively identified adults with CHB, analyzing all-cause inpatient, outpatient, and pharmaceutical utilization and costs by disease severity. We compared healthcare utilization and costs between patients with CHB, without advanced liver disease, and matched non-CHB controls. All-cause inpatient, outpatient, and pharmaceutical utilization and costs were reported for each year and adjusted to 2015 dollars. Our sample consisted of 33,904 CHB cases and 86,072 non-CHB controls. All-cause inpatient admissions (average stay 6–10 days) were more frequent in advanced liver disease states. Across all payers, patients with decompensated cirrhosis had the highest emergency department utilization (1.6–2.8 annual visits) and highest mean annual costs. The largest all-cause cost components for Commercial and Medicaid were inpatient costs for all advanced liver disease groups (Commercial: 62%, 47%, 68%; Medicaid: 81%, 72%, 74%, respectively), and decompensated cirrhosis and hepatocellular carcinoma groups for Medicare (Medicare 49% and 48%). In addition, patients with compensated liver disease incurred costs 3 times higher than non-CHB controls. Patients with CHB, regardless of payer, who experienced decompensated cirrhosis, hepatocellular carcinoma, or a liver transplant incurred the highest annual costs and utilization of healthcare resources, but even patients with CHB and compensated liver disease incurred higher costs than those without CHB. All stakeholders in disease management need to combine efforts to prevent infection and advanced liver disease through improved vaccination rates, earlier diagnosis, and treatment. Chronic hepatitis B (CHB)-induced liver disease can progress to compensated cirrhosis, hepatic decompensation, hepatocellular carcinoma (HCC), and death; and such progression occurs over many years for most patients. CHB currently affects approximately 240 million individuals worldwide, including 2 million individuals within the US, and is associated with substantial healthcare utilization.1–4 
T cells are central mediators of liver inflammation and represent potential treatment targets in cholestatic liver disease. Whereas emerging evidence shows that bile acids (BAs) affect T cell function, the role of T cells for the regulation of BA metabolism is unknown. In order to understand this interplay, we investigated the influence of T cells on BA metabolism in a novel mouse model of cholangitis. Mdr2−/− mice were crossed with transgenic K14-OVAp mice, which express an MHC class I restricted ovalbumin peptide on biliary epithelial cells (Mdr2−/−xK14-OVAp). T cell-mediated cholangitis was induced by the adoptive transfer of antigen-specific CD8+ T cells. BA levels were quantified using a targeted liquid chromatography-mass spectrometry-based approach. T cell-induced cholangitis resulted in reduced levels of unconjugated BAs in the liver and significantly increased serum and hepatic levels of conjugated BAs. Genes responsible for BA synthesis and uptake were downregulated and expression of the bile salt export pump was increased. The transferred antigen-specific CD8+ T cells alone were able to induce these changes, as demonstrated using Mdr2−/−xK14-OVAp recipient mice on the Rag1−/− background. Mechanistically, we showed by depletion experiments that alterations in BA metabolism were partly mediated by the proinflammatory cytokines TNF and IFN-γ in an FXR-dependent manner, a process that in vitro required cell contact between T cells and hepatocytes. Whereas it is known that BA metabolism is dysregulated in sepsis and related conditions, we have shown that T cells are able to control the synthesis and metabolism of BAs, a process which depends on TNF and IFN-γ. Understanding the effect of lymphocytes on BA metabolism will help in the design of combined treatment strategies for cholestatic liver diseases. In primary biliary cholangitis (PBC) and primary sclerosing cholangitis (PSC), T cells are likely involved in targeting biliary epithelial cells.1 Chronic bile duct inflammation leads to cholestasis, biliary fibrosis and potential malignant transformation in the case of PSC.2 Adaptive immune responses are considered to contribute to early as well as late stages of disease.3 Numerous genetic risk loci for PBC and PSC were identified that encode for genes involved in adaptive immune responses4–6 and portal infiltrates that were predominantly composed of CD4+ and CD8+ T cells have been described in the livers of patients with PBC and PSC.7–9 Moreover, autoantigens within the pyruvate dehydrogenase complex (PDC-E2) are targeted by lymphocytes in PBC.10 In PSC, impaired T cell homeostasis including an imbalance of IL-17 producing T cells and regulatory T cells has been described.11–16 Since current treatment options for these diseases are limited, T cells are an interesting target for novel combination therapies. 
The most prescribed non-nucleoside reverse transcriptase inhibitor, efavirenz, has been associated with elevated risk of dyslipidemia and hepatic steatosis in HIV-infected patients but the underlying mechanisms remain elusive. Herein, we investigated the role of pregnane X receptor (PXR) in mediating the adverse effects of efavirenz on lipid homeostasis. Cell-based reporter assays, primary cell culture, and multiple mouse models including conditional knockout and humanized mice were combined to study the impact of efavirenz on PXR activities and lipid homeostasis in vitro and in vivo. A novel liver-specific Pxr knockout mouse model was also generated to determine the contribution of hepatic PXR signaling to efavirenz-elicited dyslipidemia and hepatic steatosis. We found that efavirenz is a potent PXR-selective agonist that can efficiently activate PXR and induce its target gene expression in vitro and in vivo. Treatment with efavirenz-induced hypercholesterolemia and hepatic steatosis in mice but deficiency of hepatic PXR abolished these adverse effects. Interestingly, efavirenz-mediated PXR activation regulated the expression of several key hepatic lipogenic genes including fatty acid transporter CD36 and cholesterol biosynthesis enzyme squalene epoxidase (SQLE), leading to increased lipid uptake and cholesterol biosynthesis in hepatic cells. While CD36 is a known PXR target gene, we identified a DR-2-type of PXR-response element in the SQLE promoter and established SQLE as a direct transcriptional target of PXR. Since PXR exhibits considerable differences in its pharmacology across species, we also confirmed these findings in PXR-humanized mice and human primary hepatocytes. The widely prescribed antiretroviral drug efavirenz induces hypercholesterolemia and hepatic steatosis by activating PXR signaling. Activation of PXR should be taken into consideration for patients undergoing long-term treatment with PXR agonistic antiretroviral drugs. As the average lifespan of HIV-infected patients receiving antiretroviral therapy (ART) lengthens, morbidity and mortality from cardiovascular disease (CVD) pose considerable challenges.1–4 Not only does HIV infection elevate the risk of CVD, but the use of antiretroviral (ARV) drugs has also been associated with dyslipidemia and increased risk of CVD, thereby exacerbating a serious health challenge for long-term survivors.1–2,4 Current optimal ART options consist of a combination of several drug classes including protease inhibitors (PIs), nucleoside/nucleotide reverse transcriptase inhibitors (NRTIs), non-nucleoside reverse transcriptase inhibitor (NNRTIs), and integrase strand transfer inhibitors (INSTIs, also known as integrase inhibitors).2,3 Although several first-generation PIs such as amprenavir and ritonavir with documented dyslipidemic properties have been used on a limited basis, many currently recommended first-line ARV drugs have also been associated with dyslipidemia and increased CVD risk.2–6 For example, the NNRTI efavirenz, one of the most prescribed antiretroviral drugs worldwide, has been consistently associated with dyslipidemia including increased total, low-density lipoprotein (LDL), and high-density lipoprotein (HDL) cholesterol levels in multiple clinical studies.2,5–7 Despite the strong evidence linking certain ARV drugs with dyslipidemia and CVD risk, the underlying mechanisms responsible for the adverse effects of ARV drugs remain elusive. 
Irisin, the cleaved extra-cellular fragment of the Fibronectin type III domain-containing protein 5 (FNDC5) is a myokine that is proposed to have favorable metabolic activity. We aimed to elucidate the currently undefined role of variants in the FNDC5 gene in non-alcoholic fatty liver disease (NAFLD). We prioritized single nucleotide polymorphisms in FNDC5 on the basis of their putative biological function and identified rs3480 in the 3′ untranslated region (3′UTR). We studied the association of rs3480 with liver disease severity and the metabolic profile of 987 Caucasian patients with NAFLD. Functional investigations were undertaken using luciferase reporter assays of the 3′UTR of human FNDC5, pyrosequencing for allele-specific expression of FNDC5 in liver, measurement of serum irisin, and bioinformatics analysis. The rs3480 (G) allele was associated with advanced steatosis (OR 1.29; 95% CI 1.08–1.55; p = 0.004), but not with other histological features. This effect was independent but additive to PNPLA3 and TM6SF2. The rs3480 polymorphism influenced FNDC5 mRNA stability and the binding of miR-135a-5P. Compared with controls, hepatic expression of this microRNA was upregulated while FNDC5 expression was downregulated. Elevated serum irisin was associated with reduced steatosis, and an improved metabolic profile. Carriage of the FNDC5 rs3480 minor (G) allele is associated with more severe steatosis in NAFLD through a microRNA-mediated mechanism controlling FNDC5 mRNA stability. Irisin is likely to have a favorable metabolic impact on NAFLD. Non-alcoholic fatty liver disease (NAFLD) is a principal liver disorder in Western countries and is on trajectory to become the leading cause of end-stage liver disease and liver transplantation.1–3 Notably, NAFLD is part of a multisystem metabolic disturbance, as its impact is not limited to the liver but also affects extra-hepatic sites such as the cardiovascular system and kidneys.4 In NAFLD, there is pathological hepatic accumulation of fat that over time can lead to inflammation and progress to cirrhosis, end-stage liver disease and hepatocellular carcinoma. The mechanisms underlying the accumulation of liver fat are complex and gene × environment interactions play a critical role.5–7 
Progressive familial intrahepatic cholestasis type 3 (PFIC3), for which there are limited therapeutic options, often leads to end-stage liver disease before adulthood due to impaired ABCB4-dependent phospholipid transport to bile. Using adeno-associated virus serotype 8 (AAV8)-mediated gene therapy, we aimed to restore the phospholipid content in bile to levels that prevent liver damage, thereby enabling stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Ten-week-old Abcb4−/− mice received a single dose of AAV8-hABCB4 (n = 10) or AAV8-GFP (n = 7) under control of a liver specific promoter via tail vein injection. Animals were sacrificed either 10 or 26 weeks after vector administration to assess transgene persistence, after being challenged with a 0.1% cholate diet for 2 weeks. Periodic evaluation of plasma cholestatic markers was performed and bile duct cannulation enabled analysis of biliary phospholipids. Liver fibrosis and the Ki67 proliferation index were assessed by immunohistochemistry. Stable transgene expression was achieved in all animals that received AAV8-hABCB4 up to 26 weeks after administration. AAV8-hABCB4 expression restored biliary phospholipid excretion, increasing the phospholipid and cholesterol content in bile to levels that ameliorate liver damage. This resulted in normalization of the plasma cholestatic markers, alkaline phosphatase and bilirubin. In addition, AAV8-hABCB4 prevented progressive liver fibrosis and reduced hepatocyte proliferation for the duration of the study. Liver-directed gene therapy provides stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Translational studies that verify the clinical feasibility of this approach are warranted. Progressive familial intrahepatic cholestasis type 3 (PFIC3) is an autosomal-recessive liver disorder. Patients with PFIC3 present with cholestasis at a young age, which progresses to cirrhosis and end-stage liver disease before adulthood.1–3 PFIC3 is caused by impairment of phosphatidylcholine (PC) translocation to bile by the canalicular membrane protein ATP binding cassette subfamily B member 4 (ABCB4), formerly known as multidrug resistance protein 3 (MDR3), encoded by the ABCB4 gene.4–6 In bile, PC is essential in the formation of mixed micelles with bile salts that protect the lining of the biliary tree from the detergent properties of bile salts. In the absence of PC transport, bile salt–induced cytotoxicity causes progressive destruction of cholangiocytes, mainly of small bile ducts, and hepatocytes leading to intrahepatic cholestasis and progressive liver damage.4,7 
Patients with advanced liver fibrosis remain at risk of cirrhosis-related outcomes and those with severe comorbidities may not benefit from hepatitis C (HCV) eradication. We aimed to collect data on all-cause mortality and relevant clinical events within the first two years of direct-acting antiviral therapy, whilst determining the prognostic capability of a comorbidity-based model. This was a prospective non-interventional study, from the beginning of direct-acting antiviral therapy to the event of interest (mortality) or up to two years of follow-up, including 14 Spanish University Hospitals. Patients with HCV infection, irrespective of liver fibrosis stage, who received direct-acting antiviral therapy were used to build an estimation and a validation cohort. Comorbidity was assessed according to Charlson comorbidity and CirCom indexes. A total of 3.4% (65/1,891) of individuals died within the first year, while 5.4% (102/1,891) died during the study. After adjusting for cirrhosis, platelet count, alanine aminotransferase and sex, the following factors were independently associated with one-year mortality: Charlson index (hazard ratio [HR] 1.55; 95% CI 1.29–1.86; p = 0.0001), bilirubin (HR 1.39; 95% CI 1.11–1.75; p = 0.004), age (HR 1.06 95% CI 1.02–1.11; p = 0.005), international normalized ratio (HR 3.49; 95% CI 1.36–8.97; p = 0.010), and albumin (HR 0.18; 95% CI 0.09–0.37; p = 0.0001). HepCom score showed a good calibration and discrimination (C-statistics 0.90), and was superior to the other prognostic scores (model for end-stage liver disease 0.81, Child-Pugh 0.72, CirCom 0.68) regarding one- and two-year mortality. HepCom score identified low- (≤5.7 points: 2%–3%) and high-risk (≥25 points: 56%–59%) mortality groups, both in the estimation and validation cohorts. The distribution of clinical events was similar between groups. The HepCom score, a combination of Charlson comorbidity index, age, and liver function (international normalized ratio, albumin, and bilirubin) enables detection of a group at high risk of one- and two-year mortality, and relevant clinical events, after starting direct-acting antiviral therapy. The advent of new direct-acting antivirals (DAAs) has dramatically changed the landscape of hepatitis C (HCV) treatment, by increasing efficacy1 and safety.2 The cure of infection creates a rupture in the natural history of the HCV disease. Indeed, patients achieving sustained virologic response (SVR) showed decreased rates of all-cause mortality, hepatocellular carcinoma (HCC) and decompensated cirrhosis, as well as less need for liver transplantation.3 However, patients with advanced liver fibrosis remain at risk of cirrhosis-related outcomes and patients with severe comorbidities may not benefit from viral clearance. In this setting of excellent results, these factors could be disappointing for physicians, patients, and relatives. 
In patients with biliary atresia (BA), the rate of native liver survival (NLS) to adulthood has been reported as 14–44% worldwide. Complications related to portal hypertension (PHT) and cholangitis are common in adulthood. For those requiring liver transplantation (LT), the timing can be challenging. The aim of this study was to identify variables that could predict whether young people with BA would require LT when they are >16 years of age. This study was a single-centre retrospective analysis of 397 patients who underwent Kasai portoenterostomy (KP) between 1980–96 in the UK. After KP, 111/397 (28%) demonstrated NLS until 16 years of age. At final follow-up, 67 showed NLS when >16 years old (Group 1) and 22 required LT when >16 years old (Group 2). Laboratory, clinical and radiological parameters were collected for both groups at a median age of 16.06 years (13.6–17.4 years). The need for LT when >16 years old was associated with higher total bilirubin (hazard ratio 1.03, p = 0.019) and lower creatinine (hazard ratio 0.95, p = 0.040), at 16 years, on multivariate analysis. Receiver-operating characteristic curve analysis demonstrated that a total bilirubin level of ≥21 µmol/L at 16 years old (AUROC = 0.848) predicted the need for LT when >16 years old, with 85% sensitivity and 74% specificity. Cholangitis episode(s) during adolescence were associated with a 5-fold increased risk of needing LT when >16 years old. The presence of PHT or gastro-oesophageal varices in patients <16 years old was associated with a 7-fold and 8.6-fold increase in the risk of needing LT, respectively. BA in adulthood requires specialised management. Adult liver disease scoring models are not appropriate for this cohort. Bilirubin ≥21 µmol/L, PHT or gastro-oesophageal varices at 16 years, and cholangitis in adolescence, can predict the need for future LT in young people with BA. Low creatinine at 16 years also has potential prognostic value. Biliary atresia (BA) is an idiopathic neonatal cholangiopathy characterized by progressive inflammatory obliteration of the intrahepatic or extrahepatic bile ducts.1 The surgical procedure, Kasai portoenterostomy (KP), aims to restore bile flow, and alleviate jaundice. Liver transplantation (LT) is performed in cases where KP is unable to salvage the native liver, with complications including jaundice, cholangitis, portal hypertension (PHT) and/or synthetic failure.2 Five and 10-year UK native liver survival (NLS) rates in BA have been documented as 46% and 40%, respectively.3 
γδ T cells comprise a substantial proportion of tissue-associated lymphocytes. However, our current understanding of human γδ T cells is primarily based on peripheral blood subsets, while the immunobiology of tissue-associated subsets remains largely unclear. Therefore, we aimed to elucidate the T cell receptor (TCR) diversity, immunophenotype and function of γδ T cells in the human liver. We characterised the TCR repertoire, immunophenotype and function of human liver infiltrating γδ T cells, by TCR sequencing analysis, flow cytometry, in situ hybridisation and immunohistochemistry. We focussed on the predominant tissue-associated Vδ2− γδ subset, which is implicated in liver immunopathology. Intrahepatic Vδ2− γδ T cells were highly clonally focussed, with single expanded clonotypes featuring complex, private TCR rearrangements frequently dominating the compartment. Such T cells were predominantly CD27lo/− effector lymphocytes, whereas naïve CD27hi, TCR-diverse populations present in matched blood were generally absent in the liver. Furthermore, while a CD45RAhi Vδ2− γδ effector subset present in both liver and peripheral blood contained overlapping TCR clonotypes, the liver Vδ2− γδ T cell pool also included a phenotypically distinct CD45RAlo effector compartment that was enriched for expression of the tissue tropism marker CD69, the hepatic homing chemokine receptors CXCR3 and CXCR6, and liver-restricted TCR clonotypes, suggestive of intrahepatic tissue residency. Liver infiltrating Vδ2− γδ cells were capable of polyfunctional cytokine secretion, and unlike peripheral blood subsets, were responsive to both TCR and innate stimuli. These findings suggest that the ability of Vδ2− γδ T cells to undergo clonotypic expansion and differentiation is crucial in permitting access to solid tissues, such as the liver, which results in functionally distinct peripheral and liver-resident memory γδ T cell subsets. They also highlight the inherent functional plasticity within the Vδ2− γδ T cell compartment and provide information that could be used for the design of cellular therapies that suppress liver inflammation or combat liver cancer. γδ T cells are unconventional lymphocytes enriched in solid tissues, where they are thought to play critical roles in immunosurveillance.1 Studies of mouse tissue-associated γδ subsets suggest γδ T cell function can be predominantly innate-like, involving semi-invariant T cell subsets that enable fast response kinetics without a requirement for clonal selection and differentiation.2–5 This role may allow for rapid ‘lymphoid stress surveillance’, limiting damage to host tissues in the face of microbial or non-microbial challenges, prior to full activation of adaptive immunity.4,6 As such, γδ T cells may critically complement the contributions of tissue-resident αβ subsets, which provide an augmented adaptive response to infections re-encountered at body surfaces,7 potentially explaining the retention of γδ T cells alongside the αβ T cell and B cell lineage over 450 million years of vertebrate evolution.8 
Previous prognostic scores for transarterial chemoembolization (TACE) were mainly derived from real-world settings, which are beyond guideline recommendations. A robust model for outcome prediction and risk stratification of recommended TACE candidates is lacking. We aimed to develop an easy-to-use tool specifically for these patients. Between January 2010 and May 2016, 1,604 treatment-naïve patients with unresectable hepatocellular carcinoma (HCC), Child-Pugh A5-B7 and performance status 0 undergoing TACE were included from 24 tertiary centres. Patients were randomly divided into training (n = 807) and validation (n = 797) cohorts. A prognostic model was developed and subsequently validated. Predictive performance and discrimination were further evaluated and compared with other prognostic models. The final presentation of the model was “linear predictor = largest tumour diameter (cm) + tumour number”, which consistently outperformed other currently available models in both training and validation datasets as well as in different subgroups. The thirtieth percentile and the third quartile of the linear predictor, namely 6 and 12, were further selected as cut-off values, leading to the “six-and-twelve” score which could divide patients into 3 strata with the sum of tumour size and number ≤6, >6 but ≤12, and >12 presenting significantly different median survival of 49.1 (95% CI 43.7–59.4) months, 32.0 (95% CI 29.9–37.5) months, and 15.8 (95% CI 14.1–17.7) months, respectively. The six-and-twelve score may prove an easy-to-use tool to stratify recommended TACE candidates (Barcelona Clinic Liver Cancer stage-A/B) and predict individual survival with favourable performance and discrimination. Moreover, the score could stratify these patients in clinical practice as well as help design clinical trials with comparable criteria involving these patients. Further external validation of the score is required. According to the guidelines of the American Association for the Study of Liver Disease (AASLD) and the European Association for the Study of Liver (EASL), transarterial chemoembolization (TACE) is currently the only recommended treatment option for patients with intermediate stage hepatocellular carcinoma (HCC),1–3 with well-preserved liver function and performance status.4–6 These patients, as well as those at early stage but considered unresectable due to tumour size, location, patient age, and suggestions of stage migration are considered “recommended” or “ideal” TACE candidates, i.e., the best target population for TACE, and have frequently been set as the target population in pivotal studies (Table S1A-1B).7–12 However, this population is rather heterogeneous with a variable median overall survival of 13–43 months,13 rendering it crucial to develop a risk stratification tool.14 Indeed, the necessity of stratifying risk in these patients has also been underlined by the most recent guidelines.14 More importantly, a pre-procedure prognostic model providing survival estimates after TACE as a reference may enable outcome comparisons with other treatments and thus could aid clinical decision making.15 
Advanced hepatocellular carcinoma (HCC) is a lethal malignancy with limited treatment options. Sorafenib is the only FDA-approved first-line targeted drug for advanced HCC, but its effect on patient survival is limited. Further, patients ultimately present with disease progression. A better understanding of the causes of sorafenib resistance, enhancing the efficacy of sorafenib and finding a reliable predictive biomarker are crucial to achieve efficient control of HCC. The functional effects of ANXA3 in conferring sorafenib resistance to HCC cells were analyzed in apoptotic and tumorigenicity assays. The role of ANXA3/PKCδ-mediated p38 signaling, and subsequently altered autophagic and apoptotic events, was assessed by immunoprecipitation, immunoblotting, immunofluorescence and transmission electron microscopy assays. The prognostic value of ANXA3 in predicting response to sorafenib was evaluated by immunohistochemistry. The therapeutic value of targeting ANXA3 to combat HCC with anti-ANXA3 monoclonal antibody alone or in combination with sorafenib/regorafenib was investigated ex vivo and in vivo. ANXA3 conferred HCC cells with resistance to sorafenib. ANXA3 was found enriched in sorafenib-resistant HCC cells and patient-derived xenografts. Mechanistically, overexpression of ANXA3 in sorafenib-resistant HCC cells suppressed PKCδ/p38 associated apoptosis and activated autophagy for cell survival. Clinically, ANXA3 expression correlated positively with the autophagic marker LC3B in HCC and was associated with a worse overall survival in patients who went on to receive sorafenib treatment. Anti-ANXA3 monoclonal antibody therapy combined with sorafenib/regorafenib impaired tumor growth in vivo and significantly increased survival. Anti-ANXA3 therapy in combination with sorafenib/regorafenib represents a novel therapeutic strategy for HCC treatment. ANXA3 represents a useful predictive biomarker to stratify patients with HCC for sorafenib treatment. Hepatocellular carcinoma (HCC) is a very aggressive disease and represents the third leading cause of cancer-related mortality worldwide.1 Although treatment of HCC has greatly improved over the last decade, most patients with HCC are presented with an underlying chronic liver disease and diagnosed only at advanced stages, when they are no longer eligible for curative therapies such as liver resection or transplantation.2 In these cases, the multi-target tyrosine kinase inhibitor sorafenib is the only FDA-approved first-line systematic therapy, expanding patient median survival from 7.9 to 10.7 months.3–5 Despite initial response, only rarely do sorafenib-treated tumors regress completely, and the therapeutic effects of the drug are often temporary. Unfortunately, most patients develop disease progression, and in the case of HCC, radiological progression under sorafenib occurs after 4–5 months of treatment.4 Regorafenib has recently been approved by the FDA as a second-line treatment for patients with advanced HCC, where regorafenib has provided some survival benefit in those progressing on sorafenib treatment.6 As sorafenib targets several signaling pathways, acquisition of resistance might involve different mechanisms, including the activation of compensatory signaling cascades, rather than specific DNA aberrations, as was described with BCR-ABL and imatinib7 and BRAF mutations in melanoma resistant to vemurafenib.8 Further, a targeted anti-cancer drug should ideally have a biomarker to predict its clinical response. Single kinase inhibitors, such as tarceva (EGFR inhibitor) or crizotinib (ALK inhibitor), have as predictive markers EGFR mutation and ALK translocation, respectively.9,10 However, such markers are still not available for sorafenib because it targets multiple kinases, complicating the mechanism of action. As the precise molecular mechanisms underlying resistance to sorafenib are still barely understood,11 there is an urgent need to characterize drivers of resistance to identify new biomarkers that can predict sorafenib treatment outcome and therapeutic strategy that can improve the efficacy of sorafenib. 
Population-level evidence for the impact of direct-acting antiviral (DAA) therapy on hepatitis C virus (HCV)-related disease burden is lacking. We aimed to evaluate trends in HCV-related decompensated cirrhosis and hepatocellular carcinoma (HCC) hospitalisation, and liver-related and all-cause mortality in the pre-DAA (2001–2014) and DAA therapy (2015–2017) eras in New South Wales, Australia. HCV notifications (1993–2016) were linked to hospital admissions (2001–2017) and mortality (1995–2017). Segmented Poisson regressions and Poisson regression were used to assess the impact of DAA era and factors associated with liver-related mortality, respectively. Among 99,910 people with an HCV notification, 3.8% had a decompensated cirrhosis diagnosis and 1.8% had an HCC diagnosis, while 3.3% and 10.5% died of liver-related and all-cause mortality, respectively. In the pre-DAA era, the number of decompensated cirrhosis and HCC diagnoses, and liver-related and all-cause mortality consistently increased (incidence rate ratios 1.04 [95% CI 1.04–1.05], 1.08 [95% CI 1.07–1.08], 1.07 [95% CI 1.06–1.07], and 1.05 [95% CI 1.04–1.05], respectively) over each 6-monthly band. In the DAA era, decompensated cirrhosis diagnosis and liver-related mortality numbers declined (incidence rate ratios 0.97 [95% CI 0.95–0.99] and 0.96 [95% CI 0.94–0.98], respectively), and HCC diagnosis and all-cause mortality numbers plateaued (incidence rate ratio 1.00 [95% CI 0.97–1.03] and 1.01 [95% CI 1.00–1.02], respectively) over each 6-monthly band. In the DAA era, alcohol-use disorder (AUD) was common in patients diagnosed with decompensated cirrhosis and HCC (65% and 46% had a history of AUD, respectively). AUD was independently associated with liver-related mortality (incidence rate ratio 3.35; 95% CI 3.14–3.58). In the DAA era, there has been a sharp decline in liver disease morbidity and mortality in New South Wales, Australia. AUD remains a major contributor to HCV-related liver disease burden, highlighting the need to address comorbidities. The advent of highly curative, tolerable, short-duration direct-acting antiviral (DAA) therapy for hepatitis C virus (HCV) infection has transformed clinical management, and provided great optimism for the global HCV response.1 The World Health Organization (WHO) have developed a global health strategy on viral hepatitis that incorporates key service and impact targets, including declines in HCV-related mortality of 10% by 2020 and 65% by 2030.2 
Non-selective beta-blockers (NSBBs) are the mainstay of primary prophylaxis of esophageal variceal bleeding in patients with liver cirrhosis. We investigated whether non-invasive markers of portal hypertension correlate with hemodynamic responses to NSBBs in cirrhotic patients with esophageal varices. In this prospective cohort study, 106 cirrhotic patients with high-risk esophageal varices in the derivation cohort received carvedilol prophylaxis, and completed paired measurements of hepatic venous pressure gradient, liver stiffness (LS), and spleen stiffness (SS) at the beginning and end of dose titration. LS and SS were measured using acoustic radiation force impulse imaging. A prediction model for hemodynamic response was derived, and subject to an external validation in the validation cohort (63 patients). Hemodynamic response occurred in 59 patients (55.7%) in the derivation cohort, and in 33 patients (52.4%) in the validation cohort, respectively. Multivariate logistic regression analysis identified that ΔSS was the only significant predictor of hemodynamic response (odds ratio 0.039; 95% confidence interval 0.008–0.135; p <0.0001). The response prediction model (ModelΔSS = 0.0490–2.8345 × ΔSS; score = (exp[ModelΔSS])/(1 + exp[ModelΔSS]) showed good predictive performance (area under the receiver-operating characteristic curve [AUC] = 0.803) using 0.530 as the threshold value. The predictive performance of the ModelΔSS in the validation set improved using the same threshold value (AUC = 0.848). A new model based on dynamic changes in SS exhibited good performance in predicting hemodynamic response to NSBB prophylaxis in patients with high-risk esophageal varices. Gastroesophageal varices are significant challenges that are commonly faced by patients with cirrhosis.1 Variceal hemorrhage occurs in 10–15% of patients annually depending on the size of varices, the presence of red wale marks, and the severity of liver disease.2 In addition, acute variceal hemorrhage carries significant risks of morbidity and mortality, with reported 6-week mortality rates of 15–25%.3 
Cerebral oxidative stress plays an important role in the pathogenesis of hepatic encephalopathy (HE), but the underlying mechanisms are incompletely understood. Herein, we analyzed a role of heme oxygenase (HO)1, iron and NADPH oxidase 4 (Nox4) for the induction of oxidative stress and senescence in HE. Gene and protein expression in human post-mortem brain samples was analyzed by gene array and western blot analysis. Mechanisms and functional consequences of HO1 upregulation were studied in NH4Cl-exposed astrocytes in vitro by western blot, qPCR and super-resolution microscopy. HO1 and the endoplasmic reticulum (ER) stress marker grp78 were upregulated, together with changes in the expression of multiple iron metabolism-related genes, in post-mortem brain samples from patients with liver cirrhosis and HE. NH4Cl elevated HO1 protein and mRNA in cultured astrocytes through glutamine synthetase (GS)-dependent upregulation of glutamine/fructose amidotransferases 1/2 (GFAT1/2), which blocked the transcription of the HO1-targeting miR326-3p in a O-GlcNAcylation dependent manner. Upregulation of HO1 by NH4Cl triggered ER stress and was associated with elevated levels of free ferrous iron and expression changes in iron metabolism-related genes, which were largely abolished after knockdown or inhibition of GS, GFAT1/2, HO1 or iron chelation. NH4Cl, glucosamine (GlcN) and inhibition of miR326-3p upregulated Nox4, while knockdown of Nox4, GS, GFAT1/2, HO1 or iron chelation prevented NH4Cl-induced RNA oxidation and astrocyte senescence. Elevated levels of grp78 and O-GlcNAcylated proteins were also found in brain samples from patients with liver cirrhosis and HE. The present study identified glucosamine synthesis-dependent protein O-GlcNAcylation as a novel mechanism in the pathogenesis of HE that triggers oxidative and ER stress, as well as senescence, through upregulation of HO1 and Nox4. Ammonia is a major toxin in the pathogenesis of hepatic encephalopathy (HE).1 In the brain, ammonia is detoxified by glutamine synthetase (GS) in astrocytes and glutamine accumulation in astrocytes in patients with liver cirrhosis and HE triggers the development of a low grade cerebral edema2 associated with oxidative/nitrosative stress.3,4 The present study identified the O-GlcNAcylation-dependent upregulation of HO1 and Nox4 as a novel mechanism underlying ammonia-induced RNA oxidation and astrocyte senescence (Fig. 7). Given the important role of oxidative stress for the pathogenesis of HE, and the inherent limitations of a broad antioxidant therapy, our study warrants further research on the suitability of HO1 or Nox4 inhibitors or iron chelators for the treatment of HE. SECTION Financial support
microRNAs (miRNAs) are deregulated in non-alcoholic fatty liver disease (NAFLD) and have been proposed as useful markers for the diagnosis and stratification of disease severity. We conducted a meta-analysis to identify the potential usefulness of miRNA biomarkers in the diagnosis and stratification of NAFLD severity. After a systematic review, circulating miRNA expression consistency and mean fold-changes were analysed using a vote-counting strategy. The sensitivity, specificity, positive and negative likelihood ratios, diagnostic odds ratio and area under the curve (AUC) for the diagnosis of NAFLD or non-alcoholic steatohepatitis (NASH) were pooled using a bivariate meta-analysis. Deeks’ funnel plot was used to assess the publication bias. Thirty-seven studies of miRNA expression profiles and six studies of diagnostic accuracy were ultimately included in the quantitative analysis. miRNA-122 and miRNA-192 showed consistent upregulation. miRNA-122 was upregulated in every scenario used to distinguish NAFLD severity. The miRNA expression correlation between the serum and liver tissue was inconsistent across studies. miRNA-122 distinguished NAFLD from healthy controls with an AUC of 0.82 (95% CI 0.75–0.89), and miRNA-34a distinguished non-alcoholic steatohepatitis (NASH) from non-alcoholic fatty liver (NAFL) with an AUC of 0.78 (95% CI 0.67–0.88). miRNA-34a, miRNA-122 and miRNA-192 were identified as potential diagnostic markers to segregate NAFL from NASH. Both miRNA-122, in distinguishing NAFLD from healthy controls, and miRNA-34a, in distinguishing NASH from NAFL, showed moderate diagnostic accuracy. miRNA-122 was upregulated in every scenario of NAFL, NASH and fibrosis. Non-alcoholic fatty liver disease (NAFLD) is characterised by excessive fat accumulation without a history of excessive alcohol intake, and the absence of other known liver diseases, such as hepatitis B and hepatitis C virus infection.1–3 NAFLD has been described as the hepatic manifestation of metabolic syndrome, associated with insulin resistance and genetic susceptibility. NAFLD affects 30% to 40% of the United States population,4 2% to 44% of the European population,5 and 15% to 45% of the Asian population,6 while the Hispanic population is the most susceptible, as up to 45% of this population suffers from NAFLD.7 Sedentary behaviour, low physical activity and poor diet have been defined as the “triple-hit behavioural phenotype”, which is associated with cardio-metabolic health, NAFLD and overall mortality.8 Moreover, the prevalence of NAFLD in children is increasing and was estimated to be approximately 10%.9 NAFLD encompasses a wide spectrum of liver damage, ranging from non-alcoholic fatty liver (NAFL) to non-alcoholic steatohepatitis (NASH). NAFL is defined as the presence of hepatocyte steatosis without evidence of inflammation. It is usually non-progressive, while NASH (steatosis with the concomitant presence of inflammation and ballooning) is often progressive, eventually advancing to cirrhosis and hepatocellular carcinoma (HCC).10 Approximately 2–3% of the general population is affected by NASH; this incidence is increased to 20%–30% among obese or diabetic individuals.11,12 NASH is the second most common indication for liver transplantation in the United States13 and is associated with an increased mortality with excess cardiovascular-, liver-, and cancer-related deaths.14 Thus, improved detection of NASH is urgently needed. Moreover, distinguishing both NASH from NAFL and fibrosis from advanced fibrosis (>F3) are important goals.1,15,16 
To evaluate the hypothesis that increasing T cell frequency and activity may provide durable control of hepatitis B virus (HBV), we administered nivolumab, a programmed death receptor 1 (PD-1) inhibitor, with or without GS-4774, an HBV therapeutic vaccine, in virally suppressed patients with HBV e antigen (HBeAg)-negative chronic HBV. In a phase Ib study, patients received either a single dose of nivolumab at 0.1 mg/kg (n = 2) or 0.3 mg/kg (n = 12), or 40 yeast units of GS-4774 at baseline and week 4 and 0.3 mg/kg of nivolumab at week 4 (n = 10). The primary efficacy endpoint was mean change in HBV surface antigen (HBsAg) 12 weeks after nivolumab. Safety and immunologic changes were assessed through week 24. There were no grade 3 or 4 adverse events or serious adverse events. All assessed patients retained T cell PD-1 receptor occupancy 6–12 weeks post-infusion, with a mean total across 0.1 and 0.3 mg/kg cohorts of 76% (95% CI 75–77), and no significant differences were observed between cohorts (p = 0.839). Patients receiving 0.3 mg/kg nivolumab without and with GS-4774 had mean declines of −0.30 (95% CI −0.46 to −0.14) and −0.16 (95% CI −0.33 to 0.01) log10 IU/ml, respectively. Patients showed significant HBsAg declines from baseline (p = 0.035) with 3 patients experiencing declines of >0.5 log10 by the end of study. One patient, whose HBsAg went from baseline 1,173 IU/ml to undetectable at week 20, experienced an alanine aminotransferase flare (grade 3) at week 4 that resolved by week 8 and was accompanied by a significant increase in peripheral HBsAg-specific T cells at week 24. In virally suppressed HBeAg-negative patients, checkpoint blockade was well-tolerated and led to HBsAg decline in most patients and sustained HBsAg loss in 1 patient. More than 240 million people worldwide are chronically infected with the hepatitis B virus (HBV)1,2. In the Asia-Pacific region, where it is most prevalent, chronic HBV infection is the leading cause of cirrhosis, liver failure, and hepatocellular carcinoma.2–6 While currently approved oral nucleos(t)ide analogs effectively suppress viral replication, providing important clinical benefits, there are several disadvantages of this therapeutic approach. First, since antiviral therapy is rarely curative, most patients must receive life-long treatment with the attendant cost, cumulative toxicity, and risk of breakthrough through either non-adherence or the emergence of antiviral resistance. Moreover, viral suppression with nucleos(t)ide analogs does not eliminate the risk of hepatocellular carcinoma.7 A finite course of treatment that can provide sustained off-treatment HBV suppression and clinical response is a clear unmet medical need. Current guidelines recognize clearance of the hepatitis B surface antigen (HBsAg) from the patient’s serum as a so-called “functional cure” and can be distinguished from a “sterilizing cure” which requires the elimination of the covalently closed circular DNA (cccDNA) nuclear reservoir of the virus.5,8,9 Functional cure has been associated with improved outcomes, however, and is a current goal of curative approaches for HBV.10 
Prolactin (PRL) is a multifunctional polypeptide with effects on metabolism, however, little is known about its effect on hepatic steatosis and lipid metabolism. Herein, we aimed to assess the role of PRL in the development of non-alcoholic fatty liver disease (NAFLD). The serum PRL levels of 456 patients with NAFLD, 403 controls without NAFLD diagnosed by ultrasound, and 85 individuals with liver histology obtained during metabolic surgery (44 female and 30 male patients with NAFLD and 11 age-matched non-NAFLD female individuals) were evaluated. The expression of the gene encoding the prolactin receptor (PRLR) and signalling molecules involved in hepatic lipid metabolism were evaluated in human liver and HepG2 cells. The effects of overexpression of PRLR or fatty acid translocase (FAT)/CD36 or knockdown of PRLR on hepatic lipid metabolism were tested in free fatty acid (FFA)-treated HepG2 cells. Circulating PRL levels were lower in individuals with ultrasound-diagnosed NAFLD (men: 7.9 [range, 5.9–10.3] µg/L; women: 8.7 [range, 6.1–12.4] µg/L) than those with non-NAFLD (men: 9.1 [range, 6.8–13.0] µg/L, p = 0.002; women: 11.6 [range, 8.2–16.1] µg/L, p <0.001). PRL levels in patients with biopsy-proven severe hepatic steatosis were lower compared with those with mild-to-moderate hepatic steatosis in both men (8.3 [range, 5.4–9.5] µg/L vs. 9.7 [range, 7.1–12.3] µg/L, p = 0.031) and women (8.5 [range, 4.2–10.6] µg/L vs. 9.8 [range, 8.2–15.7] µg/L, p = 0.027). Furthermore, hepatic PRLR gene expression was significantly reduced in patients with NAFLD and negatively correlated with CD36 gene expression. In FFA-induced HepG2 cells, PRL treatment or PRLR overexpression significantly reduced the expression of CD36 and lipid content, effects that were abrogated after silencing of PRLR. Furthermore, overexpression of CD36 significantly reduced the PRL-mediated improvement in lipid content. Our results reveal a novel association between the central nervous system and the liver, whereby PRL/PRLR improved hepatic lipid accumulation via the CD36 pathway. . Non-alcoholic fatty liver disease (NAFLD) is a common public health problem affecting up to 25% of adults around the world1 and is associated with a series of metabolic co-morbidities. NAFLD is initially caused by an imbalance between lipid demand and supply; however, the pathogenesis of the disease also involves crosstalk between liver and extrahepatic organs, including adipose tissue and the central nervous system (CNS).2 By sensing and integrating peripheral signals (such as leptin), hypothalamic arcuate nucleus (ARC) neurons in the CNS can induce hepatic fatty acid oxidation.3 By contrast, pituitary-released hormones, such as growth hormone, have been shown to suppress lipid uptake in liver.4,5 
Glycogen storage disease type Ia (GSDIa) is a rare genetic disease associated with glycogen accumulation in hepatocytes and steatosis. With age, most adult patients with GSDIa develop hepatocellular adenomas (HCA), which can progress to hepatocellular carcinomas (HCC). In this study, we characterized metabolic reprogramming and cellular defense alterations during tumorigenesis in the liver of hepatocyte-specific G6pc deficient (L.G6pc−/−) mice, which develop all the hepatic hallmarks of GSDIa. Liver metabolism and cellular defenses were assessed at pretumoral (four months) and tumoral (nine months) stages in L.G6pc−/− mice fed a high fat/high sucrose (HF/HS) diet. In response to HF/HS diet, hepatocarcinogenesis was highly accelerated since 85% of L.G6pc−/− mice developed multiple hepatic tumors after nine months, with 70% classified as HCA and 30% as HCC. Tumor development was associated with high expression of malignancy markers of HCC, i.e. alpha-fetoprotein, glypican 3 and β-catenin. In addition, L.G6pc−/− livers exhibited loss of tumor suppressors. Interestingly, L.G6pc−/− steatosis exhibited a low-inflammatory state and was less pronounced than in wild-type livers. This was associated with an absence of epithelial-mesenchymal transition and fibrosis, while HCA/HCC showed a partial epithelial-mesenchymal transition in the absence of TGF-β1 increase. In HCA/HCC, glycolysis was characterized by a marked expression of PK-M2, decreased mitochondrial OXPHOS and a decrease of pyruvate entry in the mitochondria, confirming a “Warburg-like” phenotype. These metabolic alterations led to a decrease in antioxidant defenses and autophagy and chronic endoplasmic reticulum stress in L.G6pc−/− livers and tumors. Interestingly, autophagy was reactivated in HCA/HCC. The metabolic remodeling in L.G6pc −/− liver generates a preneoplastic status and leads to a loss of cellular defenses and tumor suppressors that facilitates tumor development in GSDI. Hepatocellular carcinoma (HCC) is one of the most common primary malignancies of the liver.1 Many studies have demonstrated a strong link between hepatic tumorigenesis and non-alcoholic fatty liver diseases (NAFLDs), including obesity, diabetes and other genetic metabolic liver diseases, such as glycogen storage disease type I (GSDI).2–4 NAFLD can go from simple steatosis characterized by hepatic fat accumulation, to the more aggressive form called non-alcoholic steatohepatitis (NASH), associated with inflammation and fibrosis. While it is considered that NASH/NAFLD can lead to liver cirrhosis, predisposing the liver to HCC, studies have reported that HCC can also develop in the absence of cirrhosis. Moreover, hepatocellular adenomas (HCA), which are rare benign tumors, can later transform into HCC in the absence of cirrhosis.5,6 
Most hepatitis C virus (HCV)-infected patients failing NS5A inhibitors develop resistance-associated substitutions (RASs). Here we report the use of resistance-guided retreatment of patients who failed prior NS5A inhibitor-containing regimens in the GEHEP-004 cohort. This is the largest direct-acting antiviral (DAA)-resistance cohort study conducted in Spain. We aim to provide indications on how to use resistance information in settings where sofosbuvir/velpatasvir/voxilaprevir may not be available. GEHEP-004 is a prospective multicenter cohort enrolling HCV-infected patients treated with interferon (IFN)-free DAA regimens. Prior to retreatment, population-based sequencing of HCV NS3, NS5A and NS5B genes was performed. After receiving a comprehensive resistance interpretation report, the retreatment regimen was chosen and the sustained virological response (SVR) at 12 weeks after treatment completion (SVR12) was recorded. A total of 342 patients experiencing virological failure after treatment with sofosbuvir/ledipasvir±ribavirin (54%), sofosbuvir/daclatasvir±ribavirin (23%), or paritaprevir-ritonavir/ombitasvir±dasabuvir±ribavirin (20%) were studied. After a resistance report, 186 patients were retreated. An SVR12 was achieved for 88.1% of the patients who failed after sofosbuvir/ledipasvir±ribavirin, 83.3% of the patients who failed after sofosbuvir/daclatasvir±ribavirin, 93.7% of the patients who failed after paritaprevir-ritonavir+ombitasvir±dasabuvir±ribavirin. In our study, we show how resistance-guided retreatment in conjunction with an interpreted report allows patients to achieve SVR rates close to 90%. We hypothesize that SVR rates may even be improved if resistance data are discussed between experienced virologists and treating clinicians. We believe that our data may be relevant for countries where the access to new DAA combination regimens is limited. According to the World Health Organization, there are approximately 71 million people infected with hepatitis C virus (HCV) worldwide and 1.75 million people are diagnosed each year.1 In the absence of antiviral treatment, HCV leads to cirrhosis, hepatocellular carcinoma, liver failure and death.2 Treatment with direct-acting antivirals (DAA) is highly efficacious and it has limited side effects.3 Current DAA combinations that are recommended as first-line treatment of HCV-infected patients by the AASLD-IDSA4 and EASL guidelines5 enable patients to achieve sustained virological response (SVR) rates >90% for all HCV genotypes. 
In a variety of animal models, omega-3 polyunsaturated fatty acids (Ω3-FAs) conferred strong protective effects, alleviating hepatic ischemia/reperfusion injury and steatosis, as well as enhancing regeneration after major tissue loss. Given these benefits along with its safety profile, we hypothesized that perioperative administration of Ω3-FAs in patients undergoing liver surgery may ameliorate the postoperative course. The aim of this study was to investigate the perioperative use of Ω3-FAs to reduce postoperative complications after liver surgery. Between July 2013 and July 2018, we carried out a multicentric, double-blind, randomized, placebo-controlled trial designed to test whether 2 single intravenous infusions of Omegaven® (Ω3-FAs) vs. placebo may decrease morbidity. The primary endpoints were postoperative complications by severity (Clavien-Dindo classification) integrated within the comprehensive complication index (CCI). A total of 261 patients (132 in the Omegaven and 129 in the placebo groups) from 3 centers were included in the trial. Most cases (87%, n = 227) underwent open liver surgery and 56% (n = 105) were major resections (≥3 segments). In an intention-to-treat analysis including the dropout cases, the mortality rate was 4% and 2% in the Omegaven and placebo groups (odds ratio 0.40; 95% CI 0.04–2.51; p = 0.447), respectively. Any complications and major complications (Clavien-Dindo ≥ 3b) occurred in 46% vs. 43% (p = 0.709) and 12% vs. 10% (p = 0.69) in the Omegaven and placebo groups, respectively. The mean CCI was 17 (±23) vs.14 (±20) (p = 0.417). An analysis excluding the dropouts provided similar results. The routine perioperative use of 2 single doses of intravenous Ω3-FAs (100 ml Omegaven) cannot be recommended in patients undergoing liver surgery (Grade A recommendation). Omega-3 polyunsaturated fatty acids (Ω3-FAs) are essential nutritional components, whose presence in the human body exclusively depends on its exogenous supply. In the late 1970s, Bang and Dyerberg observed an extraordinary low incidence of coronary heart disease among Greenland inuits, which was found to be related to a high intake of Ω3-FAs abundant in marine fish; the main food source of this population.1 The liver was subsequently found to be a central metabolic target of Ω3-FAs with growing evidence of their beneficial effects on fatty liver and metabolic syndrome.2–4 Ω3-FAs were found not only to lower intrahepatic lipid contents,5 but also to mitigate inflammatory changes in the liver.6,7 Liver surgery in patients with fatty liver leads to an amplified ischemia-reperfusion injury (I/R) and impaired regeneration (LR).8 A variety of animal models disclosed significant liver protection in hepatic surgery (reduction of I/R and improved LR) following treatment with Ω3-FAs.9–11 Of note, these effects were not only observed in steatotic, but also in lean livers.11 For example, in a mouse model, the intravenous (i.v.) administration of a single dose of Ω3-FAs (Omegaven®) prior to liver ischemia dramatically mitigated the production of reactive oxygen species (ROS) upon reperfusion.12 This beneficial effect is mediated via the GPR120 receptor located on hepatic Kupffer cells.12 
Porphyrias result from anomalies of heme biosynthetic enzymes and can lead to cirrhosis and hepatocellular cancer. In mice, these diseases can be modeled by administration of a diet containing 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC), which causes accumulation of porphyrin intermediates, resulting in hepatobiliary injury. Wnt/β-catenin signaling has been shown to be a modulatable target in models of biliary injury; thus, we investigated its role in DDC-driven injury. β-Catenin (Ctnnb1) knockout (KO) mice, Wnt co-receptor KO mice, and littermate controls were fed a DDC diet for 2 weeks. β-Catenin was exogenously inhibited in hepatocytes by administering β-catenin dicer-substrate RNA (DsiRNA), conjugated to a lipid nanoparticle, to mice after DDC diet and then weekly for 4 weeks. In all experiments, serum and livers were collected; livers were analyzed by histology, western blotting, and real-time PCR. Porphyrin was measured by fluorescence, quantification of polarized light images, and liquid chromatography-mass spectrometry. DDC-fed mice lacking β-catenin or Wnt signaling had decreased liver injury compared to controls. Exogenous mice that underwent β-catenin suppression by DsiRNA during DDC feeding also showed less injury compared to control mice receiving lipid nanoparticles. Control livers contained extensive porphyrin deposits which were largely absent in mice lacking β-catenin signaling. Notably, we identified a network of key heme biosynthesis enzymes that are suppressed in the absence of β-catenin, preventing accumulation of toxic protoporphyrins. Additionally, mice lacking β-catenin exhibited fewer protein aggregates, improved proteasomal activity, and reduced induction of autophagy, all contributing to protection from injury. β-Catenin inhibition, through its pleiotropic effects on metabolism, cell stress, and autophagy, represents a novel therapeutic approach for patients with porphyria. Porphyrins are precursors of heme, an essential co-factor for hemoproteins like cytochrome-P450, hemoglobin, and peroxidases. Heme biosynthesis starts in mitochondria, where ALA-synthase (ALA-S) catalyzes the combination of glycine and succinyl Co-A to form δ-aminolevulinic acid (ALA). ALA leaves the mitochondria, and is sequentially converted in the cytosol to porphobilinogen, hydroxymethylbilane, uroporphyrin and then to coproporphyin, which re-enters mitochondria. In the penultimate step, protoporphyrin-IX (PP-IX) is generated, which is metallated by ferrochelatase (FC) to form the iron containing heme.1,2 
Chronic alcohol (EtOH) consumption is a leading risk factor for development of hepatocellular carcinoma (HCC), which is associated with marked increase of hepatic expression of pro-inflammatory IL-17A and its receptor IL-17RA. Genetic deletion and pharmacological blocking was used to characterize the role of IL-17A/IL-17RA signaling in the pathogenesis of HCC. We demonstrate that global deletion of IL-17RA gene suppressed HCC in alcohol-fed DEN-challenged IL-17RA-/- and Mup-uPA/IL-17RA-/- mice compared to wild type mice. When the cell-specific role of IL-17RA signaling was examined, development of HCC was decreased in both alcohol-fed IL-17RAΔMΦ and IL-17RAΔHep mice devoid of IL-17RA in myeloid cells and hepatocytes, but not in IL-17RAΔHSCs mice (deficient of IL-17RA in hepatic stellate cells (HSCs)). Deletion of IL-17RA in myeloid cells ameliorated tumorigenesis via suppression of pro-tumorigenic/inflammatory and pro-fibrogenic responses in alcohol-fed IL-17RAΔMΦ mice. Remarkably, despite a normal inflammatory response, alcohol-fed IL-17RAΔHep mice developed the fewest tumors (compared to IL-17RAΔMΦ mice), with reduced steatosis and fibrosis. Steatotic IL-17RA-deficient hepatocytes downregulated expression of Cxcl1 and other chemokines, exhibited a striking defect in TNF-TNFR1-dependent Caspase-2-SREBP-1/2-DHCR7-mediated cholesterol synthesis, and upregulated production of anti-oxidant Vitamin D3. Pharmacological blocking of IL-17A/Th-17 cells using anti-IL-12/IL-23 Ab suppressed progression of HCC (by 70%) in alcohol-fed mice, indicating that targeting IL-17 signaling might provide novel strategies for treatment of alcohol-induced HCC. Overall, IL-17A is as a tumor promoting cytokine, which critically regulates alcohol-induced hepatic steatosis, inflammation, fibrosis, and HCC. Hepatocellular carcinoma (HCC) develops in response to chronic injury, such as HBV/HCV infections, and metabolic diseases including non-alcoholic steatohepatitis (NASH), and alcoholic liver disease (ALD). With the introduction of new therapies, the incidence of HBV/HCV-related HCC has declined1, while NASH- and ALD-induced HCC are rapidly rising, both of which typically progress from hepatic steatosis to steatohepatitis, fibrosis, cirrhosis, and cancer2. ALD remains a major risk factor of hepatic cirrhosis and aggressive HCC2. Consistent with the global epidemic of NASH, alcohol-induced HCC often occurs in patients with BMI > 253. Chronic alcohol consumption is believed to increase development of HCC in patients with NASH4, therefore, we used an experimental model of ALD and Western Diet which reflects the typical American patient with ALD. 
GS-9620, an oral agonist of toll-like receptor 7, is in clinical development for the treatment of chronic hepatitis B (CHB). GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the chimpanzee and woodchuck models of CHB. Herein, we investigated the immunomodulatory mechanisms underlying these antiviral effects. Archived liver biopsies and paired peripheral blood mononuclear cell samples from a previous chimpanzee study were analyzed by RNA sequencing, quantitative reverse transcription PCR, immunohistochemistry (IHC) and in situ hybridization (ISH). GS-9620 treatment of CHB chimpanzees induced an intrahepatic transcriptional profile significantly enriched with genes associated with hepatitis B virus (HBV) clearance in acutely infected chimpanzees. Type I and II interferon, CD8+ T cell and B cell transcriptional signatures were associated with treatment response, together with evidence of hepatocyte death and liver regeneration. IHC and ISH confirmed an increase in intrahepatic CD8+ T cell and B cell numbers during treatment, and revealed that GS-9620 transiently induced aggregates predominantly comprised of CD8+ T cells and B cells in portal regions. There were no follicular dendritic cells or IgG-positive cells in these lymphoid aggregates and very few CD11b+ myeloid cells. There was no change in intrahepatic natural killer cell number during GS-9620 treatment. The antiviral response to GS-9620 treatment in CHB chimpanzees was associated with an intrahepatic interferon response and formation of lymphoid aggregates in the liver. Our data indicate these intrahepatic structures are not fully differentiated follicles containing germinal center reactions. However, the temporal correlation between development of these T and B cell aggregates and the antiviral response to treatment suggests they play a role in promoting an effective immune response against HBV. Approximately 240 million individuals live with chronic hepatitis B (CHB), and over half a million people are estimated to die each year because of CHB-associated liver diseases such as cirrhosis and hepatocellular carcinoma. Various nucleos(t)ide inhibitors and interferon-α (IFN-α) are currently licensed for the treatment of CHB, but while these therapies reduce viremia and improve long-term outcome, they rarely lead to functional cure. The rarity of complete treatment response, together with the difficulty in obtaining liver biopsy specimens from patients with CHB, has hindered characterization of the immune determinants of viral control. Accordingly, animal models have been extensively used to identify potential mechanisms of hepatitis B virus (HBV) clearance. Most notably, characterization of acute hepadnavirus infection in woodchucks and chimpanzees, together with studies in transgenic and hydrodynamic mouse models have identified important determinants of HBV clearance.1–4 However, these studies were performed in models of self-limiting natural infection and/or in models that lack viral spread and the authentic HBV genome, and therefore it remains to be determined whether the identified mechanisms can be therapeutically induced to achieve functional cure of CHB in patients. Fortunately, the development of novel therapies provides new possibilities for studying the immune determinants of viral clearance during chronic infection. 
The Karnofsky performance status (KPS) has been used for almost 70 years for clinical assessment of patients. Our objective was to determine whether KPS is an independent predictor of post-liver transplant (LT) survival after adjusting for known confounders. Adult patients listed with the United Network for Organ Sharing (UNOS) from 2006 to 2016 were grouped into low (10–40%, n = 15,103), intermediate (50–70%, n = 22,183) and high (80–100%, n = 13,131) KPS groups based on KPS scores at the time of LT, after excluding those on ventilators or life support. We determined the trends in KPS before and after LT, and survival probabilities based on KPS. There was a decline in KPS scores between listing and LT and there was significant improvement after LT. The graft and patient survival differences were significantly lower (p <0.0001) in those with low KPS. After adjusting for other confounders, the hazard ratios for graft failure were 1.17 (1.12–1.22, p <0.01) for the intermediate and 1.38 (1.31–1.46, p <0.01) for the low group. Similarly, hazard ratios for patient failure were 1.18 (1.13–1.24, p <0.01) for the intermediate and 1.43 (1.35–1.52, p <0.01) for the low group. Other independent negative predictors for graft and patient survival were older age, Black ethnicity, presence of hepatic encephalopathy and donor risk index. Those who did not show significant improvements in post-LT KPS scores had poorer outcomes in all three KPS groups, but it was most obvious in the low KPS group with one-year patient survival of 33%. The KPS, before and after LT, is an independent predictor of graft and patient survival after adjusting for other important predictors of survival. The Karnofsky performance status (KPS) has been used for almost 70 years in clinical practice as a subjective ‘eyeball’ assessment of the overall performance status of patients. The KPS scores, administered by the provider or support staff, assign scores to patients on a scale of 0–100%, in increments of 10, where 100% is normal activity and 0% is dead.1 It is widely used in general oncology practice as a prognostic predictor and also for the selection of patients in clinical trials.4–10 The inter-observer reliability, validity and reproducibility of KPS scores in multiple clinical settings have shown to be excellent.2,10,11 Recently, the KPS was shown to be a useful tool for predicting survival in patients admitted to hospitals with complications of cirrhosis and was shown to be a predictor of transplant waitlist mortality.12,13 
Although off-label use of sofosbuvir-containing regimens occurs regularly in patients with hepatitis C virus (HCV) infection undergoing dialysis for severe renal impairment or end-stage renal disease (ESRD), these regimens are not licensed for this indication, and there is an absence of dosing recommendations in this population. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir in patients with HCV infection with ESRD undergoing dialysis. In this phase II, single-arm study, 59 patients with genotype 1–6 HCV infection with ESRD undergoing hemodialysis or peritoneal dialysis received open-label sofosbuvir/velpatasvir (400 mg/100 mg) once daily for 12 weeks. Patients were HCV treatment naive or treatment experienced without cirrhosis or with compensated cirrhosis. Patients previously treated with any HCV NS5A inhibitor were not eligible. The primary efficacy endpoint was the proportion of patients achieving sustained virologic response (SVR) 12 weeks after discontinuation of treatment (SVR12). The primary safety endpoint was the proportion of patients who discontinued study drug due to adverse events. Overall, 56 of 59 patients achieved SVR12 (95%; 95% CI 86–99%). Of the 3 patients who did not achieve SVR12, 2 patients had virologic relapse determined at post-treatment Week 4 (including 1 who prematurely discontinued study treatment), and 1 patient died from suicide after achieving SVR through post-treatment Week 4. The most common adverse events were headache (17%), fatigue (14%), nausea (14%), and vomiting (14%). Serious adverse events were reported for 11 patients (19%), and all were deemed to be unrelated to sofosbuvir/velpatasvir. Treatment with sofosbuvir/velpatasvir for 12 weeks was safe and effective in patients with ESRD undergoing dialysis. Sofosbuvir/velpatasvir is a combination direct-acting antiviral that is approved for treatment of patients with hepatitis C virus (HCV) infection. Despite the lack of dosing recommendations, sofosbuvir-containing regimens (including sofosbuvir/velpatasvir) are frequently used for HCV-infected patients undergoing dialysis. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir for 12 weeks in patients with HCV infection who were undergoing dialysis. Treatment with sofosbuvir/velpatasvir was safe and well tolerated, resulting in a cure rate of 95% in patients with HCV infection and end-stage renal disease. Hepatitis C virus (HCV) infection is a global health challenge with an estimated 71 million individuals infected worldwide.1 The disease burden of HCV infection is due to progression of chronic liver disease, which can lead to cirrhosis, liver failure, hepatocellular carcinoma, and death. Chronic HCV infection is also independently associated with the development of renal impairment referred to as chronic kidney disease (CKD) and has been shown to be more prevalent in patients with renal disease.2,3 Chronic HCV infection has a significant negative impact on morbidity and mortality in patients undergoing dialysis.4 HCV-infected patients with CKD have an accelerated rate of loss of kidney function, risk of progression to end-stage renal disease (ESRD), and increased risk of all-cause mortality when undergoing dialysis.2,5–7 
It remains controversial whether direct-acting antivirals (DAAs) accelerate the recurrence of hepatitis C-related hepatocellular carcinoma (HCC) after curative therapy. This study aimed to evaluate HCC recurrence after DAA treatment of chronic hepatitis C. We enrolled patients with a history of successful radiofrequency ablation treatment for hepatitis C-related HCC who received antiviral therapy with DAAs (DAA group: 147 patients) or with interferon (IFN)-based therapy (IFN group: 156 patients). We assessed HCC recurrence rates from the initiation of antiviral therapy using the Kaplan-Meier method and evaluated risk factors for HCC recurrence by multivariate Cox proportional hazard regression analysis. The recurrence pattern was categorized as follows: intrahepatic recurrence with a single tumor <2 cm (stage 0), a single tumor or up to 3 tumors ≤3 cm (stage A), multinodular (stage B), and extrahepatic metastasis or macrovascular invasion (stage C). The recurrence rates at 1 and 2 years were 39% and 61% in the IFN group and 39% and 60% in the DAA group, respectively (p = 0.43). Multivariate analysis identified higher lens culinaris agglutinin-reactive fraction of alpha-fetoprotein level, a history of multiple HCC treatments, and a shorter interval between HCC treatment and initiation of antiviral therapy as independent risk factors for HCC recurrence. HCC recurrence in stage 0, A, B, and C was found in 56 (41%), 60 (44%), 19 (14%), and 1 (0.7%) patients in the IFN group and 35 (44%), 32 (40%), 11 (14%), and 2 (2.5%) patients in the DAA group, respectively (p = 0.70). HCC recurrence rates and patterns after initiation of antiviral therapy did not differ between patients who received IFN-based therapy and DAA therapy. Hepatocellular carcinoma (HCC) is one of the most common cancers worldwide and a leading cause of cancer-related death.1 HCC usually develops in patients with chronic liver diseases, often related to hepatitis C virus (HCV) infection.2,3 Although interferon (IFN)-based therapy after HCC treatment reportedly reduces the risk of HCC recurrence,4–6 few patients were eligible for IFN therapy after HCC treatment because of aging or advanced liver fibrosis, and its antiviral effect was inversely associated with the rates of adverse effects.7 Currently, with the development of direct-acting antivirals (DAAs), patients with a history of HCC can achieve a high sustained virologic response (SVR) rate with favorable tolerability.8,9 
Tertiary lymphoid structures (TLSs) provide a local and critical microenvironment for generating anti-tumor cellular and humoral immune responses. TLSs are associated with improved clinical outcomes in most solid tumors investigated to date. However, their role in hepatocellular carcinoma (HCC) is debated, as they have recently been shown to promote the growth of malignant hepatocyte progenitors in the non-tumoral liver. We aimed to determine, by pathological review, the prognostic significance of both intra-tumoral and non-tumoral TLSs in a series of 273 patients with HCC treated by surgical resection in Henri Mondor University Hospital. Findings were further validated by gene expression profiling using a public data set (LCI cohort). TLSs were identified in 47% of the tumors, by pathological review, with lymphoid aggregates, primary and secondary follicles in 26%, 16% and 5% of the cases, respectively. Univariate and multivariate analyses showed that intra-tumoral TLSs significantly correlated with a lower risk of early relapse (<2 years after surgery, hazard ratio 0.46, p = 0.005). Interestingly, the risk of recurrence was also related to the degree of TLS maturation (primary or secondary follicles vs. lymphoid aggregates, p = 0.01). A gene expression signature associated with the presence of intra-tumoral TLS was also independently associated with a lower risk of early relapse in the LCI cohort. No association between the density of TLSs located in the adjacent non-tumoral liver and early or late recurrence was observed. We have shown that intra-tumoral TLSs are associated with a lower risk of early relapse in 2 independent cohorts of patients with HCC treated by surgical resection. Thus, intra-tumoral TLSs may reflect the existence of ongoing, effective anti-tumor immunity. Hepatocellular carcinoma (HCC) is the fifth most frequent cancer worldwide and the second leading cause of cancer-related deaths.1 The vast majority of cases develop in patients with chronic liver diseases, the main risk factors being hepatitis B virus (HBV) and hepatitis C virus (HCV) infection, alcohol intake and metabolic syndrome.2,3 Clinical outcomes remain poor, with only around one-third of patients eligible for potentially curative treatments such as surgical resection, radiofrequency ablation or liver transplantation.3 The standard of care for advanced cases is the multikinase inhibitor sorafenib, which unfortunately has limited survival benefit.4 
Although patients with cryptogenic cirrhosis have historically been considered as having “burnt-out” non-alcoholic steatohepatitis (NASH), some controversy remains. The aim of this study was to compare outcomes of patients with cryptogenic cirrhosis and NASH-related cirrhosis from a cohort with longitudinal follow-up data. Patients with cryptogenic cirrhosis or NASH cirrhosis were screened for a clinical trial. Patients with <5% hepatic steatosis regardless of other histologic features were considered to have cryptogenic cirrhosis. Clinico-laboratory data and adjudicated liver-related events (e.g. decompensation, qualification for transplantation, death) were available. A total of 247 patients with cirrhosis (55.3 ± 7.4 years, 37% male) were included; 144 had NASH cirrhosis and 103 had cryptogenic cirrhosis. During a median follow-up of 29 (IQR 21–33) months (max 45 months), 20.6% of patients had liver-related clinical events. Patients with NASH cirrhosis and cryptogenic cirrhosis were of a similar age and gender, as well as having a similar body mass index, PNPLA3 rs738409 genotype, and prevalence of diabetes (p >0.05). However, patients with cryptogenic cirrhosis had higher serum fibrosis markers and greater collagen content and α-smooth muscle actin expression on liver biopsy. Compared to cirrhotic patients with NASH, patients with cryptogenic cirrhosis experienced significantly shorter mean time to liver-related clinical events (12.0 vs. 19.4 months; p = 0.001) with a hazard ratio of 1.76 (95% CI 1.02–3.06). Populations with NASH and cryptogenic cirrhosis have similar demographics, but patients with cryptogenic cirrhosis have evidence of more active fibrosis and a higher risk of liver-related clinical events. Thus, we believe these patients belong to the same spectrum of disease, with cryptogenic cirrhosis representing a more advanced stage of fibrosis. Non-alcoholic steatohepatitis (NASH) is not only a very common cause of chronic liver disease worldwide, but also is a subtype of non-alcoholic fatty liver disease (NAFLD) that can potentially progress to cirrhosis and its complications.1 A number of studies have shown that approximately one-third of patients with NAFLD can develop progressive liver disease, while 20% develop cirrhosis and are at increased risk of mortality.1,2 
Knowledge about the regulation of anti-HBV humoral immunity during natural HBV infection is limited. We recently utilized dual fluorochrome-conjugated HBsAg to demonstrate, in patients with chronic HBV (CHB) infection, the functional impairment of their HBsAg-specific B cells. However, the features of their HBcAg-specific B cells are unknown. Here we developed a method to directly visualize, select and characterize HBcAg-specific B cells in parallel with HBsAg-specific B cells. Fluorochrome-conjugated HBcAg reagents were synthesized and utilized to directly detect ex vivo HBcAg-specific B cells in 36 patients with CHB. The frequency, phenotype, functional maturation and transcriptomic profile of HBcAg-specific B cells was studied by flow cytometry, in vitro maturation assays and NanoString-based detection of expression of immune genes, which we compared with HBsAg-specific B cells and total B cells. HBcAg-specific B cells are present at a higher frequency than HBsAg-specific B cells in patients with CHB and, unlike HBsAg-specific B cells, they mature efficiently into antibody-secreting cells in vitro. Their phenotypic and transcriptomic profiles show that HBcAg-specific B cells are preferentially IgG+ memory B cells. However, despite their phenotypic and functional differences, HBcAg- and HBsAg-specific B cells from patients with CHB share an mRNA expression pattern that differs from global memory B cells and is characterized by high expression of genes indicative of cross-presentation and innate immune activity. During chronic HBV infection, a direct relation exists between serological detection of anti-HBs and anti-HBc antibodies, and the quantity and function of their respective specific B cells. However, the transcriptomic analysis performed in HBsAg- and HBcAg-specific B cells suggests additional roles of HBV-specific B cells beyond the production of antibodies. Antibodies and T cells cooperate to control pathogens and present functional alterations during persistent infections. This scenario is also present in the infection with HBV, a hepatotropic DNA virus that establishes chronic infection in the human liver and can cause pathologies (liver cirrhosis and cancer) that have increased decisively in the last decades.1 
The impact of hepatitis B virus (HBV) infection on outcomes after resection of intrahepatic cholangiocarcinoma (ICC) has not been reported. The aim of this study was to examine the impact of antiviral therapy on survival outcomes after liver resection for patients with ICC and underlying HBV infection. Data on 928 patients with ICC and HBV infection who underwent liver resection at two medical centers between 2006 and 2011 were analyzed. Data on viral reactivation, tumor recurrence, cancer-specific survival (CSS) and overall survival (OS) were obtained. Survival rates were analyzed using the time-dependent Cox regression model adjusted for potential covariates. Postoperative viral reactivation occurred in 3.3%, 8.3% and 15.7% of patients who received preoperative antiviral therapy, who did not receive preoperative antiviral therapy with a low, or a high HBV-DNA level (< or ≥2,000 IU/ml), respectively (p <0.001). A high viral level and viral reactivation were independent risk factors of recurrence (hazard ratio [HR] 1.22 and 1.34), CSS (HR 1.36 and 1.46) and OS (HR 1.23 and 1.36). Five-year recurrence, CSS and OS were better in patients who received antiviral therapy (70.5%, 46.9% and 43.0%) compared with patients who did not receive antiviral therapy and had a high viral level (86.5%, 20.9% and 20.5%, all p <0.001), respectively. The differences in recurrence, CSS and OS were minimal compared with no-antiviral therapy patients with a low viral level (71.7%, 35.5% and 33.5%, p = 0.057, 0.051 and 0.060, respectively). Compared to patients with a high viral level who received no antiviral therapy, patients who initiated antiviral therapy either before or after surgery had better long-term outcomes (HR 0.44 and 0.54 for recurrence; 0.38 and 0.57 for CSS; 0.46 and 0.54 for OS, respectively). Viral reactivation was associated with worse prognoses after liver resection for HBV-infected patients with ICC. Antiviral therapy decreased viral reactivation and prolonged long-term survival for patients with ICC and a high viral level. Intrahepatic cholangiocarcinoma (ICC) accounts for 10%–20% of primary liver cancer, with an incidence second only to hepatocellular carcinoma (HCC).1 In addition, the incidence and related mortality of ICC are dramatically increasing worldwide.1,2 Currently, liver resection (LR) is the only established treatment to achieve possible long-term survival for patients with ICC,1,3 with an associated five-year survival ranging from 26.4% to 35.2%.4,5 The poor long-term outcomes following resection are related to the aggressive and invasive nature of this malignancy.6,7 
Multiple organ failures (OFs) are common in patients with cirrhosis, but the independent effect of the number or type of OFs on liver transplantation (LT) outcomes is not well defined. United Network for Organ Sharing data were analyzed from 2002 to 2016 for all adults listed for LT who received an LT within 30 days after listing. We estimated post-LT survival stratified by number and type of pre-transplant OFs before and after adjusting for confounding variables. During the study period, 4,714 (4.1%) patients died and 19,375 (16.6%) patients were transplanted within 30 days of listing. One or more OF were more common in those who were transplanted (57.4%) compared to those without LT (9.5%). The probability of staying alive more than 30 days on the waiting list without LT decreased with increasing number of OFs; while 90% were alive without OF, only 20% were alive with two OFs, and 2–8% with three or more OFs. The interval between listing and transplantation decreased with an increase in OFs, and the median time to transplant after listing was only 4–5 days with three or more OFs. Although the risk of post-LT mortality increased with increasing number of OFs, the 90-day patient survival was 90% and one-year survival was 81% in the presence of 5–6 OFs. The number of OFs was an independent predictor of survival, but the maximum difference in one-year graft or patient survival between those without OF and those with 5–6 OFs was only 9%. Additionally, the type of OF had minimal impact on outcomes. Liver transplantation is feasible with excellent outcomes, even in the presence of five or six OFs. Patients with advanced liver disease may develop one or more organ failures (OFs) when acute decompensation occurs. Acute decompensation is often precipitated by hepatic or extra-hepatic insults such as infections, gastrointestinal bleeding, reactivation of viral hepatitis or alcohol abuse.1–7 Recently there has been a renewed interest in the impact of OFs on short-term mortality of patients with chronic liver diseases.1–4,8–10 It has been suggested that those who develop OF after acute decompensation may have a different pathophysiology and prognosis compared to those who decompensate without the development of OF. The term acute-on-chronic liver failure (ACLF) has been used recently to describe this entity. Although studies from Asia, Europe and North America have used different definitions to describe OF, and hence ACLF, these studies have consistently shown that patients with multiple OF have poor short-term outcomes, irrespective of the precipitating cause of decompensation.1–4,11–13 These studies, based on relatively small sample sizes, also showed that mortality increased with increasing number of OFs, with 30-day mortality reaching well over 80% in those with 3–4 OFs. Patients with cirrhosis who experience an acute decompensating event and develop one or more OFs utilize enormous resources since they are managed in the intensive care units, often requiring dialysis and ventilatory support, while awaiting spontaneous recovery or liver transplantation (LT). There are, however, only limited data on the effectiveness or futility of LT in the setting of patients with multiple OF.11–21 
Although non-alcoholic fatty liver disease (NAFLD), non-alcoholic steatohepatitis (NASH) and NASH with advanced fibrosis are closely associated with type 2 diabetes mellitus (T2DM), their global prevalence rates have not been well described. Our aim was to estimate the prevalence of NAFLD, NASH, and advanced fibrosis among patients with T2DM, by regions of the world. We searched for terms including NAFLD, NASH and T2DM in studies published from January 1989 to September 2018, using PubMed, Ovid MEDLINE®, EMBASE and Web of Science. Strict exclusion criteria were applied. Regional and global mean prevalence weighted by population size in each country were estimated and pooled using random-effects meta-analysis. Potential sources of heterogeneity were investigated using stratified meta-analysis and meta-regression. Among 80 studies from 20 countries that met our inclusion criteria, there were 49,419 individuals with T2DM (mean age 58.5 years, mean body mass index 27.9 kg/m2, and males 52.9%). The global prevalence of NAFLD among patients with T2DM was 55.5% (95% CI 47.3–63.7). Studies from Europe reported the highest prevalence (68.0% [62.1–73.0%]). Among 10 studies that estimated the prevalence of NASH, the global prevalence of NASH among individuals with T2DM was 37.3% (95% CI 24.7–50.0%). Seven studies estimated the prevalence of advanced fibrosis in patients with NAFLD and T2DM to be 17.0% (95% CI 7.2–34.8). Meta-regression models showed that geographic region and mean age (p <0.5) were associated with the prevalence of NAFLD, jointly accounting for 63.9% of the heterogeneity. This study provides the global prevalence rates for NAFLD, NASH, and advanced fibrosis in patients with T2DM. These data can be used to estimate the clinical and economic burden of NASH in patients with T2DM around the world. Non-alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease worldwide, with a global prevalence of 25.2%.1 NAFLD is defined by the presence of hepatic steatosis, detected either by imaging or histology, and a lack of secondary causes of hepatic fat accumulation (i.e. excessive alcohol consumption, steatogenic medication, or monogenic hereditary disorders).2 
Acute-on-chronic liver failure (ACLF) is characterised by the presence of organ failure in patients with decompensated cirrhosis and is associated with high short-term mortality. However, there are limited data on the prevalence and short-term outcomes of ACLF in patients with cirrhosis seen in the US. We aimed to study the prevalence and risk factors associated with the development and short term mortality in a large cohort of patients in the US. Using the US Department of Veterans Affairs (VA) Corporate Data Warehouse, we identified patients with ACLF during hospitalisation for decompensated cirrhosis at any of the 127 VA hospitals between January 1, 2004, and December 31, 2014. We examined the prevalence of ACLF and variables associated with 28- and 90-day mortality in ACLF, and trends in prevalence and survival over time. Of 72,316 patients hospitalised for decompensated cirrhosis, 19,082 (26.4%) patients met the criteria of ACLF on admission. Of these, 12.8% had 1, 10.1% had 2, and 3.5% had 3 or more organ failures. Overall, 25.5% and 40.0% of ACLF patients died within 28 days and 90 days of admission, respectively. Older age, White race, liver cancer, higher model for end-stage liver disease sodium corrected score, and non-liver transplant centre were associated with increased risk of death in ACLF. Over the study period, the prevalence of ACLF decreased, and all grades but ACLF-3 had improvement in survival. In a US cohort of hospitalised patients with decompensated cirrhosis, ACLF was common and associated with high short-term mortality. Over a decade, ACLF prevalence decreased but survival improvement of ACLF-3 was not seen. Early recognition and aggressive management including timely referral to transplant centres may lead to improved outcomes in ACLF. Acute-on-chronic liver failure (ACLF) is a recently recognised condition characterised by multiorgan failure in patients with decompensated cirrhosis and associated with high short-term mortality.1–3 
The sequence of events in hepatic encephalopathy (HE) remains unclear. Using the advantages of in vivo 1H-MRS (9.4T) we aimed to analyse the time-course of disease in an established model of type C HE by analysing the longitudinal changes in a large number of brain metabolites together with biochemical, histological and behavioural assessment. We hypothesized that neurometabolic changes are detectable very early, and that these early changes will offer insight into the primary events underpinning HE. Wistar rats underwent bile-duct ligation (BDL) and were studied before BDL and at post-operative weeks 2, 4, 6 and 8 (n = 26). In vivo short echo-time 1H-MRS (9.4T) of the hippocampus was performed in a longitudinal manner, as were biochemical (plasma), histological and behavioural tests. Plasma ammonium increased early after BDL and remained high during the study. Brain glutamine increased (+47%) as early as 2–4 weeks post-BDL while creatine (−8%) and ascorbate (−12%) decreased. Brain glutamine and ascorbate correlated closely with rising plasma ammonium, while brain creatine correlated with brain glutamine. The increases in brain glutamine and plasma ammonium were correlated, while plasma ammonium correlated negatively with distance moved. Changes in astrocyte morphology were observed at 4 weeks. These early changes were further accentuated at 6–8 weeks post-BDL, concurrently with the known decreases in brain organic osmolytes. Using a multimodal, in vivo and longitudinal approach we have shown that neurometabolic changes are already noticeable 2 weeks after BDL. These early changes are suggestive of osmotic/oxidative stress and are likely the premise of some later changes. Early decreases in cerebral creatine and ascorbate are novel findings offering new avenues to explore neuroprotective strategies for HE treatment. Chronic hepatic encephalopathy (HE) is a complication of chronic liver disease (CLD) and is characterized by cognitive and motor deficits.1–3 While minimal and overt HE may affect 20–70% of patients with CLD, the diagnosis is difficult in its early stages.4,5 Tools enabling the detection of early neurometabolic changes in patients may allow for timely intervention, thereby minimizing the number of overt HE episodes and ensuing complications and costs. Using a multimodal, in vivo and longitudinal approach we have shown that biologically relevant neurometabolic changes in a rat model of chronic HE are already noticeable 2 weeks after BDL, and progress throughout the 8 week course of the disease. The well-described increase in brain Gln in response to plasma NH4+ rise was confirmed, together with a decrease in brain Cr and Asc, both novel findings. Brain Gln and plasma NH4+ were closely correlated, and plasma NH4+ correlated significantly with the decrease in distance moved by the animals. In addition, Gln showed stronger correlations with other metabolites than plasma NH4+, suggesting that some of the changes are directly related to Gln increase rather than indirectly via NH4+ toxicity. A decrease in brain organic osmolytes mirrored the rise in brain Gln concentration. In addition, changes in astrocyte morphology together with an increase in AQP4 expression were observed as early as 4 weeks post-BDL, offering evidence that osmoregulation was incomplete in spite of these adaptive changes. To conclude, these early changes are suggestive of osmotic and oxidative stress and are the likely premise of any of several later factors known to be involved in HE pathogenesis. In addition, the novel finding of decreased cerebral Cr and Asc creates new opportunities to explore neuroprotective strategies for the treatment of HE. SECTION Financial support
Dysregulation of the Keap1-Nrf2 pathway has been observed in experimental and human tumors, suggesting possible roles of the pathway in cancer development. Herein, we examined whether Nrf2 (Nfe2l2) activation occurs at early steps of rat hepatocarcinogenesis, to assess critical contributions of Nrf2 to the onset of hepatocellular carcinoma (HCC). We used wild-type (WT) and Nrf2 knockout (Nrf2KO) rats treated with a single injection of diethylnitrosamine (DENA) followed by choline-devoid methionine-deficient (CMD) diet. This experimental model causes massive fatty liver and steatohepatitis with fibrosis and enables identification of early stages of hepatocarcinogenesis. We found that Nrf2 activation takes place in early preneoplastic lesions identified by the marker glutathione S-transferase placental form (GSTP). Nrf2 missense mutations, known to disrupt the Keap1-Nrf2 binding, were present in 65.7% of GSTP-positive foci. Nrf2KO rats were used to directly investigate whether Nrf2 is critical for initiation and/or clonal expansion of DENA-damaged hepatocytes. While Nrf2 genetic inactivation did not alter DENA-induced initiation, it led to increased liver injury and chronic compensatory hepatocyte regeneration when rats were fed a CMD diet. However, in spite of such a permissive environment, the livers of Nrf2KO rats did not display any preneoplastic lesion unlike those of WT rats. These results demonstrate that, in a model of hepatocarcinogenesis resembling human non-alcoholic fatty liver disease: i) Nrf2 is activated at early steps of the tumorigenic process and ii) Nrf2 is mandatory for the clonal expansion of initiated cells, indicating that Nrf2 is critical in the onset of HCC. Hepatocellular carcinoma (HCC) is the second largest cause of cancer-related deaths worldwide.1 Unfortunately, our knowledge of the genetic/epigenetic alterations implicated in HCC initiation and progression is still fragmented. The driving genetic alterations to which tumor cells are addicted and that can be targeted have not been identified. In this scenario, HCC is probably one of the tumor types where a more complete understanding of the underlying genetic alterations could have a major impact on the development of new treatment strategies. NRF2, also known as NFE2L2, is of particular interest as the role of the KEAP1-NRF2 pathway in cancer development has provided conflicting results.2–4 NRF2 is a master transcriptional activator of genes encoding enzymes that protect cells from oxidative stress and xenobiotics, as well as various drug efflux pump-members of the multidrug resistance protein family.5 NRF2 is negatively regulated and targeted to proteasomal degradation by KEAP1.6–9 It was reported in a number of studies that point mutations in the KEAP1 or NRF2 genes are often present in primary tumors.10–16 As to human HCC, two recent studies using whole-exome sequencing have revealed mutations of either NRF2 (6.4%) or KEAP1 (8%),17,18 suggesting that the dysregulation of this pathway may play a relevant role in a subset of human HCC. Notably, increased levels of NRF2 mRNA have been found associated with poor prognosis in HCC.19 
Hepatocellular carcinoma (HCC) staging according to the Barcelona Clinical Liver Cancer (BCLC) classification is based on conventional imaging. The aim of our study was to assess the impact of dual-tracer 18F-fluorocholine and 18F-fluorodeoxyglucose positron emission tomography/computed tomography (PET/CT) on tumor staging and treatment allocation. A total of 192 dual-tracer PET/CT scans (18F-fluorocholine and 18F-fluorodeoxyglucose PET/CT) were performed in 177 patients with HCC. BCLC staging and treatment proposal were retrospectively collected based on conventional imaging, along with any new lesions detected, and changes in BCLC classification or treatment allocation based on dual-tracer PET/CT. Patients were primarily men (87.5%) with cirrhosis (71%) due to alcohol ± non-alcoholic steatohepatitis (26%), viral infection (62%) or unknown causes (12%). Among 122 patients with PET/CT performed for staging, BCLC stage based on conventional imaging was 0/A in 61 patients (50%), B in 32 patients (26%) and C in 29 patients (24%). Dual-tracer PET/CT detected new lesions in 26 patients (21%), upgraded BCLC staging in 14 (11%) and modified treatment strategy in 17 (14%). In addition, dual-tracer PET/CT modified the final treatment in 4/9 (44%) patients with unexplained elevation of alpha-fetoprotein (AFP), 10/25 patients (40%) with doubtful lesions on conventional imaging and 3/36 patients (8%) waiting for liver transplantation without active HCC after tumor response following bridging therapy. When used for HCC staging, dual-tracer PET/CT enabled BCLC upgrading and treatment modification in 11% and 14% of patients, respectively. Dual-tracer PET/CT might also be useful in specific situations (an unexplained rise in AFP, doubtful lesions or pre-transplant evaluation of patients without active HCC). Hepatocellular carcinoma (HCC) is one of the leading causes of death from cancer worldwide, developing on cirrhosis in 95% of cases.1 In this setting, the diagnosis of HCC relies on non-invasive conventional imaging criteria (computed tomography [CT] and/or magnetic resonance imaging [MRI]) and/or histology.2–4 Conventional imaging (liver CT and/or MRI and lung CT) enables HCC staging according to the Barcelona Clinical Liver Cancer (BCLC) system, linking each stage to therapeutic modalities.1,5 Accurate staging with reliable imaging methods is therefore crucial to determine the best treatment strategy.6 
In cholangiocarcinoma, early metastatic spread via lymphatic vessels often precludes curative therapies. Cholangiocarcinoma invasiveness is fostered by an extensive stromal reaction, enriched in cancer-associated fibroblasts (CAFs) and lymphatic endothelial cells (LECs). Cholangiocarcinoma cells recruit and activate CAFs by secreting PDGF-D. Herein, we investigated the role of PDGF-D and liver myofibroblasts in promoting lymphangiogenesis in cholangiocarcinoma. Human cholangiocarcinoma specimens were immunostained for podoplanin (LEC marker), α-SMA (CAF marker), VEGF-A, VEGF-C, and their cognate receptors (VEGFR2, VEGFR3). VEGF-A and VEGF-C secretion was evaluated in human fibroblasts obtained from primary sclerosing cholangitis explants. Using human LECs incubated with conditioned medium from PDGF-D-stimulated fibroblasts we assessed migration, 3D vascular assembly, transendothelial electric resistance and transendothelial migration of cholangiocarcinoma cells (EGI-1). We then studied the effects of selective CAF depletion induced by the BH3 mimetic navitoclax on LEC density and lymph node metastases in vivo. In cholangiocarcinoma specimens, CAFs and LECs were closely adjacent. CAFs expressed VEGF-A and VEGF-C, while LECs expressed VEGFR2 and VEGFR3. Upon PDGF-D stimulation, fibroblasts secreted increased levels of VEGF-C and VEGF-A. Fibroblasts, stimulated by PDGF-D induced LEC recruitment and 3D assembly, increased LEC monolayer permeability, and promoted transendothelial EGI-1 migration. These effects were all suppressed by the PDGFRβ inhibitor, imatinib. In the rat model of cholangiocarcinoma, navitoclax-induced CAF depletion, markedly reduced lymphatic vascularization and reduced lymph node metastases. PDGF-D stimulates VEGF-C and VEGF-A production by fibroblasts, resulting in expansion of the lymphatic vasculature and tumor cell intravasation. This critical process in the early metastasis of cholangiocarcinoma may be blocked by inducing CAF apoptosis or by inhibiting the PDGF-D-induced axis. Cholangiocarcinoma (CCA) originates from the intrahepatic or extrahepatic bile ducts and carries a very poor prognosis.1 Despite its increasing incidence, effective treatment options for CCA are scarce and limited to surgical resection or liver transplantation in few highly selected patients.1 Less than one-third of patients are eligible for curative surgery at the time of diagnosis due to a proclivity for early lymph node metastasis.1,2 Although mechanisms promoting CCA invasiveness are still unclear,3 the lymphatic vessels that develop within the tumor provide an important initial route of metastatic dissemination. Indeed, several lines of evidence indicate that the expansion of the lymphatic bed correlates with both increased metastasis and poor prognosis in CCA.4,5 
Nivolumab, an immune checkpoint inhibitor, is approved in several countries to treat sorafenib-experienced patients with HCC, based on results from the CheckMate 040 study (NCT01658878). Marked differences exist in HCC clinical presentation, aetiology, treatment patterns and outcomes across regions. This analysis assessed the safety and efficacy of nivolumab in the Asian cohort of CheckMate 040. CheckMate 040 is an international, multicentre, open-label, phase I/II study of nivolumab in adults with advanced HCC, regardless of aetiology, not amenable to curative resection or local treatment and with/without previous sorafenib treatment. This analysis included all sorafenib-experienced patients in the intent-to-treat (ITT) overall population and Asian cohort. The analysis cut-off date was March 2018. There were 182 and 85 patients in the ITT population and Asian cohort, respectively. In both populations, most patients were older than 60 years, had BCLC (Barcelona Clinic Liver Cancer) Stage C disease, and had received previous systemic therapy. A higher percentage of Asian patients had HBV infections, extrahepatic metastases and prior therapies. Median follow-up was 31.6 and 31.3 months for the ITT and Asian patients, respectively. Objective response rates were 14% and 15% in the ITT population and Asian cohort, respectively. In the Asian cohort, patients with HBV, HCV or those who were uninfected had objective response rates of 13%, 14% and 21%, respectively. The median duration of response was longer in the ITT (19.4 months) vs. Asian patients (9.7 months). Median overall survival was similar between the ITT (15.1 months) and Asian patients (14.9 months), and unaffected by aetiology in Asian patients. The nivolumab safety profile was similar and manageable across both populations. Nivolumab safety and efficacy are comparable between sorafenib-experienced ITT and Asian patients. Worldwide, liver cancer is predicted to be the fourth most common cause of cancer-related mortality, accounting for an estimated 782,000 deaths in 2018, with most liver cancers HCC.1 However, there are global differences in HCC incidence and trends, with Eastern and Southeast Asia having among the highest incidence of liver cancer.1 This difference in HCC incidence between Asian and non-Asian regions is related to the high incidence of chronic viral hepatitis in Asia.2,3 Most Asian countries have high rates of HBV infection,2 with the exception of Japan, which has the highest rate of HCV infection of all industrialised countries.4 The incidence of HCC is trending downward in Asian regions, associated with improved control of HBV and HCV, whereas it is increasing in non-Asian regions, mainly related to non-viral etiologies. Genomic studies indicate differences in genetic mutations and signatures in HCC tumours with different aetiologies, suggesting that the genomic profile of HCC in Asian regions differs from that in non-Asian regions.3 
Sterile inflammation resulting in alcoholic hepatitis (AH) occurs unpredictably after many years of excess alcohol intake. The factors responsible for the development of AH are not known but mitochondrial damage with loss of mitochondrial function are common features. Hcar2 is a G-protein coupled receptor which is activated by β-hydroxybutyrate (BHB). We aimed to determine the relevance of the BHB-Hcar2 pathway in alcoholic liver disease. We tested if loss of BHB production can result in increased liver inflammation. We further tested if BHB supplementation is protective in AH through interaction with Hcar2, and analyzed the immune and cellular basis for protection. Humans with AH have reduced hepatic BHB, and inhibition of BHB production in mice aggravated ethanol-induced AH, with higher plasma alanine aminotransferase levels, increased steatosis and greater neutrophil influx. Conversely supplementation of BHB had the opposite effects with reduced alanine aminotransferase levels, reduced steatosis and neutrophil influx. This therapeutic effect of BHB is dependent on the receptor Hcar2. BHB treatment increased liver Il10 transcripts, and promoted the M2 phenotype of intrahepatic macrophages. BHB also increased the transcriptional level of M2 related genes in vitro bone marrow derived macrophages. This skewing towards M2 related genes is dependent on lower mitochondrial membrane potential (Δψ) induced by BHB. Collectively, our data shows that BHB production during excess alcohol consumption has an anti-inflammatory and hepatoprotective role through an Hcar2 dependent pathway. This introduces the concept of metabolite-based therapy for AH. Excess alcohol intake has many effects on the liver and can present with several clinical syndromes.1 One of the most serious is acute alcoholic hepatitis (AH), which occurs unexpectedly after decades of high levels of alcohol consumption, and is characterized by sterile liver inflammation, jaundice and can progress to a systemic inflammatory response.2 There is a mixed inflammatory infiltrate characterized by neutrophils, and upregulation of a variety of inflammatory cytokines including IL-1β, TNF-α and IL-6. Due to the ubiquitous development of hepatocyte steatosis there has been sustained interest in aspects of mitochondrial function related to lipid metabolism, particularly mitochondrial β-oxidation.3 In vivo and in vitro models have shown diverse effects of alcohol on mitochondrial biology including abnormal mitochondrial morphology (giant mitochondria), mitochondrial DNA fragmentation, deceased mitochondrial protein synthesis by inhibition of mitochondrial ribosome activity, and oxidation of mitochondrial proteins.3 Measurement of the steps in β-oxidation has demonstrated inhibition after acute and chronic alcohol exposure, which is caused by downregulation of genes involved in fatty acid uptake as well as oxidation.4,5 
Liver repair following hepatic ischemia/reperfusion (I/R) injury is crucial to survival. This study aims to examine the role of endogenous prostaglandin E2 (PGE2) produced by inducible microsomal PGE synthase-1 (mPGES-1), a terminal enzyme of PGE2 generation, in liver injury and repair following hepatic I/R. mPGES-1 deficient (Ptges−/−) mice or their wild-type (WT) counterparts were subjected to partial hepatic ischemia followed by reperfusion. The role of E prostanoid receptor 4 (EP4) was then studied using a genetic knockout model and a selective antagonist. Compared with WT mice, Ptges−/− mice exhibited reductions in alanine aminotransferase (ALT), necrotic area, neutrophil infiltration, chemokines, and proinflammatory cytokine levels. Ptges−/− mice also showed promoted liver repair and increased Ly6Clow macrophages (Ly6Clow/CD11bhigh/F4/80high-cells) with expression of anti-inflammatory and reparative genes, while WT mice exhibited delayed liver repair and increased Ly6Chigh macrophages (Ly6Chigh/CD11bhigh/F4/80low-cells) with expression of proinflammatory genes. Bone marrow (BM)-derived mPGES-1-deficient macrophages facilitated liver repair with increases in Ly6Clow macrophages. In vitro, mPGES-1 was expressed in macrophages polarized toward the proinflammatory profile. Mice treated with the mPGES-1 inhibitor Compound III displayed increased liver protection and repair. Hepatic I/R enhanced the hepatic expression of PGE receptor subtype, EP4, in WT mice, which was reduced in Ptges−/− mice. A selective EP4 antagonist and genetic deletion of Ptger4, which codes for EP4, accelerated liver repair. The proinflammatory gene expression was upregulated by stimulation of EP4 agonist in WT macrophages but not in EP4-deficient macrophages. These results indicate that mPGES-1 regulates macrophage polarization as well as liver protection and repair through EP4 signaling during hepatic I/R. Inhibition of mPGES-1 could have therapeutic potential by promoting liver repair after acute liver injury. Hepatic ischemia reperfusion (I/R) injury is a major complication of severe hypotension followed by fluid resuscitation, liver resection, and liver transplantation.1 Hepatic I/R injury followed by hepatic tissue repair significantly affects post-operative liver function and survival, since insufficient liver restoration is associated with increased morbidity and mortality.2,3 Thus, the balance between hepatocyte death and subsequent liver repair and regeneration determines the prognosis of patients with hepatic I/R injury. Nonetheless, therapeutic options for the prevention of hepatic I/R injury and that stimulate liver repair are limited. 
Virus-induced fulminant hepatitis is a major cause of acute liver failure. During acute viral hepatitis the impact of type I interferon (IFN-I) on myeloid cells, including liver-resident Kupffer cells (KC), is only partially understood. Herein, we dissected the impact of locally induced IFN-I responses on myeloid cell function and hepatocytes during acute liver inflammation. Two different DNA-encoded viruses, vaccinia virus (VACV) and murine cytomegalovirus (MCMV), were studied. In vivo imaging was applied to visualize local IFN-β induction and IFN-I receptor (IFNAR) triggering in VACV-infected reporter mice. Furthermore, mice with a cell type-selective IFNAR ablation were analyzed to dissect the role of IFNAR signaling in myeloid cells and hepatocytes. Experiments with Cx3cr1+/gfp mice revealed the origin of reconstituted KC. Finally, mixed bone marrow chimeric mice were studied to specifically analyze the effect of IFNAR triggering on liver infiltrating monocytes. VACV infection induced local IFN-β responses, which lead to IFNAR signaling primarily within the liver. IFNAR triggering was needed to control the infection and prevent fulminant hepatitis. The severity of liver inflammation was independent of IFNAR triggering of hepatocytes, whereas IFNAR triggering of myeloid cells protected from excessive inflammation. Upon VACV or MCMV infection KC disappeared, whereas infiltrating monocytes differentiated to KC afterwards. During IFNAR triggering such replenished monocyte-derived KC comprised more IFNAR-deficient than -competent cells in mixed bone marrow chimeric mice, whereas after the decline of IFNAR triggering both subsets showed an even distribution. Upon VACV infection IFNAR triggering of myeloid cells, but not of hepatocytes, critically modulates acute viral hepatitis. During infection with DNA-encoded viruses IFNAR triggering of liver-infiltrating blood monocytes delays the development of monocyte-derived KC, pointing towards new therapeutic strategies for acute viral hepatitis. Many different viral infections can cause acute hepatitis that may result in acute liver failure. Besides infections with hepatotropic virus (e.g., hepatitis B virus and hepatitis C virus) other viruses such as cytomegalovirus (CMV) can cause acute hepatitis.1–5 Yet, it is unclear to what extent viral pathogenicity or immunopathology cause liver damage. The liver is the central metabolic organ, which is characterized by a tolerogenic environment. Hepatic sinusoids are populated with antigen-presenting cells such as Kupffer cells (KC). KC are self-maintaining liver-resident macrophages, which shape the local immune milieu.6–8 During homeostasis, KC display an overall anti-inflammatory phenotype (M2-like phenotype) and primarily work as scavenger cells that eliminate insoluble macromolecules and antigens from blood.9–11 Upon infection, KC express enhanced levels of scavenger receptors, take up and kill invading pathogens, and express major histocompatibility complex class II molecules to present antigens (M1-like phenotype).8,12 Macrophage-depleted mice show enhanced susceptibility to infection with viruses and bacteria.13,14 Furthermore, liver-infiltrating monocytes can contribute to KC responses and differentiate to DC or monocyte-derived macrophages (MoMF).15–17 
Direct-acting antivirals (DAAs) have dramatically improved the outcome of patients with hepatitis C virus (HCV) infection including those with decompensated cirrhosis (DC). We analyzed the evolution of indications and results of liver transplantation (LT) in the past 10 years in Europe, focusing on the changes induced by the advent of DAAs. This is a cohort study based on data from the European Liver Transplant Registry (ELTR). Data of adult LTs performed between January 2007 to June 2017 for HCV, hepatitis B virus (HBV), alcohol (EtOH) and non-alcoholic steatohepatitis (NASH) were analyzed. The period was divided into different eras: interferon (IFN/RBV; 2007-2010), protease inhibitor (PI; 2011-2013) and second generation DAA (DAA; 2014-June 2017). Out of a total number of 60,527 LTs, 36,382 were performed in patients with HCV, HBV, EtOH and NASH. The percentage of LTs due to HCV-related liver disease varied significantly over time (p <0.0001), decreasing from 22.8% in the IFN/RBV era to 17.4% in the DAA era, while those performed for NASH increased significantly (p <0.0001). In the DAA era, the percentage of LTs for HCV decreased significantly (p <0.0001) from 21.1% (first semester 2014) to 10.6% (first semester 2017). This decline was more evident in patients with DC (HCV-DC, −58.0%) than in those with hepatocellular carcinoma (HCC) associated with HCV (HCV-HCC, −41.2%). Conversely, three-year survival of LT recipients with HCV-related liver disease improved from 65.1% in the IFN/RBV era to 76.9% in the DAA era, and is now comparable to the survival of recipients with HBV infection (p = 0.3807). In Europe, the number of LTs due to HCV infection is rapidly declining for both HCV-DC and HCV-HCC indications and post-LT survival has dramatically improved over the last three years. This is the first comprehensive study of the overall impact of DAA treatment for HCV on liver transplantation in Europe. Viral hepatitis C has long been the most common indication for liver transplantation (LT) in Europe and in the US, with over 20% of all LT candidates on the waiting list having chronic hepatitis C virus (HCV) infection.1,2 Until recently, the expected survival rates for HCV infected liver graft recipients were the lowest among all indications, due to severe and rapid HCV recurrence with interferon (IFN)-based therapies giving low cure rates.1–3 The approval of first generation direct-acting antiviral agents (DAAs), telaprevir and boceprevir, in 2011, marked the beginning of a new era. These protease inhibitors (PI) were more effective than the previous IFN-based regimens, but side-effects and frequent drug-drug interactions limited their use for patients with advanced liver disease. In 2014, more potent and better tolerated DAAs became available, and were offered first to patients with compensated and even decompensated cirrhosis. Most patients achieved a sustained virological response (SVR), allowing hepatic function to improve within months of completing treatment in the majority of patients with decompensated cirrhosis (DC) and a model for end-stage liver disease (MELD) score below 20. This resulted in one of four patients being removed from the waiting list.4–12 Concurrently, the great majority of patients with compensated cirrhosis treated with DAAs did not progress to DC and avoided LT. To better understand the impact of the new DAAs pre and post-LT, we have interrogated the ELTR registry. The two main objectives of this study were to investigate whether DAAs had influenced indications for LT and improved post-LT outcome of HCV recipients. Only patients with HCV, hepatitis B virus (HBV), alcohol (EtOH) and non-alcoholic steatohepatitis (NASH) etiologies and listed for DC or for hepatocellular carcinoma (HCC) over the last decade were analyzed. SECTION Patients and methods 
Low levels of toll-like receptor 3 (TLR3) in patients with hepatocellular carcinoma (HCC) are associated with poor prognosis, primarily owing to the loss of inflammatory signaling and subsequent lack of immune cell recruitment to the liver. Herein, we explore the role of TLR3-triggered apoptosis in HCC cells. Quantitative reverse transcription PCR, western blotting, immunohistochemistry and comparative genomic hybridization were used to analyze human and mouse HCC cell lines, as well as surgically resected primary human HCCs, and to study the impact of TLR3 expression on patient outcomes. Functional analyses were performed in HCC cells, following the restoration of TLR3 by lentiviral transduction. The role of TLR3-triggered apoptosis in HCC was analyzed in vivo in a transgenic mouse model of HCC. Lower expression of TLR3 in tumor compared to non-tumor matched tissue was observed at both mRNA and protein levels in primary HCC, and was predictive of shorter recurrence-free survival after surgical resection in both univariate (hazard ratio [HR] 1.79; 95% CI 1.04–3.06; p = 0.03) and multivariate analyses (HR 1.73; CI 1.01–2.97; p = 0.04). Immunohistochemistry confirmed frequent downregulation of TLR3 in human and mouse primary HCC cells. None of the 6 human HCC cell lines analyzed expressed TLR3, and ectopic expression of TLR3 following lentiviral transduction not only restored the inflammatory response but also sensitized cells to TLR3-triggered apoptosis. Lastly, in the transgenic mouse model of HCC, absence of TLR3 expression was accompanied by a lower rate of preneoplastic hepatocyte apoptosis and accelerated hepatocarcinogenesis without altering the tumor immune infiltrate. Downregulation of TLR3 protects transforming hepatocytes from direct TLR3-triggered apoptosis, thereby contributing to hepatocarcinogenesis and poor patient outcome. Hepatocytes are constantly exposed to chemicals, activated by bacterial products, and frequently infected by viruses. These assaults can lead to chronic hepatitis or cirrhosis, conditions associated with hepatocarcinogenesis. Similarly to other cells, hepatocytes defend themselves against viruses by initiating an innate immune response resulting in the production of type I interferons endowed with anti-viral activities.1 Activation of the innate immune system depends on the detection by Pattern Recognition Receptors (PRRs) of molecular cues signaling the presence of microbes.2 Among Toll-Like Receptors (TLRs), which represent the largest family of human PRRs, TLR3 is expressed in the lysosomes of various types of cells, including immune cells, epithelial cells, and hepatocytes. TLR3 recognizes double-strand RNA (dsRNA)3 of viral4 origin but also self dsRNA,5 and activates NF-kB-, JNK-, AP-1-, and IRF3-dependent signaling pathways in normal cells. In the liver, recognition of HCV dsRNA replicative intermediates by TLR3 protects human hepatocytes in vitro,6,7 but there is no direct evidence of its involvement in viral hepatitis. Aside from the inflammatory response, TLR3 can also trigger apoptosis in cancer cells.8–12 We previously described the signaling complex that physically recruits caspase 8 to TLR3 and drives cancer cell death while sparing normal epithelial cells,13 and a former publication suggests that TLR3 signaling could be “skewed to apoptosis” in hepatocellular carcinoma (HCC).14 In this context, poor prognosis associated with low TLR3 mRNA in HCC and decreased expression of the protein by human transformed hepatocytes have previously been reported. Indeed, Chew et al. proposed a model wherein TLR3 expressed by parenchymal and non-parenchymal liver cells, including natural killer (NK) cells, drives the intratumoral recruitment of T cells that consequently kill cancer cells.15 In the present study, we investigate the expression of TLR3 in cancer hepatocytes and its involvement in apoptotic signaling, showing that TLR3 suppression acts as an escape mechanism from apoptosis, thus leading to enhanced hepatocarcinogenesis. SECTION Materials and methods SECTION Origin and analysis of cell lines and patient samples 
Affordable point-of-care tests for hepatitis C (HCV) viraemia are needed to improve access to treatment in low- and middle-income countries. Our aims were to determine the target limit of detection (LOD) necessary to diagnose the majority of people with HCV eligible for treatment, and identify characteristics associated with low-level viraemia (LLV) (defined as the lowest 3% of the distribution of HCV RNA) to understand those at risk of being misdiagnosed. We established a multi-country cross-sectional dataset of first available quantitative HCV RNA measurements linked to demographic and clinical data. We excluded individuals on HCV treatment. We analysed the distribution of HCV RNA and determined critical thresholds for detection of HCV viraemia. We then performed logistic regression to evaluate factors associated with LLV, and derived relative sensitivities for significant covariates. The dataset included 66,640 individuals with HCV viraemia from across the world. The LOD for the 95th and 99th percentiles were 3,311 IU/ml and 214 IU/ml. The LOD for the 97th percentile was 1,318 IU/ml (95% CI 1,298.4–1,322.3). Factors associated with LLV, defined as HCV RNA <1,318 IU/ml, were younger age 18–30 vs. 51–64 years (odds ratios [OR] 2.56; 95% CI 2.19–2.99), female vs. male sex (OR 1.32; 95% CI 1.18–1.49), and advanced fibrosis stage F4 vs. F0-1 (OR 1.44; 95% CI 1.21–1.69). Only the younger age group had a decreased relative sensitivity below 95%, at 93.3%. In this global dataset, a test with an LOD of 1,318 IU/ml would identify 97% of viraemic HCV infections among almost all populations. This LOD will help guide manufacturers in the development of affordable point-of-care diagnostics to expand HCV testing and linkage to care in low- and middle-income countries. Globally, viral hepatitis is responsible for 1.34 million deaths1,2 and more than 50 million of the estimated 70 million cases of chronic hepatitis C virus (HCV) occur in low and middle-income countries (LMICs).3 The World Health Organization (WHO) defined goals towards the elimination of viral hepatitis as a public health threat, with a 90% reduction in new infections, and a 65% reduction in mortality by 2030.1,4 Achievement of these targets requires scale-up of access to affordable testing and treatment alongside interventions for HCV prevention (harm reduction and safe blood donation and injections).5 Progress in treatment scale-up is encouraging with more than 3 million treated with direct-acting antivirals since 2015, however, testing coverage and diagnosis rates are still less than 10% in LMICs.6 
Caspase 8 (CASP8) is the apical initiator caspase in death receptor-mediated apoptosis. Strong evidence for a link between death receptor signaling pathways and cholestasis has recently emerged. Herein, we investigated the role of CASP8-dependent and independent pathways during experimental cholestasis. Liver injury was characterized in a cohort of human sera (n = 28) and biopsies from patients with stage IV primary biliary cholangitis. In parallel, mice with either specific deletion of Casp8 in liver parenchymal cells (Casp8Δhepa) or hepatocytes (Casp8Δhep), and mice with constitutive Ripk3 (Ripk3−/−) deletion, were subjected to surgical ligation of the common bile duct (BDL) from 2 to 28 days. Floxed (Casp8fl/fl) and Ripk3+/+ mice were used as controls. Moreover, the pan-caspase inhibitor IDN-7314 was used, and cell death mechanisms were studied in primary isolated hepatocytes. Overexpression of activated caspase 3, CASP8 and RIPK3 was characteristic of liver explants from patients with primary biliary cholangitis. Twenty-eight days after BDL, Casp8Δhepa mice showed decreased necrotic foci, serum aminotransferase levels and apoptosis along with diminished compensatory proliferation and ductular reaction. These results correlated with a decreased inflammatory profile and ameliorated liver fibrogenesis. A similar phenotype was observed in Ripk3−/− mice. IDN-7314 treatment decreased CASP8 levels but failed to prevent BDL-induced cholestasis, independently of CASP8 in hepatocytes. These findings show that intervention against CASP8 in liver parenchymal cells – specifically in cholangiocytes – might be a beneficial option for treating obstructive cholestasis, while broad pan-caspase inhibition might trigger undesirable side effects. Cholestasis refers to a decrease in bile flow due to impaired secretion by hepatocytes or obstruction of the intra- or extrahepatic bile ducts. In humans, the most prominent disease is biliary obstruction of the common bile duct by gallstones. Other diseases include primary biliary cholangitis (PBC), primary sclerosing cholangitis or biliary atresia. Experimental surgical ligation of the common bile duct (BDL) is the standard experimental model of obstructive cholestatic injury.1 BDL causes complete biliary obstruction and accumulation of toxic bile acids in liver and serum as observed in patients. Although this model has shed some light on the mechanisms leading to hepatocellular injury, treatment options especially for non-obstructive cholestatic liver injury need to be improved. 
Robust data on hepatocellular carcinoma (HCC) incidence among HIV/hepatitis B virus (HBV)-coinfected individuals on antiretroviral therapy (ART) are needed to inform HCC screening strategies. We aimed to evaluate the incidence and risk factors of HCC among HIV/HBV-coinfected individuals on tenofovir disoproxil fumarate (TDF)-containing ART in a large multi-cohort study. We included all HIV-infected adults with a positive hepatitis B surface antigen test followed in 4 prospective European cohorts. The primary outcome was the occurrence of HCC. Demographic and clinical information was retrieved from routinely collected data, and liver cirrhosis was defined according to results from liver biopsy or non-invasive measurements. Multivariable Poisson regression was used to assess HCC risk factors. A total of 3,625 HIV/HBV-coinfected patients were included, of whom 72% had started TDF-containing ART. Over 32,673 patient-years (py), 60 individuals (1.7%) developed an HCC. The incidence of HCC remained stable over time among individuals on TDF, whereas it increased steadily among those not on TDF. Among individuals on TDF, the incidence of HCC was 5.9 per 1,000 py (95% CI 3.60–9.10) in cirrhotics and 1.17 per 1,000 py (0.56–2.14) among non-cirrhotics. Age at initiation of TDF (adjusted incidence rate ratio per 10-year increase: 2.2, 95% CI 1.6–3.0) and the presence of liver cirrhosis (4.5, 2.3–8.9) were predictors of HCC. Among non-cirrhotic individuals, the incidence of HCC was only above the commonly used screening threshold of 2 cases per 1,000 py in patients aged >45 years old at TDF initiation. Whereas the incidence of HCC was high in cirrhotic HIV/HBV-coinfected individuals, it remained below the HCC screening threshold in patients without cirrhosis who started TDF aged <46 years old. Hepatitis B virus (HBV) infection is the most important cause of liver cirrhosis and hepatocellular carcinoma (HCC) worldwide.1 In high-income settings, between 5 and 10% of HIV-infected individuals are coinfected with HBV, which is a major cause of severe morbidity and mortality in this population.2 HIV infection accelerates the progression of HBV-related liver disease and mortality is higher among HIV/HBV-coinfected individuals compared to HBV-monoinfected ones.3 While the incidence of HBV-related HCC is estimated to range between 0.1 and 0.4% per year among non-cirrhotics and to be above 3% per year among cirrhotics, it is uncertain if the risk of developing HCC is different among HIV/HBV-coinfected individuals.4 In fact, many factors which have a profound impact on HBV-related HCC incidence, including age, sex, liver cirrhosis, HBV viral load, hepatitis B e antigen (HBeAg) and hepatitis delta virus (HDV)-coinfection, are distributed differently among HIV-infected individuals compared to HIV-uninfected ones.5–8 As most HIV/HBV-coinfected individuals currently in care in high-income countries are non-cirrhotic with a suppressed HBV viral load on a TDF-containing regimen, it is of major importance to have reliable HCC incidence estimates to guide HCC surveillance. Our data suggest that the incidence of HCC is low in this group of patients, especially if TDF is initiated early during the course of disease. However, HCC events still occur in these patients and it will be important to further assess risk factors and derive predictive scores for HCC, tailored to HIV/HBV-coinfected populations. SECTION Financial support
HCV subtypes which are unusual in Europe are more prevalent in the African region, but little is known of their response to direct-acting antivirals (DAAs). These include non-1a/1b/ non-subtypeable genotype 1 (G1) or non-4a/4d (G4). In this report we aimed to describe the genotype distribution and treatment outcome in a south London cohort of African patients. We identified all patients born in Africa who attended our clinic from 2010-2018. Information on HCV genotype, treatment regimen and outcome were obtained. Non-subtypeable samples were analysed using Glasgow NimbleGen next-generation sequencing (NGS). Phylogenetic analysis was carried out by generating an uncorrected nucleotide p-distance tree from the complete coding regions of our sequences. Of 91 African patients, 47 (52%) were infected with an unusual subtype. Fourteen novel, as yet undesignated subtypes (G1*), were identified by NGS. Three individuals were infected with the same subtype, now designated as subtype 1p. Baseline sequences were available for 22 patients; 18/22 (82%) had baseline NS5A resistance-associated substitutions (RASs). Sustained virological response (SVR) was achieved in 56/63 (89%) overall, yet only in 21/28 (75%) of those with unusual G1 subtypes, with failure in 3/16 G1*, 1/2 G1p and 3/3 in G1l. Six treatment failures occurred with sofosbuvir/ledipasvir compared to 1 failure on a PI-based regimen. The SVR rate for all other genotypes and subtypes was 35/35 (100%). Most individuals in an unselected cohort of African patients were infected with an unusual genotype, including novel subtype 1p. The SVR rate of those with unusual G1 subtypes was 75%, raising concern about expansion of DAAs across Africa. Depending on the regimen used, higher failure rates in African cohorts could jeopardise HCV elimination. Direct-acting antiviral therapy (DAA) therapy has revolutionised hepatitis C (HCV) treatment. High cure rates with short courses of treatment make global eradication of HCV feasible; consequently, the World Health Organisation has promulgated a call for elimination of viral hepatitis as a public health threat by 2030.1 
More than 90% of cases of hepatocellular carcinoma (HCC) occur in patients with cirrhosis, of which alcohol is a major cause. The CIRRAL cohort aimed to assess the burden of complications in patients with alcoholic cirrhosis, particularly the occurrence of HCC. Patients with biopsy-proven compensated alcoholic cirrhosis were included then prospectively followed. The main endpoint was the incidence of HCC. Secondary outcomes were incidence of hepatic focal lesions, overall survival (OS), liver-related mortality and event-free survival (EFS). From October 2010 to April 2016, 652 patients were included in 22 French and Belgian centers. During follow-up (median 29 months), HCC was diagnosed in 43 patients. With the limitation derived from the uncertainty of consecutive patients’ inclusion and from a sizable proportion of dropouts (153/652), the incidence of HCC was 2.9 per 100 patient-years, and one- and two-year cumulative incidences of 1.8% and 5.2%, respectively. Although HCC fulfilled the Milan criteria in 33 cases (77%), only 24 patients (56%) underwent curative treatment. An explorative prognostic analysis showed that age, male gender, baseline alpha-fetoprotein, bilirubin and prothrombin were significantly associated with the risk of HCC occurrence. Among 73 deaths, 61 had a recorded cause and 27 were directly attributable to liver disease. At two years, OS, EFS and cumulative incidences of liver-related deaths were 93% (95% CI 90.5–95.4), 80.3% (95% CI 76.9–83.9), and 3.2% (95% CI 1.6–4.8) respectively. This large prospective cohort incompletely representative of the whole population with alcoholic cirrhosis showed: a) an annual incidence of HCC of up to 2.9 per 100 patient-years, suggesting that surveillance might be cost effective in these patients; b) a high proportion of HCC detected within the Milan criteria, but only one-half of detected HCC cases were referred for curative treatments; c) a two-year mortality rate of up to 7%. Primary liver cancer has a high incidence in Europe, especially in France, which has almost 9,000 cases per year.1 In more than 90% of cases, hepatocellular carcinoma (HCC) occurs in patients with cirrhosis. Alcohol is the leading cause of the underlying cirrhosis that is associated with HCC in France; it is responsible for more than 60% of the cases,2 which is much higher than the proportion of cases from hepatitis B and C and non-alcoholic steatohepatitis. Although precise data are lacking, as France has lost its longstanding European leadership in alcohol consumption, one can speculate that alcoholic cirrhosis is currently the main cause of HCC occurrence in Europe, and it will gain an even more important predominance as alcoholic consumption is on the rise in many countries, while active viral infection is on the decline. 
Terbinafine is an antifungal agent that has been associated with rare instances of hepatotoxicity. In this study we aimed to describe the presenting features and outcomes of patients with terbinafine hepatotoxicity and to investigate the role of human leukocyte antigen (HLA)-A*33:01. Consecutive high causality cases of terbinafine hepatotoxicity enrolled into the Drug Induced Liver Injury Network were reviewed. DNA samples underwent high-resolution confirmatory HLA sequencing using the Ilumina MiSeq platform. All 15 patients with terbinafine hepatotoxicity were more than 40 years old (median = 57 years), 53% were female and the median latency to onset was 38 days (range 24 to 114 days). At the onset of drug-induced liver injury, 80% were jaundiced, median serum alanine aminotransferase was 448 U/L and alkaline phosphatase was 333 U/L. One individual required liver transplantation for acute liver failure during follow-up, and 7 of the 13 (54%) remaining individuals had ongoing liver injury at 6 months, with 4 demonstrating persistently abnormal liver biochemistries at month 24. High-resolution HLA genotyping confirmed that 10 of the 11 (91%) European ancestry participants were carriers of the HLA-A*33:01, B*14:02, C*08:02 haplotype, which has a carrier frequency of 1.6% in European Ancestry population controls. One African American patient was also an HLA-A*33:01 carrier while 2 East Asian patients were carriers of a similar HLA type: A*33:03. Molecular docking studies indicated that terbinafine may interact with HLA-A*33:01 and A*33:03. Patients with terbinafine hepatotoxicity most commonly present with a mixed or cholestatic liver injury profile and frequently have residual evidence of chronic cholestatic injury. A strong genetic association of HLA-A*33:01 with terbinafine drug-induced liver injury was confirmed amongst Caucasians. Terbinafine is an orally administered antifungal agent that is commonly used to treat superficial fungal infections of the skin and nails. It was approved for use in the United States in 1998 and is typically given in an oral dose of 250 mg daily for 6 to 12 weeks. Terbinafine use is associated with mild liver biochemistry abnormalities in <1% of treated patients. However, the incidence of clinically significant liver injury attributed to terbinafine is not known because of a lack of sensitive and reliable means to detect hepatotoxicity in a prospective, population-based cohort. The liver injury may be hepatocellular at initial presentation but usually evolves into a cholestatic profile over time.1 United States Hypersensitivity and autoimmune features are uncommon and some studies suggest that a reactive metabolite may be involved that can accumulate in the bile.2 In addition, rare instances of severe hepatitis, acute liver failure and fatalities have been described with terbinafine use.3,4 
There is an emerging need to assess the metabolic state of liver allografts especially in the novel setting of machine perfusion preservation and donor in cardiac death (DCD) grafts. High-resolution magic-angle-spinning nuclear magnetic resonance (HR-MAS-NMR) could be a useful tool in this setting as it can extemporaneously provide untargeted metabolic profiling. The purpose of this study was to evaluate the potential value of HR-MAS-NMR metabolomic analysis of back-table biopsies for the prediction of early allograft dysfunction (EAD) and donor-recipient matching. The metabolic profiles of back-table biopsies obtained by HR-MAS-NMR, were compared according to the presence of EAD using partial least squares discriminant analysis. Network analysis was used to identify metabolites which changed significantly. The profiles were compared to native livers to identify metabolites for donor-recipient matching. The metabolic profiles were significantly different in grafts that caused EAD compared to those that did not. The constructed model can be used to predict the graft outcome with excellent accuracy. The metabolites showing the most significant differences were lactate level >8.3 mmol/g and phosphocholine content >0.646 mmol/g, which were significantly associated with graft dysfunction with an excellent accuracy (AUROClactates = 0.906; AUROCphosphocholine = 0.816). Native livers from patients with sarcopenia had low lactate and glycerophosphocholine content. In patients with sarcopenia, the risk of EAD was significantly higher when transplanting a graft with a high-risk graft metabolic score. This study underlines the cost of metabolic adaptation, identifying lactate and choline-derived metabolites as predictors of poor graft function in both native livers and liver grafts. HR-MAS-NMR seems a valid technique to evaluate graft quality and the consequences of cold ischemia on the graft. It could be used to assess the efficiency of graft resuscitation on machine perfusion in future studies. Liver transplantation is a life-saving procedure for patients with end-stage liver disease and a potentially curative treatment for hepatocellular carcinoma. The major limitation for liver transplantation is the current organ shortage caused by an increasing discrepancy between indications and a stable donor pool. In an attempt to answer to the issue of the increasing number of patients on waiting lists, many teams have extended the criteria for acceptance of liver grafts. Although there is hardly a wide consensus on its definition, extended criteria donors (ECD) represent a growing proportion of the donors. Many donor factors have been reported as influencing the outcome of liver transplantation mainly age,1 steatosis2,3 and cold ischemia time.4 While ECD, particularly DCD, has increased the pool of donors, it may be associated with higher graft loss5 or increased risk of vascular and biliary complications.6,7 Indeed the tolerance of allograft to cold ischemia-reperfusion injury is altered in ECD allografts.8,9 
The thymocyte selection-associated high mobility group box protein (TOX) plays a vital role in T cell development and differentiation, however, its role in T cell exhaustion was unexplored. Here, we aim to investigate the role of TOX in regulating the antitumor effect of CD8+ T cells in hepatocellular carcinoma. Fully functional, partially and severely exhausted tumor-infiltrating CD8+ T cells were sorted by flow cytometry and subjected to transcriptome sequencing analysis. Upregulated TOX expression was validated by flow cytometry. The antitumor function of CD8+ T cells with TOX downregulation or overexpression was studied in a mouse HCC model and HCC patient-derived xenograft mouse model. Transcriptome sequencing analysis was performed in TOX-overexpressing and control CD8+ T cells. The mechanism underlying the TOX-mediated regulation of PD1 expression was studied by laser confocal detection, immune co-precipitation and flow cytometer. TOX was upregulated in exhausted CD8+ T cells in hepatocellular carcinoma. TOX downregulation in CD8+ T cells inhibited tumor growth, increased CD8+ T cell infiltration, alleviated CD8+ T cell exhaustion and improved the anti-PD1 response of CD8+ T cells. The mechanism behind this involved the binding of TOX to PD1 in the cytoplasm, which facilitated the endocytic recycling of PD1, thus maintaining abundant PD1 expression at the cell surface. High expression of TOX in peripheral CD8+ T cells correlated with poorer anti-PD1 responses and prognosis. TOX promotes CD8+ T cell exhaustion in hepatocellular carcinoma by regulating endocytic recycling of PD1. Downregulating TOX expression in CD8+ T cells exerts synergistic effects with anti-PD1 therapy, highlighting a promising strategy for cancer immunotherapy. Compromised antitumor immunity characterized by the presence of dysfunctional CD8+ T cells in the tumor microenvironment (TME) is a hallmark of cancer.1,2 Long-term persistence of tumor antigens and/or the suppressive TME drive the progression of antitumor effector CD8+ T cells into a functionally impaired state called ‘T cell exhaustion’.3 Exhausted CD8+ T cells possess diminished effector functions and a distinct transcriptional profile relative to those of effector cells. Exhausted CD8+ T cells also express high amounts of inhibitory receptors, such as programmed cell death-1 (PD1), T-cell immunoglobulin and mucin-domain containing-3 (TIM3), lymphocyte-activation gene 3 (LAG3) and cytotoxic T lymphocyte-associated antigen-4 (CTLA4). In addition, exhausted CD8+ T cells show decreases in proliferative potential, diminished cytotoxic function, and reduced ability to produce effector cytokines.1,4 T cell exhaustion plays a vital role in the development and progression of cancer, but it also provides a new avenue for cancer treatment.4 It has been well documented that reversing T cell dysfunction can re-establish immune responses against virus or cancer cells.4 This effect is also evident in cancer immunotherapies that target exhausted CD8+ T cells to reinvigorate their anti- tumorigenic function by blocking inhibitory receptors.5–7 
Microvascular invasion (MVI) impairs surgical outcomes in patients with hepatocellular carcinoma (HCC). As there is no single highly reliable factor to preoperatively predict MVI, we developed a computational approach integrating large-scale clinical and imaging modalities, especially radiomic features from contrast-enhanced CT, to predict MVI and clinical outcomes in patients with HCC. In total, 495 surgically resected patients were retrospectively included. MVI-related radiomic scores (R-scores) were built from 7,260 radiomic features in 6 target volumes. Six R-scores, 15 clinical factors, and 12 radiographic scores were integrated into a predictive model, the radiographic-radiomic (RR) model, with multivariate logistic regression. Radiomics related to tumor size and intratumoral heterogeneity were the top-ranked MVI predicting features. The related R-scores showed significant differences according to MVI status (p <0.001). Regression analysis identified 8 MVI risk factors, including 5 radiographic features and an R-score. The R-score (odds ratio [OR] 2.34) was less important than tumor capsule (OR 5.12), tumor margin (OR 4.20), and peritumoral enhancement (OR 3.03). The RR model using these predictors achieved an area under the curve (AUC) of 0.909 in training/validation and 0.889 in the test set. Progression-free survival (PFS) and overall survival (OS) were significantly different between the RR-predicted MVI-absent and MVI-present groups (median PFS: 49.5 vs. 12.9 months; median OS: 76.3 vs. 47.3 months). RR-computed MVI probability, histologic MVI, tumor size, and Edmondson-Steiner grade were independently associated with disease-specific recurrence and mortality. The computational approach, integrating large-scale clinico-radiologic and radiomic features, demonstrates good performance for predicting MVI and clinical outcomes. However, radiomics with current CT imaging analysis protocols do not provide statistically significant added value to radiographic scores. Hepatocellular carcinoma (HCC) is one of the most common primary hepatic malignant tumors and its incidence is increasing worldwide.1 It is the second leading cause of cancer-specific mortality in the Asia-Pacific regions, and especially in China.2 Surgical resection and liver transplantation (LT) are potentially curative for patients with HCC,3 but recurrence after surgical treatment is common. Some studies maintained that approximately 70% of patients would suffer from recurrence within 5 years after surgical resection, and 35% after LT.4–8 
Primary sclerosing cholangitis (PSC) is an idiopathic, chronic cholestatic liver disorder characterized by biliary inflammation and fibrosis. Increased numbers of intrahepatic interferon-γ- (IFNγ) producing lymphocytes have been documented in patients with PSC, yet their functional role remains to be determined. Liver tissue samples were collected from patients with PSC. The contribution of lymphocytes to liver pathology was assessed in Mdr2−/− x Rag1−/− mice, which lack T and B cells, and following depletion of CD90.2+ or natural killer (NK)p46+ cells in Mdr2−/− mice. Liver pathology was also determined in Mdr2−/− x Ifng−/− mice and following anti-IFNγ antibody treatment of Mdr2−/− mice. Immune cell composition was analysed by multi-colour flow cytometry. Liver injury and fibrosis were determined by standard assays. Patients with PSC showed increased IFNγ serum levels and elevated numbers of hepatic CD56bright NK cells. In Mdr2−/− mice, hepatic CD8+ T cells and NK cells were the primary source of IFNγ. Depletion of CD90.2+ cells reduced hepatic Ifng expression, NK cell cytotoxicity and liver injury similar to Mdr2−/− x Rag1−/− mice. Depletion of NK cells resulted in reduced CD8+ T cell cytotoxicity and liver fibrosis. The complete absence of IFNγ in Mdr2−/−x Ifng−/− mice reduced NK cell and CD8+ T cell frequencies expressing the cytotoxic effector molecules granzyme B and TRAIL and prevented liver fibrosis. The antifibrotic effect of IFNγ was also observed upon antibody-dependent neutralisation in Mdr2−/− mice. IFNγ changed the phenotype of hepatic CD8+ T cells and NK cells towards increased cytotoxicity and its absence attenuated liver fibrosis in chronic sclerosing cholangitis. Therefore, unravelling the immunopathogenesis of PSC with a particular focus on IFNγ might help to develop novel treatment options. Primary sclerosing cholangitis (PSC) is a poorly understood chronic progressive biliary disease of unknown aetiology, characterized by biliary inflammation and fibrosis, development of cholestasis, end-stage liver disease and a high risk of malignancy. Approximately 60% of patients with PSC are male and 70 to 80% have inflammatory bowel disease. Both the incidence and prevalence of PSC are increasing, indicating that current medical treatment is poorly effective.1 
Acetaminophen (APAP) induced hepatotoxicity is a leading cause of acute liver failure worldwide. It is well established that the liver damage induced by acetaminophen exhibits diurnal variation. However, the detailed mechanism for the hepatotoxic variation is not clear. Herein, we aimed to determine the relative contributions of gut microbiota in modulating the diurnal variation of hepatotoxicity induced by APAP. Male Balb/C mice were treated with or without antibiotics and a single dose of orally administered APAP (300 mg/kg) at ZT0 (when the light is on-start of resting period) and ZT12 (when the light is off-start of active period). In agreement with previous findings, hepatic injury was markedly enhanced at ZT12 compared with ZT0. Interestingly, upon antibiotic treatment, ZT12 displayed a protective effect against APAP hepatotoxicity similar to ZT0. Moreover, mice that received the cecal content from ZT12 showed more severe liver damage than mice that received the cecal content from ZT0. 16S sequencing data revealed significant differences in the cecal content between ZT0 and ZT12 in the compositional level. Furthermore, metabolomic analysis showed that the gut microbial metabolites were also different between ZT0 and ZT12. Specifically, the level of 1-phenyl-1,2-propanedione (PPD) was significantly higher at ZT12 than ZT0. Treatment with PPD alone did not cause obvious liver damage. However, PPD synergistically enhanced APAP-induced hepatic injury in vivo and in vitro. Finally, we found Saccharomyces cerevisiae, which could reduce intestinal PPD levels, was able to markedly alleviate APAP-induced liver damage at ZT12. The gut microbial metabolite PPD was responsible, at least in part, for the diurnal variation of hepatotoxicity induced by APAP by decreasing glutathione levels. Acetaminophen (APAP) is a widely used analgesic and antipyretic drug.1,2 However, severe hepatotoxicity resulting from excessive doses is the leading cause for acute liver failure (ALF) in the Western world.3,4 APAP is absorbed from the intestine and processed by liver.5 When taken at a low dose, APAP is mainly eliminated by sulfation and glucuronidation, with the toxicity pathway making only a small contribution and mainly detoxified by glutathione (GSH).6–8 With elevation of APAP concentration in hepatocytes, a greater portion of APAP can be metabolized into N-acetyl-p-benzoquinone imine (NAPQI) by cytochrome P450s, mainly by CYP2E1.9,10 NAPQI is detoxified by conjugation with hepatic GSH. However, if GSH is exhausted, the generated reactive metabolite accumulates and exerts toxic effects initiated by covalent binding to macromolecules. This causes mitochondrial dysfunction, ultimately leading to hepatocyte necrosis and ALF.11–13 
In vitro, cell function can be potently regulated by the mechanical properties of cells and of their microenvironment. Cells measure these features by developing forces via their actomyosin cytoskeleton, and respond accordingly by regulating intracellular pathways, including the transcriptional coactivators YAP/TAZ. Whether mechanical cues are relevant for in vivo regulation of adult organ homeostasis, and whether this occurs through YAP/TAZ, remains largely unaddressed. We developed Capzb conditional knockout mice and obtained primary fibroblasts to characterize the role of CAPZ in vitro. In vivo functional analyses were carried out by inducing Capzb inactivation in adult hepatocytes, manipulating YAP/Hippo activity by hydrodynamic tail vein injections, and treating mice with the ROCK inhibitor, fasudil. We found that the F-actin capping protein CAPZ restrains actomyosin contractility: Capzb inactivation alters stress fiber and focal adhesion dynamics leading to enhanced myosin activity, increased traction forces, and increased liver stiffness. In vitro, this rescues YAP from inhibition by a small cellular geometry; in vivo, it induces YAP activation in parallel to the Hippo pathway, causing extensive hepatocyte proliferation and leading to striking organ overgrowth. Moreover, Capzb is required for the maintenance of the differentiated hepatocyte state, for metabolic zonation, and for gluconeogenesis. In keeping with changes in tissue mechanics, inhibition of the contractility regulator ROCK, or deletion of the Yap1 mechanotransducer, reverse the phenotypes emerging in Capzb-null livers. These results indicate a previously unsuspected role for CAPZ in tuning the mechanical properties of cells and tissues, which is required in hepatocytes for the maintenance of the differentiated state and to regulate organ size. More generally, it indicates for the first time that mechanotransduction has a physiological role in maintaining liver homeostasis in mammals. Cell behavior is powerfully regulated by the mechanical properties of the microenvironment. For example, seminal studies indicated that extracellular matrix (ECM) stiffness and the resulting cell geometry can drive the choice between proliferation, cell death or differentiation, often dominating soluble cues and oncogenes.1–7 The current model explaining these observations is that cells probe the physical properties of the microenvironment by exerting contractile forces on adhesion complexes generated by their actomyosin cytoskeleton.8–12 In turn, actomyosin contractility regulates intracellular signaling pathways to regulate cell behavior. 
The efficacy and safety of glecaprevir/pibrentasvir (G/P) for patients infected with hepatitis C virus (HCV) have only been investigated in clinical trials, with no real-world data currently available. The aim of our study was to investigate the effectiveness and safety of G/P in a real-world setting. All patients with HCV consecutively starting G/P between October 2017 and January 2018 within the NAVIGATORE-Lombardia Network were analyzed. G/P was administered according to drug label (8, 12 or 16 weeks). Fibrosis was staged either histologically or by liver stiffness measurement. Sustained virological response (SVR) was defined as undetectable HCV-RNA 12 weeks after the end of treatment. A total of 723 patients (50% males) were treated with G/P, 89% for 8 weeks. The median age of our cohort was 58 years, with a median body mass index of 23.9 kg/m2, and median liver stiffness measurement of 6.1 kPa; 84% were F0-2 and 16% were interferon-experienced. Median HCV-RNA was 1,102,600 IU/ml, and 49% of patients had HCV genotype 1 (32% 1b), 28% genotype 2, 10% genotype 3 and 13% genotype 4. The median estimated glomerular filtration rate was 90.2 ml/min, platelet count 209x103/mm3 and albumin 4.3 g/dl. The SVR rates were 94% in intention-to-treat and 99.3% in per protocol analysis (8-week vs. 12 or 16-week: 99.2% vs. 100%). Five patients failed therapy because of post-treatment relapse; a post-treatment NS5A resistance-associated substitution was detected in 1 case. SVR rates were lower in males (p = 0.002) and in HCV genotype-3 (p = 0.046) patients treated for 8 weeks, but independent of treatment duration, fibrosis stage, baseline HCV-RNA, HIV co-infection, chronic kidney disease stage and viral kinetics. Mild adverse events were reported in 8.3% of the patients, and 0.7% of them prematurely withdrew treatment. Three patients died of drug-unrelated causes. In a large real-world cohort of Italian patients, we confirmed the excellent effectiveness and safety of G/P administered for 8, 12 or 16 weeks. Chronic infection with hepatitis C virus (HCV) still remains a leading cause of morbidity and mortality worldwide, with more than 70 million individuals infected despite recent improvements in antiviral therapies.1,2 In the last years, direct-acting antivirals (DAA) have dramatically revolutionized this scenario, since the availability of potent interferon (IFN)-free regimens has led to an increase in sustained virological response (SVR) rates, especially among patients in whom IFN-based antiviral therapies were previously contraindicated.2,3 Moreover, because of the lack of tolerability issues, treatment indications have been widely extended to include patients with milder liver diseases, thus contributing to the goal of HCV eradication.1 
Direct-acting antiviral agents (DAAs) are safe and effective in patients with hepatitis C. Conflicting data were reported on the risk of hepatocellular carcinoma (HCC) during/after therapy with DAAs. The aim of this study was to evaluate the incidence of newly diagnosed HCC and associated risk factors in patients with advanced hepatitis C treated with DAAs. The study is based on the NAVIGATORE platform, a prospectively recording database of all patients with hepatitis C receiving DAAs in the Veneto region of Italy. The inclusion criteria were: fibrosis stage ≥F3. The exclusion criteria were: Child-Turcotte-Pugh (CTP)-C, liver transplantation before DAAs, history or presence of HCC, follow-up <4 weeks after starting DAAs. A total of 3,917 out of 4,234 consecutive patients were included, with a mean follow-up of 536.2 ± 197.6 days. Overall, HCC was diagnosed in 55 patients. During the first year, HCC incidence was 0.46% (95% CI 0.12–1.17) in F3, 1.49% (1.03–2.08) in CTP-A and 3.61% (1.86–6.31) in CTP-B cirrhotics; in the second year, HCC incidences were 0%, 0.2%, and 0.69%, respectively. By multivariate analysis, HCC was significantly associated with an aspartate aminotransferase to platelet ratio ≥2.5 (hazard ratio [HR] 2.03; 95% CI 1.14–3.61; p = 0.016) and hepatitis B virus infection (HR 3.99; 1.24–12.91; p = 0.021). Failure to achieve a sustained virological response was strongly associated with development of HCC (HR 9.09; 5.2–16.1; p = 0.0001). A total of 29% of patients with HCC had an aggressive tumor, often seen in the early phase of treatment. These data, obtained in a large, prospective, population-based study, indicate that in patients with advanced hepatitis C receiving DAAs, the risk of “de novo” hepatocarcinoma during the first year is not higher, and might be lower, than that of untreated patients. The risk further declines thereafter. Early hepatocarcinoma appearance may reflect pre-existing, microscopic, undetectable tumors. Therapy for hepatitis C virus (HCV) has been revolutionized by direct-acting antiviral agents (DAAs).1,2 DAAs are safe and highly effective in eradicating HCV in patients with different stages of HCV infection, including decompensated cirrhosis.3–8 Indeed, evidence has been provided that DAAs enable more than 90% of patients with compensated cirrhosis (Child-Turcotte-Pugh [CTP]-A) and more than 80% of those with decompensated cirrhosis (CTP-B or -C) to achieve a sustained virological response (SVR). Eradication of HCV has created great expectations in terms of clinical outcomes in these patients. Indeed, SVR obtained by DAAs was found to be associated, on a short-term basis, with significant improvement of liver function, and of prognostic scores such as CTP and model for end-stage liver disease scores.7–11 In addition, a reduction in hospitalization and mortality has been observed in HCV cirrhotics achieving SVR with DAAs.12,13 In this exciting field, controversial data have been reported on the rate of development of hepatocellular carcinoma (HCC) in patients treated with DAAs.14–19 In the interferon era, the risk of developing HCC in patients with HCV-related cirrhosis, was reported to be equal to 2–3% yearly in those who were not treated, and reduced to 0.66% in those who achieved HCV eradication with IFN-based therapy.20–24 It was therefore surprising and at the same time alarming the report of “an unexpected high rate” of HCC early recurrence during or after DAAs in patients with a previous history of cured HCC.14 Since than, there have been several publications on this topic, reporting different rates of recurrence as well as of “de novo” occurrence in patients with HCV treated with DAAs. More recently, a meta-analysis of 26 studies of interferon-based or DAA-based therapies, suggested that DAAs are not associated with increased risk of HCC occurrence.25 Accordingly, two recent large retrospective studies did not find any evidence that DAAs might favor development of HCC.26,27 Nevertheless, the issue is still debated, continuing to represent a reason of concern for treating physicians in the absence of large prospective studies. The aim of this study was therefore to assess rate of newly diagnosed HCC and associated risk factors in a large prospective, population-based study of patients with HCV and advanced liver fibrosis treated with DAAs. SECTION Patients and methods SECTION Study design 
As a nicotinamide adenine dinucleotide-dependent deacetylase and a key epigenetic regulator, sirtuin 6 (SIRT6) has been implicated in the regulation of metabolism, DNA repair, and inflammation. However, the role of SIRT6 in alcohol-related liver disease (ALD) remains unclear. The aim of this study was to investigate the function and mechanism of SIRT6 in ALD pathogenesis. We developed and characterized Sirt6 knockout (KO) and transgenic mouse models that were treated with either control or ethanol diet. Hepatic steatosis, inflammation, and oxidative stress were analyzed using biochemical and histological methods. Gene regulation was analyzed by luciferase reporter and chromatin immunoprecipitation assays. The Sirt6 KO mice developed severe liver injury characterized by a remarkable increase of oxidative stress and inflammation, whereas the Sirt6 transgenic mice were protected from ALD via normalization of hepatic lipids, inflammatory response, and oxidative stress. Our molecular analysis has identified a number of novel Sirt6-regulated genes that are involved in antioxidative stress, including metallothionein 1 and 2 (Mt1 and Mt2). Mt1/2 genes were downregulated in the livers of Sirt6 KO mice and patients with alcoholic hepatitis. Overexpression of Mt1 in the liver of Sirt6 KO mice improved ALD by reducing hepatic oxidative stress and inflammation. We also identified a critical link between SIRT6 and metal regulatory transcription factor 1 (Mtf1) via a physical interaction and functional coactivation. Mt1/2 promoter reporter assays showed a strong synergistic effect of SIRT6 on the transcriptional activity of Mtf1. Our data suggest that SIRT6 plays a critical protective role against ALD and it may serve as a potential therapeutic target for ALD. Chronic and excessive alcohol consumption causes nearly half of liver cirrhosis-associated mortality in the United States, but there remains no effective treatment for the underlying liver disorder.1 An early stage of alcohol-related liver disease (ALD), which is featured as simple hepatic steatosis, is reversible; however, chronic and excessive alcohol consumption can lead to progressive steatohepatitis (ASH) and fibrosis, and in some cases, the disease further progresses to cirrhosis and even hepatocellular carcinoma.2,3 Alcohol-induced hepatic steatosis initially manifests as lipid droplet accumulation in the liver, but as the liver tissue gets injured by the lipid overload, circulating and resident immune cells including Kupffer cells and infiltrated macrophages and neutrophils respond to the liver injury by producing inflammatory cytokines, such as tumor necrosis factor-α (TNF-α) and interleukin (IL)-1β. Chronic alcohol drinking and drinking patterns such as binges can lead to repeated liver injury, inflammation, and oxidative stress.2,4 Therefore, there is a clinical need for the better understanding of ALD pathogenesis and identification of therapeutic targets. 
Adult patients suffering from liver disease of unknown cause represent an understudied and underserved population. The use of whole-exome sequencing (WES) for the assessment of a broader spectrum of non-oncological diseases, among adults, remains poorly studied. We assessed the utility of WES in the diagnosis and management of adults with unexplained liver disease despite comprehensive evaluation by a hepatologist and with no history of alcohol overuse. We performed WES and deep phenotyping of 19 unrelated adult patients with idiopathic liver disease recruited at a tertiary academic health care center in the US. Analysis of the exome in 19 cases identified 4 monogenic disorders in 5 unrelated adults. Patient 1 suffered for 18 years from devastating complications of undiagnosed type 3 familial partial lipodystrophy due to a deleterious heterozygous variant in PPARG. Molecular diagnosis enabled initiation of leptin replacement therapy with subsequent normalization of liver aminotransferases, amelioration of dyslipidemia, and decreases in daily insulin requirements. Patients 2 and 3 were diagnosed with MDR3 deficiency due to recessive mutations in ABCB4. Patient 4 with a prior diagnosis of non-alcoholic steatohepatitis was found to harbor a mitochondrial disorder due to a homozygous pathogenic variant in NDUFB3; this finding enabled initiation of disease preventive measures including supplementation with antioxidants. Patient 5 is a lean patient with hepatic steatosis of unknown etiology who was found to have a damaging heterozygous variant in APOB. Genomic analysis yielded an actionable diagnosis in a substantial number (∼25%) of selected adult patients with chronic liver disease of unknown etiology. This study supports the use of WES in the evaluation and management of adults with idiopathic liver disease in clinical practice. Chronic liver disease (CLD) is a significant health problem affecting more than 4 million people in the United States and leading to over 40,000 deaths annually.1 CLD is often undiagnosed for many years unless there is awareness of subtle clinical signs, behavioral risk factors and/or investigation of abnormal liver function tests. In many patients, by the time overt manifestations of CLD emerge, liver injury has advanced to result in portal hypertension or hepatic decompensation. The taxonomy of CLD in clinical practice is based broadly on categories of etiology such as exposure to toxins, viral infections, cholestatic, autoimmune, metabolic and select genetic disorders. A significant limitation of this approach is that it precludes consideration of a wider array of underlying genetic disorders masquerading within these broad phenotypes. Additionally, it is estimated that up to 30% of cases of cirrhosis and up to 14% of adults awaiting liver transplantation suffer from liver disease of unknown etiology.2,3 These patients often undergo a long and costly odyssey of diagnostic tests, interventions, inappropriate therapies and medical opinions. Understanding the etiology of CLD may be essential to halt progression of liver dysfunction, as illustrated by the development of a vaccine and antiviral therapy for hepatitis B, and the highly effective, safe and curative antiviral therapies for hepatitis C.4 Advances in human genetics and genomics have created an unprecedented opportunity for gene discovery and diagnosis in the clinic. Specifically, whole-exome sequencing (WES), which consists of sequencing all the ∼20,000 human protein-coding genes, currently represents a remarkable balance between cost, time of analysis and information collected, making it attractive and suitable for clinical use and translational research studies. In pediatric cohorts, we5–7 and others8–10 have shown that WES combined with deep clinical phenotyping is an effective and unbiased means to identify rare protein-altering coding variants in individual genes. However, to date, most studies that investigate the use of next generation sequencing technologies in diagnosis and individualization of medical care have been performed in either pediatric or cancer patients. There is paucity of information on the clinical utility of these approaches for a broader spectrum of diseases among adults, such as CLD.11–13 By using unbiased genomic analysis, we also begin to understand parameters of adult clinical presentations that harbor an underlying monogenic cause, and to develop a more comprehensive category of ‘genetic’ liver diseases in adults beyond the traditionally considered disorders such as Wilson’s disease or hemochromatosis. Here, we provide data to support the utility of WES in the diagnosis and management of adults with liver disease of unknown cause with or without involvement of other diseases and/or unusual clinical findings. SECTION Patients and methods SECTION Patients 
In liver transplantation, organ shortage leads to the use of marginal grafts that are more susceptible to ischemia–reperfusion (IR) injury. We identified nucleotide-binding oligomerization domain 1 (NOD1) as an important modulator of polymorphonuclear neutrophil (PMN)-induced liver injury, which occurs in IR. Herein, we aimed to elucidate the role of NOD1 in IR injury, particularly focusing on its effects on the endothelium and hepatocytes. Nod1 WT and KO mice were treated with NOD1 agonists and subjected to liver IR. Expression of adhesion molecules was analyzed in total liver, isolated hepatocytes and endothelial cells. Interactions between PMNs and hepatocytes were studied in an ex vivo co-culture model using electron microscopy and lactate dehydrogenase levels. We generated NOD1 antagonist-loaded nanoparticles (np ALINO). NOD1 agonist treatment increased liver injury, PMN tissue infiltration and upregulated ICAM-1 and VCAM-1 expression 20 hours after reperfusion. NOD1 agonist treatment without IR increased expression of adhesion molecules (ICAM-1, VCAM-1) in total liver and more particularly in WT hepatocytes, but not in Nod1 KO hepatocytes. This induction is dependent of p38 and ERK signaling pathways. Compared to untreated hepatocytes, a NOD1 agonist markedly increased hepatocyte lysis in co-culture with PMNs as shown by the increase of lactate dehydrogenase in supernatants. Interaction between hepatocytes and PMNs was confirmed by electron microscopy. In a mouse model of liver IR, treatment with np ALINO significantly reduced the area of necrosis, aminotransferase levels and ICAM-1 expression. NOD1 regulates liver IR injury through induction of adhesion molecules and modulation of hepatocyte-PMN interactions. NOD1 antagonist-loaded nanoparticles reduced liver IR injury and provide a potential approach to prevent IR, especially in the context of liver transplantation. In the field of liver transplantation, organ shortage is a major issue; especially in western countries where the living donor transplantation is still scare. This situation led transplant units to use of liver allografts following donation after cardiac death along with marginal and extended criteria donors.1 These marginal grafts are known to be more susceptible to early graft dysfunction and retransplantation, increasing morbidity and mortality2–5 mostly due to liver ischemia–reperfusion (IR) injuries.6,7 Liver IR is a biphasic phenomenon in which hypoxia-induced lesions are exacerbated, after oxygen delivery is restored, by shear stress and tissue infiltration of polymorphonuclear neutrophils (PMNs).8 This process of liver injury also occurs during hemodynamic instability and hepatic resection. It is a frequent cause of acute liver dysfunction.2–5,9,10 Thus, preventing IR and its consequences remains a clinical challenge. 
Placement of an irradiation stent has been demonstrated to offer longer patency and survival than an uncovered self-expandable metallic stent (SEMS) in patients with unresectable malignant biliary obstruction (MBO). We aim to further assess the efficacy of an irradiation stent compared to an uncovered SEMS in those patients. We performed a randomized, open-label trial of participants with unresectable MBO at 20 centers in China. A total of 328 participants were allocated in parallel to the irradiation stent group (ISG) or the uncovered SEMS group (USG). Endpoints included stent patency (primary), technical success, relief of jaundice, overall survival, and complications. The first quartile stent patency time (when 25% of the patients experienced stent restenosis) was 212 days for the ISG and 104 days for the USG. Irradiation stents were significantly associated with a decrease in the rate of stent restenosis (9% vs. 15% at 90 days; 16% vs. 27% at 180 days; 21% vs. 33% at 360 days; p = 0.010). Patients in the ISG obtained longer survival time (median 202 days vs. 140 days; p = 0.020). No significant results were observed in technical success rate (93% vs. 95%; p = 0.499), relief of jaundice (85% vs. 80%; p = 0.308), and the incidence of grade 3 and 4 complications (8.5% vs. 7.9%; p = 0.841). Insertion of irradiation stents instead of uncovered SEMS could improve patency and overall survival in patients with unresectable MBO. Malignant biliary obstruction (MBO) is a common condition caused by biliary tract cancer (cholangiocarcinoma, gallbladder carcinoma, or ampulla carcinoma), pancreatic cancer, or metastatic lymph nodes.1 The incidence of MBO is modest in the Western world, although consistently rise; while in East and Southeast Asia, the incidence is remarkably high and poses significant public health issues.2 Surgical excision of detectable tumor is associated with improvemed survival.1 However, less than 20% of patients are surgical candidates once obstructive jaundice has occurred,3 and the long-term survival remains dismal.4,5 For unresectable tumors, stenting is considered the preferred palliative modality to relieve pruritus, cholangitis, pain, and jaundice, with or without chemotherapy.3,6 Meanwhile, conventional self-expandable metallic stent (SEMS) has a high incidence of restenosis which limits the survival benefit. Despite many advances in stent design over the past decade, none of them was acknowledged as a substitute for conventional SEMS.7 An irradiation biliary stent loaded with iodine 125 (125I) seeds has been demonstrated to offer significantly longer survival in patients with unresectable MBO when compared to an uncovered SEMS in a single-center randomized trial.8 This phase III trial aimed to further assess the efficacy of this irradiation stent in those patients. SECTION Patients and methods SECTION Study design and participants In conclusion, placement of an irradiation stent provides longer patency and subsequently improves the overall survival compared with a conventional uncovered SEMS in patients with unresectable MBO. SECTION Financial support
α1-Antitrypsin deficiency (A1ATD) is an autosomal recessive disorder caused by mutations in the SERPINA1 gene. Individuals with the Z variant (Gly342Lys) retain polymerised protein in the endoplasmic reticulum (ER) of their hepatocytes, predisposing them to liver disease. The concomitant lack of circulating A1AT also causes lung emphysema. Greater insight into the mechanisms that link protein misfolding to liver injury will facilitate the design of novel therapies. Human-induced pluripotent stem cell (hiPSC)-derived hepatocytes provide a novel approach to interrogate the molecular mechanisms of A1ATD because of their patient-specific genetic architecture and reflection of human physiology. To that end, we utilised patient-specific hiPSC hepatocyte-like cells (ZZ-HLCs) derived from an A1ATD (ZZ) patient, which faithfully recapitulated key aspects of the disease at the molecular and cellular level. Subsequent functional and “omics” comparisons of these cells with their genetically corrected isogenic-line (RR-HLCs) and primary hepatocytes/human tissue enabled identification of new molecular markers and disease signatures. Our studies showed that abnormal A1AT polymer processing (immobilised ER components, reduced luminal protein mobility and disrupted ER cisternae) occurred heterogeneously within hepatocyte populations and was associated with disrupted mitochondrial structure, presence of the oncogenic protein AKR1B10 and two upregulated molecular clusters centred on members of inflammatory (IL-18 and Caspase-4) and unfolded protein response (Calnexin and Calreticulin) pathways. These results were validated in a second patient-specific hiPSC line. Our data identified novel pathways that potentially link the expression of Z A1AT polymers to liver disease. These findings could help pave the way towards identification of new therapeutic targets for the treatment of A1ATD. α1-Antitrypsin (A1AT) is a 52 kDa protein encoded by the SERPINA1 gene synthesised primarily by hepatocytes.1 Secreted into the blood stream, it acts to control the function of neutrophil elastase, particularly in the lung.2 A1AT also exerts anti-apoptotic and anti-inflammatory properties during inflammation and hepatic injury. Most people carry the wild-type M allele, while the rarer Z variant (found in 1–3% of the population), is associated with the most common and severe form of clinically significant A1AT deficiency (A1ATD).3 The Z allele is caused by a Glu342Lys mutation in exon 5 of the SERPINA1 gene, leading to conformational instability within the protein.4 Approximately 70% of synthesised Z A1AT is degraded by intracellular quality control mechanisms, 15% is secreted whilst the remaining 15% accumulates in hepatocytes as ordered polymers. These polymers are associated with neonatal hepatitis, cirrhosis and hepatocellular carcinoma. Furthermore, the significant reduction in circulating plasma A1AT levels leads to uncontrolled proteolytic activity within the lung and development of early onset panlobular emphysema.5 
The impact of hepatitis B core antibody (anti-HBc) positive liver grafts on survival and the risk of de novo hepatitis B virus (HBV) infection after liver transplantation (LT) remain controversial. Therefore, we aimed to analyze this risk and the associated outcomes in a large cohort of patients. This was a retrospective study that included all adults who underwent LT at Queen Mary Hospital, Hong Kong, between 2000 and 2015. Data were retrieved from a prospectively collected database. Antiviral monotherapy prophylaxis was given for patients receiving grafts from anti-HBc positive donors. A total of 964 LTs were performed during the study period, with 416 (43.2%) anti-HBc positive and 548 (56.8%) anti-HBc negative donors. The median follow-up time was 7.8 years. Perioperative outcomes (hospital mortality, complications, primary nonfunction and delayed graft function) were similar between the 2 groups. The 1-, 5- and 10-year graft survival rates were comparable in anti-HBc positive (93.3%, 85.3% and 76.8%) and anti-HBc negative groups (92.5%, 82.9% and 78.4%, p = 0.944). The 1-, 5- and 10-year patient survival rates in anti-HBc positive group were 94.2%, 87% and 79% and were similar to the anti-HBc negative group (93.5%, 84% and 79.7%, p = 0.712). One-hundred and eight HBsAg negative recipients received anti-HBc positive grafts, of whom 64 received lamivudine and 44 entecavir monotherapy prophylaxis. The risk of de novo HBV was 3/108 (2.8%) and all occurred in the lamivudine era. There were 659 HBsAg-positive patients and 308 (46.7%) received anti-HBc positive grafts. The risk of HBV recurrence was similar between the 2 groups. Donor anti-HBc status did not impact on long-term patient and graft survival, or the risk of hepatocellular carcinoma recurrence after LT. De novo HBV was exceedingly rare especially with entecavir prophylaxis. Anti-HBc positive grafts did not impact on perioperative and long-term outcomes after transplant. Liver transplantation (LT) has become the standard of care for patients with end-stage liver disease and early non-resectable hepatocellular carcinoma (HCC). The growth in demand for LT has not been paralleled by a similar increase in organ supply.1 Efforts have been made to promote organ donation, to develop surgical innovations such as living donor liver transplantation (LDLT), and to promote the use of extended criteria donor (ECD) organs.1 
It remains unclear whether the classic imaging criteria for the non-invasive diagnosis of hepatocellular carcinoma (HCC) can be applied to chronic vascular liver diseases, such as Budd-Chiari syndrome (BCS). Herein, we aimed to evaluate the diagnostic value of washout for the discrimination between benign and malignant lesions in patients with BCS. This retrospective study included all patients admitted to our institution with a diagnosis of BCS and focal lesions on MRI from 2000 to 2016. MRI images were reviewed by 2 radiologists blinded to the nature of the lesions. Patient and lesion characteristics were recorded, with a focus on washout on portal venous and/or delayed phases. Lesions were compared using Chi-square, Fisher’s, Student’s t or Mann-Whitney U tests. A total of 49 patients (mean age 35 ± 12 years; 34 women [69%] and 15 men [31%]) with 241 benign lesions and 12 HCC lesions were analyzed. Patients with HCC were significantly older (mean age 44 ± 16 vs. 33 ± 9 years, p = 0.005), with higher alpha-fetoprotein (AFP) levels (median 16 vs. 3 ng/ml, p = 0.007). Washout was depicted in 9/12 (75%) HCC, and 69/241 (29%) benign lesions (p <0.001). A total of 52/143 (36%) lesions ≥1 cm with arterial hyperenhancement showed washout (9 HCC and 43 benign lesions). In this subgroup, the specificity of washout for the diagnosis of HCC was 67%. Adding T1-w hypointensity raised the specificity to 100%. A serum AFP >15 ng/ml was associated with 95% specificity. Washout was observed in close to one-third of benign lesions, leading to an unacceptably low specificity for the diagnosis of HCC. The non-invasive diagnostic criteria proposed for cirrhotic patients cannot be extrapolated to patients with BCS. Primary Budd-Chiari Syndrome (BCS) is a rare vascular disorder involving hepatic venous outflow impairment at any level between the small hepatic veins and the right atrium.1–4 In Western countries, primary BCS is commonly associated with prothrombotic conditions such as myeloproliferative neoplasms, coagulation disorders of various causes, paroxysmal nocturnal hemoglobinuria, antiphospholipid syndrome and Behçet’s disease. The use of oral contraceptives is frequently associated with BCS, but it seems to act as a co-factor.3,5–7 
Excess liver iron content is common and is linked to the risk of hepatic and extrahepatic diseases. We aimed to identify genetic variants influencing liver iron content and use genetics to understand its link to other traits and diseases. First, we performed a genome-wide association study (GWAS) in 8,289 individuals from UK Biobank, whose liver iron level had been quantified by magnetic resonance imaging, before validating our findings in an independent cohort (n = 1,513 from IMI DIRECT). Second, we used Mendelian randomisation to test the causal effects of 25 predominantly metabolic traits on liver iron content. Third, we tested phenome-wide associations between liver iron variants and 770 traits and disease outcomes. We identified 3 independent genetic variants (rs1800562 [C282Y] and rs1799945 [H63D] in HFE and rs855791 [V736A] in TMPRSS6) associated with liver iron content that reached the GWAS significance threshold (p <5 × 10−8). The 2 HFE variants account for ∼85% of all cases of hereditary haemochromatosis. Mendelian randomisation analysis provided evidence that higher central obesity plays a causal role in increased liver iron content. Phenome-wide association analysis demonstrated shared aetiopathogenic mechanisms for elevated liver iron, high blood pressure, cirrhosis, malignancies, neuropsychiatric and rheumatological conditions, while also highlighting inverse associations with anaemias, lipidaemias and ischaemic heart disease. Our study provides genetic evidence that mechanisms underlying higher liver iron content are likely systemic rather than organ specific, that higher central obesity is causally associated with higher liver iron, and that liver iron shares common aetiology with multiple metabolic and non-metabolic diseases. Liver disease constitutes the third most common cause of premature death in the UK, and its prevalence is substantially higher compared to other countries in Western Europe.1–3 Excess liver iron is associated with increased severity and progression of liver diseases including cirrhosis and hepatocellular carcinoma in individuals with non-alcoholic fatty liver disease (NAFLD),4–6 and is the direct cause of liver disease in those with hereditary haemochromatosis and thalassaemia.7,8 Observational associations have been described between excess liver iron content and several metabolic diseases such as high blood pressure, obesity, polycystic ovarian syndrome and type 2 diabetes, in a condition recognised as dysmetabolic iron overload syndrome (DIOS) which affects up to 5–10% of the general population.9,10 
Alterations of individual genes variably affect the development of hepatocellular carcinoma (HCC). Thus, we aimed to characterize the function of tumor-promoting genes in the context of gene regulatory networks (GRNs). Using data from The Cancer Genome Atlas, from the LIRI-JP (Liver Cancer – RIKEN, JP project), and from our transcriptomic, transfection and mouse transgenic experiments, we identify a GRN which functionally links LIN28B-dependent dedifferentiation with dysfunction of β-catenin (CTNNB1). We further generated and validated a quantitative mathematical model of the GRN using human cell lines and in vivo expression data. We found that LIN28B and CTNNB1 form a GRN with SMARCA4, Let-7b (MIRLET7B), SOX9, TP53 and MYC. GRN functionality is detected in HCC and gastrointestinal cancers, but not in other cancer types. GRN status negatively correlates with HCC prognosis, and positively correlates with hyperproliferation, dedifferentiation and HGF/MET pathway activation, suggesting that it contributes to a transcriptomic profile typical of the proliferative class of HCC. The mathematical model predicts how the expression of GRN components changes when the expression of another GRN member varies or is inhibited by a pharmacological drug. The dynamics of GRN component expression reveal distinct cell states that can switch reversibly in normal conditions, and irreversibly in HCC. The mathematical model is available via a web-based tool which can evaluate the GRN status of HCC samples and predict the impact of therapeutic agents on the GRN. We conclude that identification and modelling of the GRN provide insights into the prognosis of HCC and the mechanisms by which tumor-promoting genes impact on HCC development. Various etiologies are associated with HCC, leading to heterogeneity in clinical outcome, histology, transcriptomic profile and mutational spectrum.1–5 Such heterogeneity causes a variable response to therapeutic agents, as in mouse models with Ctnnb1-induced HCCs which show heterogeneous sensitivity to CTNNB1 inhibitors.6 Thus, designing novel therapeutic strategies against HCC requires the identification of inhibitors of individual tumor-promoting genes and also the characterization of the molecular networks in which those genes exert their functions. 
Neutralising antibodies (NAb) play a key role in clearance of hepatitis C virus (HCV). NAbs have been isolated and mapped to several domains on the HCV Envelope proteins. However, the immunodominance of these epitopes in HCV infection remains unknown, hindering vaccine efforts to elicit optimal epitope-specific responses. Furthermore, it remains unclear which epitope-specific responses are associated with broad NAb (bNAb) activity in primary HCV infection. The aim of this study was to define B cell immunodominance in primary HCV, and its implications on neutralisation breadth and clearance. Using samples from 168 subjects with primary HCV infection, the antibody responses targeted two immunodominant domains, termed domains B and C. Genotype 1 and 3 infections were associated with responses targeted towards different bNAb domains. No epitopes were uniquely targeted by clearers versus those who developed chronic infection. Samples with bNAb activity were enriched for multi-specific responses directed towards epitopes AR3, AR4 and domain D, and did not target non-neutralising domains. This study outlines for the first time a clear NAb immunodominance profile in primary HCV infection, and indicates that it is influenced by the infecting virus. It also highlights the need for a vaccination strategy to induce multi-specific responses that do not target non-neutralising domains. Induction of neutralising antibodies is a major goal in viral vaccine design. For highly mutable RNA viruses such as human immunodeficiency virus (HIV), hepatitis C virus (HCV) and influenza, broadly neutralising antibodies (bNAb) are thought to be required if the vaccine is to protect against the diverse strains in circulation, as well as mutants generated within-host under immune pressure. Understanding the mechanisms underlying the development of bNAb in the context of these human viral infections is therefore essential for the rational design of bNAb-inducing vaccines [1]. 
In the sera of infected patients, hepatitis C virus (HCV) particles display heterogeneous forms with low-buoyant densities (<1.08), underscoring their lipidation via association with apoB-containing lipoproteins, which was proposed to occur during assembly or secretion from infected hepatocytes. However, the mechanisms inducing this association remain poorly-defined and most cell culture grown HCV (HCVcc) particles exhibit higher density (>1.08) and poor/no association with apoB. We aimed to elucidate the mechanisms of lipidation and to produce HCVcc particles resembling those in infected sera. We produced HCVcc particles of Jc1 or H77 strains from Huh-7.5 hepatoma cells cultured in standard conditions (10%-fetal calf serum) vs. in serum-free or human serum conditions before comparing their density profiles to patient-derived virus. We also characterized wild-type and Jc1/H77 hypervariable region 1 (HVR1)-swapped mutant HCVcc particles produced in serum-free media and incubated with different serum types or with purified lipoproteins. Compared to serum-free or fetal calf serum conditions, production with human serum redistributed most HCVcc infectious particles to low density (<1.08) or very-low density (<1.04) ranges. In addition, short-time incubation with human serum was sufficient to shift HCVcc physical particles to low-density fractions, in time- and dose-dependent manners, which increased their specific infectivity, promoted apoB-association and induced neutralization-resistance. Moreover, compared to Jc1, we detected higher levels of H77 HCVcc infectious particles in very-low-density fractions, which could unambiguously be attributed to strain-specific features of the HVR1 sequence. Finally, all 3 lipoprotein classes, i.e., very-low-density, low-density and high-density lipoproteins, could synergistically induce low-density shift of HCV particles; yet, this required additional non-lipid serum factor(s) that include albumin. The association of HCV particles with lipids may occur in the extracellular milieu. The lipidation level depends on serum composition as well as on HVR1-specific properties. These simple culture conditions allow production of infectious HCV particles resembling those of chronically-infected patients. Hepatitis C virus (HCV) infection is a major cause of chronic liver diseases worldwide. Although direct-acting antivirals (DAAs) can now cure most patients, there remain major challenges in basic, translational and clinical research.1 As DAAs are only curative, the development of a protective vaccine remains an important goal; yet, this requires deeper knowledge of the HCV particle’s structure. Indeed, the HCV virion has unusually heterogenous morphology, size and properties.2 Immunocapture of its surface proteins revealed particles of 50–80 nm without symmetrical arrangement.3–6 HCV particles harbor 2 envelope glycoproteins, E1 and E2, inserted on a membranous envelope that surrounds a nucleocapsid, composed of a core protein multimer and RNA+ viral genome; yet, the organization of the virion surface remains elusive and there is currently no clear model of the HCV particle’s topology. 
Assembly of infectious hepatitis C virus (HCV) particles is known to involve host lipoproteins, giving rise to unique lipo-viro-particles (LVPs), but proteome studies now suggest that additional cellular proteins are associated with HCV virions or other particles containing the viral envelope glycoprotein E2. Many of these host cell proteins are common markers of exosomes, most notably the intracellular adaptor protein syntenin, which is required for exosome biogenesis. We aimed to elucidate the role of syntenin/E2 in HCV infection. Using cell culture-derived HCV, we studied the biogenesis and function of E2-coated exosomes in both hepatoma cells and primary human hepatocytes (PHHs). Knockout of syntenin had a negligible impact on HCV replication and virus production, whereas ectopic expression of syntenin at physiological levels reduced intracellular E2 abundance, while concomitantly increasing the secretion of E2-coated exosomes. Importantly, cells expressing syntenin and HCV structural proteins efficiently released exosomes containing E2 but lacking the core protein. Furthermore, infectivity of HCV released from syntenin-expressing hepatoma cells and PHHs was more resistant to neutralization by E2-specific antibodies and chronic-phase patient serum. We also found that high E2/syntenin levels in sera correlate with lower serum neutralization capability. E2- and syntenin-containing exosomes are a major type of particle released from cells expressing high levels of syntenin. Efficient production of E2-coated exosomes renders HCV infectivity less susceptible to antibody neutralization in hepatoma cells and PHHs. Around 71 million people worldwide are infected with the hepatitis C virus (HCV).1 Up to 80% of infected individuals are unable to clear the virus. Persistently infected individuals have a high risk of developing liver cirrhosis and hepatocellular carcinoma. Direct-acting antivirals have profoundly increased treatment efficiency, but a prophylactic vaccine required for the global control of new HCV infections is not available. 
Liver macrosteatosis (MS) is a major predictor of graft dysfunction after transplantation. However, frozen section techniques to quantify steatosis are often unavailable in the context of procurements, and the findings of preoperative imaging techniques correlate poorly with those of permanent sections, so that the surgeon is ultimately responsible for the decision. Our aim was to assess the accuracy of a non-invasive pocket-sized micro-spectrometer (PSM) for the real-time estimation of MS. We prospectively evaluated a commercial PSM by scanning the liver capsule. A double pathological quantification of MS was performed on permanent sections. Initial calibration (training cohort) was performed on 35 livers (MS ≤60%) and an algorithm was created to correlate the estimated (PSM) and known (pathological) MS values. A second assessment (validation cohort) was then performed on 154 grafts. Our algorithm achieved a coefficient of determination R2 = 0.81. Its validation on the second cohort demonstrated a Lin’s concordance coefficient of 0.78. Accuracy reached 0.91%, with reproducibility of 86.3%. The sensitivity, specificity, positive and negative predictive values for MS ≥30% were 66.7%, 100%, 100% and 98%, respectively. The PSM could predict the absence (<30%)/presence (≥30%) of MS with a kappa coefficient of 0.79. Neither graft weight nor height, donor body mass index nor the CT-scan liver-to-spleen attenuation ratio could accurately predict MS. We demonstrated that a PSM can reliably and reproducibly assess mild-to-moderate MS. Its low cost and the immediacy of results may offer considerable added-value decision support for surgeons. This tool could avoid the detrimental and prolonged ischaemia caused by the pathological examination of (potentially) marginal grafts. This device now needs to be assessed in the context of a large-scale multicentre study. The current organ shortage has led most liver transplant teams to use marginal grafts that modify the benefit-risk ratio for recipients and impose a heavy responsibility on the surgical teams.1 
Cardiovascular disease is the principle cause of death in patients with elevated liver fat unrelated to alcohol consumption, more so than liver-related morbidity and mortality. The aim of this study was to evaluate the relationship between liver fat and cardiac and autonomic function, as well as to assess how impairment in cardiac and autonomic function is influenced by metabolic risk factors. Cardiovascular and autonomic function were assessed in 96 sedentary individuals: i) non-alcoholic fatty liver disease (NAFLD) (n = 46, hepatic steatosis >5% by magnetic resonance spectroscopy), ii) Hepatic steatosis and alcohol (dual aetiology fatty liver disease [DAFLD]) (n = 16, hepatic steatosis >5%, consuming >20 g/day of alcohol) and iii) CONTROL (n = 34, no cardiac, liver or metabolic disorders, <20 g/day of alcohol). Patients with NAFLD and DAFLD had significantly impaired cardiac and autonomic function when compared with controls. Diastolic variability and systolic variability (LF/HF-sBP [n/1]; 2.3 (1.7) and 2.3 (1.5) vs. 3.4 (1.5), p <0.01) were impaired in patients with NAFLD and DAFLD when compared to controls, with DAFLD individuals showing a decrease in diastolic variability relative to NAFLD patients. Hepatic steatosis and fasting glucose were negatively correlated with stroke volume index. Fibrosis stage was significantly negatively associated with mean blood pressure (r = −0.47, p = 0.02), diastolic variability (r = −0.58, p ≤0.01) and systolic variability (r = −0.42, p = 0.04). Hepatic steatosis was independently associated with cardiac function (p ≤0.01); TNF-α (p ≤0.05) and CK-18 (p ≤0.05) were independently associated with autonomic function. Cardiac and autonomic impairments appear to be dependent on level of liver fat, metabolic dysfunction, inflammation and fibrosis staging, and to a lesser extent alcohol intake. Interventions should be sought to moderate the excess cardiovascular risk in patients with NAFLD or DAFLD. Current clinical care in chronic liver disease divides fatty liver disease into non-alcoholic fatty liver disease (NAFLD) or alcoholic fatty liver disease (ALD), predominantly based on alcohol intake.1 In line with the increase in obesity NAFLD has become the leading cause of liver disease in developed countries,2,3 closely followed by ALD, which together account for the 2 most common liver diseases worldwide.4 
Since iPSC human develop into hepatic organoids through stages that resemble human embryonic liver development, they can be used to study developmental processes and disease pathology. Therefore, we examined the early stages of hepatic organoid formation to identify key pathways affecting early liver development. Single cell RNA-sequencing and metabolomic analysis was performed on developing organoid cultures at the iPSC, hepatoblast (day 9) and mature organoid stage. The importance of the phosphatidyl-ethanolamine biosynthesis pathway to early liver development was examined in developing organoid cultures using iPSC with a CRISPR-mediated gene knockout and an over the counter medication (meclizine) that inhibits the rate-limiting enzyme in this pathway. Meclizine’s effect on the growth of a human hepatocarcinoma cell line in a xenotransplantation model and on the growth of acute myeloid leukemia cells in vitro was also examined. Transcriptomic and metabolomic analysis of organoid development indicated that the phosphatidyl-ethanolamine biosynthesis pathway is essential for early liver development. Unexpectedly, early hepatoblasts were selectively sensitive to the cytotoxic effect of meclizine. We demonstrate that meclizine could be repurposed for use in a new synergistic combination therapy for primary liver cancer: a glycolysis inhibitor reprograms cancer cell metabolism to make it susceptible to the cytotoxic effect of meclizine. This combination inhibited the growth of a human liver carcinoma cell line in vitro; and in a xenotransplantation model without causing significant side effets. This drug combination was also highly active against acute myeloid leukemic cells. Our data indicates that the phosphatidyl-ethanolamine biosynthesis is a targetable pathway for cancer; and that meclizine may have clinical efficacy as a repurposed anti-cancer drug when used as part of a new combination therapy. Organoids have provided a very powerful model system for analyzing organogenesis and disease pathophysiology (1, 2). We recently developed a novel method to direct the differentiation of induced pluripotent stem cells (iPSCs) into 3-dimensional human hepatic organoids (HOs), which consist of sheets of hepatocytes, and cholangiocytes that are organized into epithelia that surround the lumina of duct-like structures. The HOs have biosynthetic, and functional capabilities of human liver; and HOs with engineered mutations provide a unique model for studying human disease-causing genetic mutations (3). In response to changes in the growth factors added to the media, HOs develop through stages that resemble human liver during embryonic development. Because of these properties, HOs could be used to identify pathways regulating discrete stages in liver development and disease pathogenesis. 
Hepatitis C virus (HCV) infection causes chronic liver disease. Antivirals have been developed and cure infection. However, resistance can emerge and salvage therapies with alternative modes of action could be useful. Several licensed drugs have emerged as HCV entry inhibitors and are thus candidates for drug repurposing. We aimed to dissect their mode of action, identify improved derivatives and determine their viral targets. HCV entry inhibition was tested for a panel of structurally related compounds, using chimeric viruses representing diverse genotypes, in addition to viruses containing previously determined resistance mutations. Chemical modeling and synthesis identified improved derivatives, while generation of susceptible and non-susceptible chimeric viruses pinpointed E1 determinants of compound sensitivity. Molecules of the diphenylpiperazine, diphenylpiperidine, phenothiazine, thioxanthene, and cycloheptenepiperidine chemotypes inhibit HCV infection by interfering with membrane fusion. These molecules and a novel p-methoxy-flunarizine derivative with improved efficacy preferentially inhibit genotype 2 viral strains. Viral residues within a central hydrophobic region of E1 (residues 290–312) control susceptibility. At the same time, viral features in this region also govern pH-dependence of viral membrane fusion. Small molecules from different chemotypes related to flunarizine preferentially inhibit HCV genotype 2 membrane fusion. A hydrophobic region proximal to the putative fusion loop controls sensitivity to these drugs and the pH range of membrane fusion. An algorithm considering viral features in this region predicts viral sensitivity to membrane fusion inhibitors. Resistance to flunarizine correlates with more relaxed pH requirements for fusion. Hepatitis C virus (HCV) is a highly variable, enveloped virus of the family Flaviviridae. According to sequence analyses, viral isolates are classified into 7 genotypes and 86 subtypes.1 HCV particles harbor a plus-strand RNA genome of positive polarity that encodes a polyprotein comprising structural proteins, the p7 ion channel and various non-structural proteins. Virus particles are composed of the core protein that encases the viral RNA, and of the envelope 1 and envelope 2 (E1-E2) glycoproteins, which are embedded in the viral lipid membrane. Virion-associated E1-E2 glycoproteins coordinate interactions with cellular receptors, cell uptake and membrane fusion which is triggered by the low pH in cellular endosomes. 
Clinical evidence has indicated a close link between non-alcoholic fatty liver disease (NAFLD) and cardiovascular disease (CVD). However, the underlying mechanism remains to be elucidated. This study aimed to explore a potential role of hepatocyte-derived extracellular vesicles (EVs) in endothelial inflammation and atherogenesis in the context of NAFLD. EVs were isolated, quantified and characterized from steatotic hepatocytes. An endothelial cell-specific PCR array was used to screen the functional properties of EVs. Profiling of global microRNA expression was conducted in EVs. The expression level and biological function of microRNA-1 (miR-1) was determined by quantitative PCR, immunoblot and reporter gene assays, respectively. The in vivo effect of miR-1 on atherogenesis was investigated in apolipoprotein E (ApoE)-deficient mice administered with a miR-1-specific inhibitor, antagomiR-1. Steatotic hepatocytes released more EVs, which had significantly altered miRNA expression profiles compared to the EVs released by control hepatocytes. Endothelial cells co-cultured with steatotic hepatocytes, or treated with their EVs or miR-1, expressed significantly more proinflammatory molecules, as well as exhibiting increased NF-κB activity and reduced Kruppel-like factor 4 (KLF4) expression. EV-induced endothelial inflammation was prevented by either downregulation or inhibition of miR-1. While miR-1 treatment suppressed KLF4 expression and reporter gene activity, overexpression of KLF4 dramatically abolished the miR-1-induced endothelial inflammation. Moreover, not only did the miR-1 inhibitor reduce endothelial inflammation in vitro, but it also attenuated atherogenesis in ApoE-deficient mice. Steatotic hepatocyte-derived EVs promote endothelial inflammation and facilitate atherogenesis by miR-1 delivery, KLF4 suppression and NF-κB activation. The findings illustrate an important role of hepatocyte-derived EVs in distant communications between the liver and vasculature, suggesting a new mechanism underlying the link between NAFLD and CVD. Non-alcoholic fatty liver disease (NAFLD) has become a major public health concern worldwide. The disease currently affects 20–35% of the general population in Western countries, and 10% of patients can progress from benign steatosis to more severe conditions, including steatohepatitis, cirrhosis, and liver failure.1,2 Moreover, multiple lines of evidence have revealed a strong association between NAFLD and several markers of subclinical atherosclerosis independent of traditional risk factors.3–5 Thus, not only is the adverse effect of NAFLD confined to the progression of deteriorating liver function, but it also confers an independent risk for the development of atherosclerosis and other related cardiovascular diseases (CVDs). 
Endoplasmic reticulum aminopeptidase 1 (ERAP1) polymorphisms are linked with human leukocyte antigen (HLA) class I-associated autoinflammatory disorders, including ankylosing spondylitis and Behçet’s disease. Disease-associated ERAP1 allotypes exhibit distinct functional properties, but it remains unclear how differential peptide trimming in vivo affects the repertoire of epitopes presented to CD8+ T cells. The aim of this study was to determine the impact of ERAP1 allotypes on the virus-specific CD8+ T cell epitope repertoire in an HLA-B*27:05+ individual with acute hepatitis C virus (HCV) infection. We performed genetic and functional analyses of ERAP1 allotypes and characterized the HCV-specific CD8+ T cell repertoire at the level of fine epitope specificity and HLA class I restriction, in a patient who had acquired an HCV genotype 1a infection through a needle-stick injury. Two hypoactive allotypic variants of ERAP1 were identified in an individual with acute HCV infection. The associated repertoire of virus-derived epitopes recognized by CD8+ T cells was uncommon in a couple of respects. Firstly, reactivity was directed away from classically immunodominant epitopes, preferentially targeting either novel or subdominant epitopes. Secondly, reactivity was biased towards longer epitopes (10–11-mers). Despite the patient exhibiting favorable prognostic indicators, these atypical immune responses failed to clear the virus and the patient developed persistent low-level infection with HCV. ERAP1 allotypes modify the virus-specific CD8+ T cell epitope repertoire in vivo, leading to altered immunodominance patterns that may contribute to the failure of antiviral immunity after infection with HCV. Endoplasmic reticulum aminopeptidase 1 (ERAP1) trims peptides to an optimal length (usually 8 or 9 amino acids) for presentation in the context of human leukocyte antigen (HLA) class I molecules.1,2 Genome-wide association studies have identified single nucleotide polymorphisms (SNPs) in ERAP1 as important risk factors in several HLA class I-associated autoinflammatory disorders,3 including ankylosing spondylitis,4 especially in conjunction with HLA-B*27,5 and Behçet’s disease, which is strongly linked with HLA-B*51.6 In addition, SNPs in ERAP1 can combine to encode discrete allotypes with composite functional properties that further increase the risk of disease.7 Biochemical analyses have revealed differential peptide trimming among ERAP1 allotypes, with hypoactive forms typically generating longer fragments (10–12-mers), and hyperactive forms typically generating shorter fragments (7–8-mers).8–10 However, the impact of specific ERAP1 allotypes on the naturally presented repertoire of peptide epitopes is currently unclear.11 
Fontan surgery is used to treat a variety of congenital heart malformations, and may lead to advanced chronic liver disease in the long-term. This study examines the prevalence, characteristics and predictors of liver nodules in patients with Fontan surgery. This was a prospective, cross-sectional and observational study conducted at eight European centres. Consecutive patients with Fontan surgery underwent blood tests, abdominal ultrasonography (US), transient elastography (Fibroscan®), echocardiography, hemodynamics, and abdominal MRI/CT scan. The primary outcome measure was liver nodules detected in the MRI/CT scan. Predictors of liver nodules were identified by multivariate logistic regression. One hundred and fifty-two patients were enrolled (mean age 27.3 years). The mean time elapsed from surgery to inclusion was 18.3 years. Liver nodule prevalences were 29.6% (95% CI: 23–37%) on US and 47.7% (95% CI: 39-56%) on MRI/CT. Nodules were usually hyperechoic (76.5%), round-shaped (>80%), hyperenhancing in the arterial phase (92%) and located in the liver periphery (75%). The sensitivity and specificity of US were 50% (95% CI: 38-62%) and 85.3% (95% CI: 75-92%), respectively. Inter-imaging test agreement was low (adjusted kappa: 0.34). In the multivariate analysis, time since surgery > 10 years was the single independent predictor of liver nodules (OR: 4.18, P=0.040). Hepatocellular carcinoma was histologically diagnosed in 2 of the 8 patients with hypervascular and washout liver nodules. While liver nodules are frequent in Fontan patients, they may go unnoticed in US. Liver nodules are usually hyperechoic, hypervascular and predominantly peripheral. This population is at risk of hepatocellular carcinoma, the diagnosis of which requires biopsy confirmation.  
Hepatic encephalopathy (HE) is a syndrome of decreased vigilance and has been associated with impaired driving ability. The aim of this study was to evaluate the psychomotor vigilance task (PVT), which is used to assess both vigilance and driving ability, in a group of patients with cirrhosis and varying degrees of HE. A total of 145 patients (120 males, 59 ± 10 years, model for end-stage liver disease [MELD] score 13 ± 5) underwent the PVT; a subgroup of 117 completed a driving questionnaire and a subgroup of 106 underwent the psychometric hepatic encephalopathy score (PHES) and an electroencephalogram (EEG), based on which, plus a clinical evaluation, they were classed as being unimpaired (n = 51), or as having minimal (n = 35), or mild overt HE (n = 20). All patients were followed up for an average of 13 ± 5 months in relation to the occurrence of accidents and/or traffic offences, HE-related hospitalisations and death. Sixty-six healthy volunteers evenly distributed by sex, age and education served as a reference cohort for the PVT. Patients showed worse PVT performance compared with healthy volunteers, and PVT indices significantly correlated with MELD, ammonia levels, PHES and the EEG results. Significant associations were observed between neuropsychiatric performance/PVT indices and licence/driving status. PVT, PHES and EEG results all predicted HE-related hospitalisations and/or death over the follow-up period; none predicted accidents or traffic offences. However, individuals with the slowest reaction times and most lapses on the PVT were often not driving despite having a licence. When patients who had stopped driving for HE-related reasons (n = 6) were modelled as having an accident or fine over the subsequent 6 and 12 months, PVT was a predictor of accidents and traffic offences, even after correction for MELD and age. The PVT is worthy of further study for the purposes of both HE and driving ability assessment. Patients with hepatic encephalopathy (HE) exhibit psychomotor slowing and impairment of visuomotor coordination, inhibition and executive function, which can negatively impact on their fitness to drive. In 1995 Watanabe et al. found that 31% of patients with cirrhosis and 44% of patients with HE were unfit to drive based on their neuropsychological profiles.1 Bajaj et al. recently conducted a cost-effectiveness analysis to assess the benefits of different strategies of minimal HE diagnosis and treatment for reducing accident-related costs, concluding that diagnosis by the inhibitory control test (ICT) and subsequent treatment with lactulose was the most cost-effective approach, with a significant, potential reduction in societal costs by prevented accidents.2 In a standardised on-road driving test carried out by Wein et al., a professional driving instructor assessed driving performance in 14 patients with minimal HE and 34 unimpaired patients with cirrhosis. The number of times the instructor had to intervene to avoid accidents was nearly 10 times higher in patients with minimal HE compared with unimpaired patients and controls.3 Kircheis et al. studied healthy controls and patients with cirrhosis with and without HE by a real driving test (multiple sensor and camera-equipped car), laboratory, in-car computerised psychometry, and a driving instructor's assessment. Patients with HE showed significantly worse performances compared with healthy controls and unimpaired patients. Moreover, they tended to overestimate their driving abilities.4 In relation to this, Bajaj et al. were able to demonstrate some degree of improvement in self-assessment in patients with minimal HE who underwent driving simulation, including both testing and navigation tasks.5 
People with cirrhosis have unmet needs, which could benefit from a palliative care approach. Developing effective services needs to be based on evidence from those with personal experience. This review aims to explore; patient and family perspectives of perceived needs including communication; health professionals’ perspectives on delivery of care and improving palliative care between specialities. A literature search was conducted in Medline, Embase and CINAHL using key words reporting on the perspectives of patients with liver cirrhosis (18 years and over), family members or health professionals on the provision of care in liver cirrhosis. Study quality was assessed using the Mixed Methods Appraisal Tool. Qualitative and quantitative findings were grouped together according to the main relevant themes identified. Nineteen research studies predominantly from high-income Western countries were identified, with a total sample consisting of 1,413 patients, 31 family carers and 733 health professionals. Patients and family members had limited understanding of cirrhosis or its impact. They wanted better information about their disease, its treatment and help with psychological and practical needs. Health professionals had difficulty communicating about these issues to patients and their families. General practitioners left care predominantly to the liver clinicians, who lacked confidence to have discussions about prognosis or future care preferences. The role of palliative care was recognised as important in caring for this group through earlier integration with liver and community services. Health professionals need support to improve their communication with patients, to address patients’ broader needs beyond medical treatment and to develop new models to improve palliative care coordination between different medical specialities. Future research should focus on developing communication aides, testing existing tools to identify suitable patients for supportive care and exploring robust ways of evaluating supportive care interventions, with more studies needed from middle- and low-income countries. Registration number: PROSPERO CRD42017064770. Advanced liver cirrhosis is characterised by the development of clinical complications of portal hypertension or liver insufficiency.1 It is a growing international public health problem due to increases in alcohol consumption, rates of obesity and viral hepatitis.2–5 It often affects people of working age2,6 and is the third most common cause of premature death in the UK.7 
Hepatic recruitment of monocyte-derived macrophages (MoMFs) contributes to the inflammatory response in non-alcoholic steatohepatitis (NASH). However, how hepatocyte lipotoxicity promotes MoMF inflammation is unclear. Here we demonstrate that lipotoxic hepatocyte-derived extracellular vesicles (LPC-EVs) are enriched with active integrin β1 (ITGβ1), which promotes monocyte adhesion and liver inflammation in murine NASH. Hepatocytes were treated with either vehicle or the toxic lipid mediator lysophosphatidylcholine (LPC); EVs were isolated from the conditioned media and subjected to proteomic analysis. C57BL/6J mice were fed a diet rich in fat, fructose, and cholesterol (FFC) to induce NASH. Mice were treated with anti-ITGβ1 neutralizing antibody (ITGβ1Ab) or control IgG isotype. Ingenuity® Pathway Analysis of the LPC-EV proteome indicated that ITG signaling is an overrepresented canonical pathway. Immunogold electron microscopy and nanoscale flow cytometry confirmed that LPC-EVs were enriched with activated ITGβ1. Furthermore, we showed that LPC treatment in hepatocytes activates ITGβ1 and mediates its endocytic trafficking and sorting into EVs. LPC-EVs enhanced monocyte adhesion to liver sinusoidal cells, as observed by shear stress adhesion assay. This adhesion was attenuated in the presence of ITGβ1Ab. FFC-fed, ITGβ1Ab-treated mice displayed reduced inflammation, defined by decreased hepatic infiltration and activation of proinflammatory MoMFs, as assessed by immunohistochemistry, mRNA expression, and flow cytometry. Likewise, mass cytometry by time-of-flight on intrahepatic leukocytes showed that ITGβ1Ab reduced levels of infiltrating proinflammatory monocytes. Furthermore, ITGβ1Ab treatment significantly ameliorated liver injury and fibrosis. Lipotoxic EVs mediate monocyte adhesion to LSECs mainly through an ITGβ1-dependent mechanism. ITGβ1Ab ameliorates diet-induced NASH in mice by reducing MoMF-driven inflammation, suggesting that blocking ITGβ1 is a potential anti-inflammatory therapeutic strategy in human NASH. With the worldwide increase in obesity, non-alcoholic fatty liver disease (NAFLD) is currently the most common chronic liver disease.1 A subset of patients with NAFLD develop a more severe inflammatory form termed non-alcoholic steatohepatitis (NASH) which can progress to end-stage liver disease. NASH is currently the leading cause of liver-related mortality in many western countries.2 Therefore, there is an unmet need for mechanism-based therapeutic strategies that reverse established NASH and control the progression of the disease. 
There is growing evidence that liver graft ischemia-reperfusion (I/R) is a risk factor for hepatocellular carcinoma (HCC) recurrence, but the mechanisms involved are unclear. Herein, we tested the hypothesis that mesenteric congestion resulting from portal blood flow interruption induces endotoxin-mediated Toll-like receptor 4 (Tlr4) engagement, resulting in elevated liver cancer burden. We also assessed the role of remote ischemic preconditioning (RIPC) in this context. C57Bl/6j mice were exposed to standardized models of liver I/R injury and RIPC, induced by occluding the hepatic and femoral blood vessels. HCC was induced by injecting RIL-175 cells into the portal vein. We further evaluated the impact of the gut–liver axis (lipopolysaccharide (LPS)-Tlr4 pathway) in this context by studying mice with enhanced (lipopolysaccharide infusion) or defective (Tlr4−/− mice, gut sterilization, and Tlr4 antagonist) Tlr4 responses. Portal triad clamping provoked upstream mesenteric venous engorgement and increased bacterial translocation, resulting in aggravated tumor burden. RIPC prevented this mechanism by preserving intestinal integrity and reducing bacterial translocation, thereby mitigating HCC recurrence. These observations were linked to the LPS-Tlr4 pathway, as supported by the high and low tumor burden displayed by mice with enhanced or defective Tlr4 responses, respectively. Modulation of the gut–liver axis and the LPS-Tlr4 response by RIPC, gut sterilization, and Tlr4 antagonism represents a potential therapeutic target to prevent I/R lesions, and to alleviate HCC recurrence after liver transplantation and resection. Hepatocellular carcinoma (HCC) is the third most common cause of cancer-related death worldwide (www.who.int). Although surgical treatments, including liver resection and liver transplantation, are the best options for patients with curable HCC, cancer recurrence remains the main limitation to these strategies.1,2 In addition to tumor biology, which is the main driving factor for HCC recurrence, preclinical and clinical evidence suggests that organ damage, such as ischemia/reperfusion (I/R) injury, favors the recurrence of HCC and colorectal metastases after liver surgery.3,4 In this regard, experimental studies have demonstrated that liver I/R injury leads to sinusoidal microthrombi, neutrophil sequestration, and the release of proinflammatory and proregenerative molecules5 in the liver, facilitating the entrapment and proliferation of circulating cancer cells in the injured liver. 
Acetaminophen (APAP)-induced acute liver failure is associated with substantial alterations in the hemostatic system. In mice, platelets accumulate in the liver after APAP overdose and appear to promote liver injury. Interestingly, patients with acute liver injury have highly elevated levels of the platelet-adhesive protein von Willebrand factor (VWF), but a mechanistic connection between VWF and progression of liver injury has not been established. We tested the hypothesis that VWF contributes directly to experimental APAP-induced acute liver injury. Wild-type mice and VWF-deficient (Vwf−/−) mice were given a hepatotoxic dose of APAP (300 mg/kg, i.p.) or vehicle (saline). VWF plasma levels were measured by ELISA, and liver necrosis or hepatocyte proliferation was measured by immunohistochemistry. Platelet and VWF deposition were measured by immunofluorescence. In wild-type mice, VWF plasma levels, high molecular weight (HMW) VWF multimers, and VWF activity decreased 24 h after APAP challenge. These changes coupled to robust hepatic VWF and platelet deposition, although VWF deficiency had minimal effect on peak hepatic platelet accumulation or liver injury. VWF plasma levels were elevated 48 h after APAP challenge, but with relative reductions in HMW multimers and VWF activity. Whereas hepatic platelet aggregates persisted in livers of APAP-challenged wild-type mice, platelets were nearly absent in Vwf−/− mice 48 h after APAP challenge. The absence of platelet aggregates was linked to dramatically accelerated repair of the injured liver. Complementing observations in Vwf−/− mice, blocking VWF or the platelet integrin αIIbβ3 during development of injury significantly reduced hepatic platelet aggregation and accelerated liver repair in APAP-challenged wild-type mice. These studies are the first to suggest a mechanistic link between VWF, hepatic platelet accumulation, and liver repair. Targeting VWF might provide a novel therapeutic approach to improve repair of the APAP-injured liver. Acetaminophen (paracetamol, APAP) overdose is a leading cause of drug-induced acute liver injury and acute liver failure (ALF) in the Western world.1 Accumulating evidence from experimental and clinical studies suggests that the hemostatic system contributes to the progression of acute liver injury after APAP overdose.2–4 Although not associated with clinically significant bleeding,5 substantial alterations in the hemostatic system are evident in patients with APAP-induced liver failure, including a reduced platelet count.6,7 The ALF study group has demonstrated in a large cohort of ALF patients that a much more profound thrombocytopenia is associated with poor outcome (i.e. death or the need for a liver transplant).8 Platelets have been proposed to drive disease progression through the formation of microthrombi within the liver microvasculature.9 APAP-induced liver damage in mice causes persistent thrombocytopenia, which is coupled to accumulation of platelets in the injured liver.3 Notably, platelets have been shown to promote APAP-induced liver injury in mice.3 However, the mechanisms driving platelet accumulation in the APAP-injured liver remain to be elucidated. 
Chronic liver injury often results in the activation of hepatic myofibroblasts and the development of liver fibrosis. Hepatic myofibroblasts may originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes, with varying contributions depending on the etiology of liver injury. Here, we assessed the composition of hepatic myofibroblasts in multidrug resistance gene 2 knockout (Mdr2−/−) mice, a genetic model that resembles primary sclerosing cholangitis in patients. Mdr2−/− mice expressing a collagen-GFP reporter were analyzed at different ages. Hepatic non-parenchymal cells isolated from collagen-GFP Mdr2−/− mice were sorted based on collagen-GFP and vitamin A. An NADPH oxidase (NOX) 1/4 inhibitor was administrated to Mdr2−/− mice aged 12–16 weeks old to assess the therapeutic approach of targeting oxidative stress in cholestatic injury. Thy1+ activated PFs accounted for 26%, 51%, and 54% of collagen-GFP+ myofibroblasts in Mdr2−/− mice at 4, 8, and 16 weeks of age, respectively. The remaining collagen-GFP+ myofibroblasts were composed of activated HSCs, suggesting that PFs and HSCs are both activated in Mdr2−/− mice. Bone-marrow-derived fibrocytes minimally contributed to liver fibrosis in Mdr2−/− mice. The development of cholestatic liver fibrosis in Mdr2−/− mice was associated with early recruitment of Gr1+ myeloid cells and upregulation of pro-inflammatory cytokines (4 weeks). Administration of a NOX inhibitor to 12-week-old Mdr2−/− mice suppressed the activation of myofibroblasts and attenuated the development of cholestatic fibrosis. Activated PFs and activated HSCs contribute to cholestatic fibrosis in Mdr2−/− mice, and serve as targets for antifibrotic therapy. Chronic liver injury often results in liver fibrosis. The development of liver fibrosis is associated with migration and proliferation of collagen type I-producing myofibroblasts, which are not present in the normal liver. Activated myofibroblasts originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes.1−4 Activated HSCs (aHSCs) were implicated in the pathogenesis of experimental toxic liver fibrosis, such as chronic CCl4 administration and alcoholic liver disease,1,5 while PFs are predominantly activated in response to cholestatic liver fibrosis, such as bile duct ligation (BDL).2 
The progression of hepatosteatosis to non-alcoholic steatohepatitis (NASH) is a critical step in the pathogenesis of hepatocellular cancer. However, the underlying mechanism(s) for this progression is essentially unknown. This study was designed to determine the role of miR-378 in regulating NASH progression. We used immunohistochemistry, luciferase assays and immunoblotting to study the role of miR-378 in modulating an inflammatory pathway. Wild-type mice kept on a high-fat diet (HFD) were injected with miR-378 inhibitors or a mini-circle expression system containing miR-378, to study loss and gain-of functions of miR-378. MiR-378 expression is increased in livers of dietary obese mice and patients with NASH. Further studies revealed that miR-378 directly targeted Prkag2 that encodes AMP-activated protein kinase γ 2 (AMPKγ2). AMPK signaling negatively regulates the NF-κB-TNFα inflammatory axis by increasing deacetylase activity of sirtuin 1. By targeting Prkag2, miR-378 reduced sirtuin 1 activity and facilitated an inflammatory pathway involving NF-κB-TNFα. In contrast, miR-378 knockdown induced expression of Prkag2, increased sirtuin 1 activity and blocked the NF-κB-TNFα axis. Additionally, knockdown of increased Prkag2 offset the inhibitory effects of miR-378 inhibitor on the NF-κB-TNFα axis, suggesting that AMPK signaling mediates the role of miR-378 in facilitating this inflammatory pathway. Liver-specific expression of miR-378 triggered the development of NASH and fibrosis by activating TNFα signaling. Ablation of TNFα in miR-378-treated mice impaired the ability of miR-378 to facilitate hepatic inflammation and fibrosis, suggesting that TNFα signaling is required for miR-378 to promote NASH. MiR-378 plays a key role in the development of hepatic inflammation and fibrosis by positively regulating the NF-κB-TNFα axis. MiR-378 is a potential therapeutic target for the treatment of NASH. The incidence of non-alcoholic fatty liver disease (NAFLD) is estimated to be 20–45% in the general population of Western countries.1,2 Although NAFLD carries a relatively benign prognosis, a significant proportion of patients can progress to non-alcoholic steatohepatitis (NASH) and later cirrhosis with the risk of developing hepatocellular carcinoma (HCC).3,4 It is estimated that NASH-related HCC accounts for ≥13% of cases in the US.5–7 Although the progression of hepatosteatosis to NASH has been studied extensively, the precise mechanism(s) is still under intense investigation. 
To eliminate hepatitis B virus (HBV) infection, it is essential to scale up antiviral treatment through decentralized services. However, access to the conventional tools to assess treatment eligibility (liver biopsy/Fibroscan®/HBV DNA) is limited and not affordable in resource-limited countries. We developed and validated a simple score to easily identify patients in need of HBV treatment in Africa. As a reference, we used treatment eligibility determined by the European Association for the Study of the Liver based on alanine aminotransferase (ALT), liver histology and/or Fibroscan and HBV DNA. We derived a score indicating treatment eligibility by a stepwise logistic regression using a cohort of chronic HBV infection in The Gambia (n = 804). We subsequently validated the score in an external cohort of HBV-infected Africans from Senegal, Burkina Faso, and Europe (n = 327). Out of several parameters, two remained in the final model, namely HBV e antigen (HBeAg) and ALT level, constituting a simple score (treatment eligibility in Africa for the hepatitis B virus: TREAT-B). The score demonstrated a high area under the receiver operating characteristic curve (0.85, 95% CI 0.79–0.91) in the validation set. The score of 2 and above (HBeAg-positive and ALT ≥20 U/L or HBeAg-negative and ALT ≥40 U/L) had a sensitivity and specificity for treatment eligibility of 85% and 77%, respectively. The sensitivity and specificity of the World Health Organization criteria based on the aspartate aminotransferase-to-platelet ratio index (APRI) and ALT were 90% and 40%, respectively. A simple score based on HBeAg and ALT had a high diagnostic accuracy for the selection of patients for HBV treatment. This score could be useful in African settings. Viral hepatitis is a major global health problem. In 2013, an estimated 1.45 million people died from viral hepatitis.1 This is the seventh leading cause of death worldwide, ranked higher than any of the major infectious agents: human immunodeficiency virus (HIV), tuberculosis and malaria. Of these hepatitis-related deaths, most of them occur in low-income and middle-income countries (LMICs), and about half are attributable to hepatitis B virus (HBV) infection, causing cirrhosis and hepatocellular carcinoma (HCC).1 
Neuronal function is exquisitely sensitive to alterations in the extracellular environment. In patients with hepatic encephalopathy (HE), accumulation of metabolic waste products and noxious substances in the interstitial fluid of the brain is thought to result from liver disease and may contribute to neuronal dysfunction and cognitive impairment. This study was designed to test the hypothesis that the accumulation of these substances, such as bile acids, may result from reduced clearance from the brain. In a rat model of chronic liver disease with minimal HE (the bile duct ligation [BDL] model), we used emerging dynamic contrast-enhanced MRI and mass-spectroscopy techniques to assess the efficacy of the glymphatic system, which facilitates clearance of solutes from the brain. Immunofluorescence of aquaporin-4 (AQP4) and behavioural experiments were also performed. We identified discrete brain regions (olfactory bulb, prefrontal cortex and hippocampus) of altered glymphatic clearance in BDL rats, which aligned with cognitive/behavioural deficits. Reduced AQP4 expression was observed in the olfactory bulb and prefrontal cortex in HE, which could contribute to the pathophysiological mechanisms underlying the impairment in glymphatic function in BDL rats. This study provides the first experimental evidence of impaired glymphatic flow in HE, potentially mediated by decreased AQP4 expression in the affected regions. The mechanisms underlying the pathogenesis of hepatic encephalopathy (HE) in patients with cirrhosis (chronic liver disease) are not completely understood. Data available in the literature suggest that noxious substances and metabolites such as lactate, glutamate, bile acids and drugs accumulate in the brain of patients with HE.1 The prevailing hypothesis proposes that this occurs because of metabolic and transporter defects induced by hyperammonaemia, inflammation and alterations in blood brain barrier function.2,3 
The microenvironment regulates hepatoma stem cell behavior. However, the contributions of lymphatic endothelial cells to the hepatoma stem cell niche remain largely unknown; we aimed to analyze this contribution and elucidate the mechanisms behind it. Associations between lymphatic endothelial cells and CD133+ hepatoma stem cells were analyzed by immunofluorescence and adhesion assays; with the effects of their association on IL-17A expression examined using western blot, quantitative reverse transcription PCR and luciferase reporter assay. The effects of IL-17A on the self-renewal and tumorigenesis of hepatoma stem cells were examined using sphere and tumor formation assays. The role of IL-17A in immune escape by hepatoma stem cells was examined using flow cytometry. The expression of IL-17A in hepatoma tissues was examined using immunohistochemistry. CD133+ hepatoma stem cells preferentially interact with lymphatic endothelial cells. The interaction between the mannose receptor and high-mannose type N-glycans mediates the interaction between CD133+ hepatoma stem cells and lymphatic endothelial cells. This interaction activates cytokine IL-17A expression in lymphatic endothelial cells. IL-17A promotes the self-renewal of hepatoma stem cells. It also promotes their immune escape, partly through upregulation of PD-L1. Interactions between lymphatic endothelial cells and hepatoma stem cells promote the self-renewal and immune escape of hepatoma stem cells, by activating IL-17A signaling. Thus, inhibiting IL-17A signaling may be a promising approach for hepatoma treatment. Hepatoma stem cells are thought to be responsible for the initial development, progression and therapeutic resistance of hepatomas.1 For example, CD133+ hepatoma cells highly express stemness genes, possess multi-lineage differentiation capabilities and are highly tumorigenic in immunocompromised mice.2,3 Exploring the mechanisms behind the highly tumorigenic nature of hepatoma stem cells will contribute to the development of therapeutic approaches for hepatocellular carcinoma. 
Frail patients with low model for end-stage liver disease (MELD) scores may be under-prioritised. Low skeletal muscle mass, namely sarcopenia, has been identified as a risk factor for waiting list mortality. A recent study proposed incorporating sarcopenia in the MELD score (MELD-Sarcopenia score). We aimed to investigate the association between sarcopenia and waiting list mortality, and to validate the MELD-Sarcopenia score (i.e. MELD + 10.35 * Sarcopenia). We identified consecutive patients with cirrhosis listed for liver transplantation in the Eurotransplant registry between 2007–2014 and measured skeletal muscle mass on computed tomography. A competing risk analysis was used to compare survival of patients with and without sarcopenia, and concordance (c) indices were calculated to assess performance of the MELD and MELD-Sarcopenia score. We created a nomogram of the best predictive model. We included 585 patients with a median MELD score of 14 (interquartile range 9–19), of which 254 (43.4%) were identified as having sarcopenia. Median waiting list survival was shorter in patients with sarcopenia than those without (p <0.001). This effect was even more pronounced in patients with MELD ≤15. The discriminative performance of the MELD-Sarcopenia score (c-index 0.820) for three-month mortality was lower than MELD score alone (c-index 0.839). Apart from sarcopenia and MELD score, other predictive variables were occurrence of hepatic encephalopathy before listing and recipient age. A model including all these variables yielded a c-index of 0.851. Sarcopenia was associated with waiting list mortality in liver transplant candidates with cirrhosis, particularly in patients with lower MELD scores. The MELD-Sarcopenia score was successfully validated in this cohort. However, incorporating sarcopenia in the MELD score had limited added value in predicting waiting list mortality. Model for end-stage liver disease (MELD) score is the most frequently used method to prioritise patients with end-stage liver disease for liver transplantation and it is calculated using serum levels of bilirubin, creatinine and the international normalized ratio (INR).1 Despite its strong predictive value, the MELD score underestimates disease severity in about 15–20% of patients with cirrhosis, resulting in an inaccurate prediction of survival.2 Amongst others, conditions such as hyponatremia and hypoalbuminemia have been identified as additional risk factors for impaired waiting list survival. This knowledge resulted in modifications of the original MELD score; i.e. the MELDNa and five-variable MELD score, respectively.3–5 Moreover, a frequently reported drawback of the MELD score is the lack of an objective parameter reflecting patients’ physical and nutritional status, as was albumin in the old Child-Turcotte-Pugh score. Consequently, patients with a biochemically low MELD score, but with malnutrition or low skeletal muscle mass (sarcopenia), may be under prioritised in the current system.6 Indeed, sarcopenia, a hallmark of frailty and functional decline,7,8 has recently been found to predict waiting list mortality.9,10 Montano-Loza et al. found a significantly shorter waiting list survival in patients with sarcopenia, and therefore included sarcopenia in the MELD score (MELD-Sarcopenia score). This score showed a higher predictive accuracy for waiting list mortality than the MELD score alone.10 
Chronic hepatitis C virus (HCV) infection is a global health burden. Although HCV infection rarely contributes to morbidity during childhood, most HCV-infected children develop chronic HCV with a lifetime risk of liver disease. Little is known about the development of long-term liver disease and the effect of treatment in patients infected with HCV in childhood. This study was a retrospective review of patients infected with HCV in childhood enrolled in HCV Research UK. A total of 1,049 patients were identified and included. The main routes of infection were intravenous drug use (53%), blood product exposure (24%) and perinatal infection (11%). Liver disease developed in 32% of patients, a median of 33 years after infection, irrespective of the mode of infection. Therefore, patients with perinatal exposure developed cirrhosis at an earlier age than the rest of the risk groups. The incidence of hepatocellular carcinoma (HCC) was 5%, liver transplant 4% and death occurred in 3%. Overall, 663 patients were treated (55% with interferon/pegylated interferon and 40% with direct-acting antivirals). Sustained virological response (SVR) was achieved in 406 (75%). There was a higher mortality rate among patients without SVR vs. those with SVR (5% vs. 1%, p = 0.003). Treatment was more effective in patients without cirrhosis and disease progression was less frequent (13%) than in patients with cirrhosis at the time of therapy (28%) p < 0.001. Patients with cirrhosis were more likely to develop HCC, require liver transplantation, or die. HCV infection in young people causes significant liver disease, which can now be prevented with antiviral therapy. Early treatment, especially before development of cirrhosis is essential. Detection of HCV should be aimed at relevant risk groups and antiviral therapy should be made available in childhood to prevent long-term liver disease and spread of HCV. Chronic hepatitis C virus (HCV) infection is a global health burden with an estimated prevalence varying between 0.6%–10% dependent on geographical location and an estimated 71 million people worldwide with chronic infection.1–3 In Western Europe, the estimated prevalence is 1.5%–3.5%, but in the UK it is 0.5%. Chronic HCV is associated with increased morbidity and mortality and is a leading cause of end-stage liver disease, cirrhosis and liver cancer worldwide.3–6 Although HCV infection rarely contributes to morbidity during childhood, the majority of HCV-infected children develop chronic HCV with a lifetime risk of serious liver disease.7 Furthermore, some studies indicate that HCV affects childhood quality of life and behaviour, as cognitive function has been shown to be affected and families report increased stress which affects family dynamics and wellbeing.7,8 
Non-alcoholic fatty liver disease/non-alcoholic steatohepatitis (NAFLD/NASH) is an increasing clinical problem associated with progression to hepatocellular carcinoma (HCC). The effect of a high-fat diet on the early immune response in HCC is poorly understood, while the role of metformin in treating NAFLD and HCC remains controversial. Herein, we visualized the early immune responses in the liver and the effect of metformin on progression of HCC using optically transparent zebrafish. We used live imaging to visualize liver inflammation and disease progression in a NAFLD/NASH-HCC zebrafish model. We combined a high-fat diet with a transgenic zebrafish HCC model induced by hepatocyte-specific activated beta-catenin and assessed liver size, angiogenesis, micronuclei formation and inflammation in the liver. In addition, we probed the effects of metformin on immune cell composition and early HCC progression. We found that a high-fat diet induced an increase in liver size, enhanced angiogenesis, micronuclei formation and neutrophil infiltration in the liver. Although macrophage number was not affected by diet, a high-fat diet induced changes in macrophage morphology and polarization with an increase in liver associated TNFα-positive macrophages. Treatment with metformin altered macrophage polarization, reduced liver size and reduced micronuclei formation in NAFLD/NASH-associated HCC larvae. Moreover, a high-fat diet reduced T cell density in the liver, which was reversed by treatment with metformin. These findings suggest that diet alters macrophage polarization and exacerbates the liver inflammatory microenvironment and cancer progression in a zebrafish model of NAFLD/NASH-associated HCC. Metformin specifically affects the progression induced by diet and modulates the immune response by affecting macrophage polarization and T cell infiltration, suggesting possible effects of metformin on tumor surveillance. Hepatocellular carcinoma (HCC) is a common cause of cancer-related deaths with increasing mortality worldwide.1 In Western societies, 30–40% of patients with HCC are obese and have non-alcoholic steatohepatitis (NASH), an aggressive form of non-alcoholic fatty liver disease (NAFLD).2–5 Abnormal lipid accumulation in hepatocytes increases oxidative stress and leads to lipotoxicity, which triggers liver inflammation, a hallmark of NAFLD progression to HCC.6 Pro-tumorigenic subsets of neutrophils, macrophages, and other immune cells provide the tumor microenvironment (TME) with growth factors, matrix-remodeling factors and inflammatory mediators that optimize tumor growth.7–10 Hepatic macrophages in particular, including both monocyte-derived or tissue-resident macrophages known as Kupffer cells, have been identified as potential drug targets to treat liver disease.11 Several studies have shown that NAFLD progression to HCC involves inflammatory macrophages12 and Kupffer cells.13 Adaptive immune cells can also be modulators of hepatocarcinogenesis. NAFLD/NASH impairs tumor surveillance by inducing apoptosis of CD4+ T cells.14 Taken together, this previous work suggests that the innate and adaptive immune systems are key players in the progression of NAFLD-associated HCC. However, the specific cellular and molecular immune mechanisms that regulate the pathogenesis of early NAFLD/NASH-associated HCC remain unclear. 
PAGE-B and modified PAGE-B (mPAGE-B) scores are developed to predict risk of hepatocellular carcinoma (HCC) in patients on nucleos(t)ide analogue therapy. However, how and when to use these risk scores in clinical practice is uncertain. Consecutive adult patients with chronic hepatitis B who had received entecavir or tenofovir for at least 6 months between January 2005 and June 2018 were identified from a territory-wide database in Hong Kong. Performance of PAGE-B and mPAGE-B scores on HCC prediction at 5 years was assessed by area under the time-dependent receiver operating characteristic curve (AUROC), and different cut-off values of these two scores were evaluated by survival analysis. Of 32,150 identified chronic hepatitis B patients, 20,868 (64.9%) were male. Their mean age was 53.0±13.2 years. At a median (interquartile range) follow-up of 3.9 (1.8–5.0) years, 1,532 (4.8%) patients developed HCC. The AUROC (95% confidence interval [CI]) of PAGE-B and mPAGE-B scores to predict HCC at 5 years was 0.77 (0.76–0.78) and 0.80 (0.79–0.81), respectively (P<0.001). 9,417 (29.3%) patients were classified as low HCC risk by either PAGE-B or mPAGE-B scores; their 5-year cumulative incidence (95% CI) of HCC was 0.6% (0.4%–0.8%). This classification achieved a negative predictive value (95% CI) of 99.5% (99.4%–99.7%) to exclude patients without HCC development in five years. The AUROC of PAGE-B and mPAGE-B scores at baseline and 2-year on-treatment to predict HCC were similar. PAGE-B and mPAGE-B scores can be applied to identify patients who have low risk of HCC development on antiviral therapy. These patients may be considered exemption from HCC surveillance due to their very low HCC risk. Chronic hepatitis B (CHB) is a major healthcare problem in Asia. Hepatocellular carcinoma (HCC) is a key complication and cause of mortality of CHB.[1] To prevent mortality related to HCC, high-risk patients should be identified and treated. With the introduction of nucleos(t)ide analogues (NA), potent viral suppression leads to fibrosis and cirrhosis regression, reduction of cirrhotic complications, and reduced liver-related mortality.[2] Although the risk of HCC is reduced by NA, this dreadful complication still exists particularly in cirrhotic patients.[3, 4] In a long-term follow-up study in Europe, new cases of HCC still developed even after more than 5 years of NA treatment.[5] A Korean report showed that the age-standardised death rate related to HCC only reduced modestly by 27% as compared to a 75% reduction in liver-related death between 1999-2013 despite extensive use of antiviral agents.[6] 
It is unclear if a reduction in hepatic fat content (HFC) is a major mediator of the cardiometabolic benefit of lifestyle intervention, and whether it has prognostic significance beyond the loss of visceral adipose tissue (VAT). In the present sub-study, we hypothesized that HFC loss in response to dietary interventions induces specific beneficial effects independently of VAT changes. In an 18-month weight-loss trial, 278 participants with abdominal obesity/dyslipidemia were randomized to low-fat (LF) or Mediterranean/low-carbohydrate (MED/LC + 28 g walnuts/day) diets with/without moderate physical activity. HFC and abdominal fat-depots were measured using magnetic resonance imaging at baseline, after 6 (sub-study, n = 158) and 18 months. Of 278 participants (mean HFC 10.2% [range: 0.01%–50.4%]), the retention rate was 86.3%. The %HFC substantially decreased after 6 months (−6.6% absolute units [−41% relatively]) and 18 months (−4.0% absolute units [−29% relatively]; p <0.001 vs. baseline). Reductions of HFC were associated with decreases in VAT beyond weight loss. After controlling for VAT loss, decreased %HFC remained independently associated with reductions in serum gamma glutamyltransferase and alanine aminotransferase, circulating chemerin, and glycated hemoglobin (p <0.05). While the reduction in HFC was similar between physical activity groups, MED/LC induced a greater %HFC decrease (p = 0.036) and greater improvements in cardiometabolic risk parameters (p <0.05) than the LF diet, even after controlling for VAT changes. Yet, the greater improvements in cardiometabolic risk parameters induced by MED/LC were all markedly attenuated when controlling for HFC changes. %HFC is substantially reduced by diet-induced moderate weight loss and is more effectively reduced by the MED/LC diet than the LF diet, independently of VAT changes. The beneficial effects of the MED/LC diet on specific cardiometabolic parameters appear to be mediated more by decreases in %HFC than VAT loss. Beyond total body fat content, fat distribution, both within adipose tissue depots and in ectopic fat deposits, is increasingly being shown to determine obesity-related health impact.1,2 Visceral adipose tissue (VAT), due to its unique anatomical location, releases free fatty acids (FFAs) and adipokines to the liver via the portal vein. Previous studies have demonstrated the inter-relationship between VAT and hepatic fat content (HFC), and indeed, increases in HFC were associated with similar metabolic abnormalities as observed for increases in VAT.3,4 In addition, reductions in VAT and HFC are increasingly thought to mediate the beneficial cardiometabolic outcomes of weight loss.1,5 Though closely associated with HFC, VAT and HFC may uniquely associate with specific effects and be linked independently with risk factors of cardiometabolic disease.6 Interestingly, data from recent studies found that HFC was more strongly associated with obesity’s metabolic complications than VAT,7 including the deterioration of glucose tolerance,8 possibly by mediating the link between obesity and metabolic dysfunction.9,10 Most recently, the decrease in HFC was associated with diabetes remission.11 
Systemic corticosteroids may cause HBV reactivation, but the impact on patients with previous HBV exposure is poorly defined. We aimed to study the risk of HBsAg seroreversion and hepatitis flare in patients with previous HBV exposure. Patients who were negative for HBsAg and received corticosteroids between 2001–2010 were included. Patients who were positive for antibody to HBsAg (anti-HBs) and/or to HBcAg (anti-HBc) were defined as having previous HBV exposure. The primary endpoint was HBsAg seroreversion; the secondary endpoint was hepatitis flare (alanine aminotransferase >80 U/L) at 1 year. A total of 12,997 patients fulfilled the inclusion criteria: anti-HBs positive only (n = 10,561); anti-HBc positive only (n = 970); anti-HBs & anti-HBc positive (n = 830) and anti-HBs & anti-HBc negative (n = 636). HBsAg seroreversion occurred in 165 patients. Patients who were anti-HBc positive only had a higher risk of HBsAg seroreversion (1-year incidence 1.8%) than those negative for both anti-HBs & anti-HBc (0%; p = 0.014). Patients with previous HBV exposure had a similarly low risk of liver failure as unexposed individuals (1.1% vs. 0.9%). The risk of a hepatitis flare started to increase in those receiving corticosteroids at peak daily doses of 20–40 mg (adjusted hazard ratio [HR] 2.19, p = 0.048) or >40 mg (aHR 2.11, p = 0.015) prednisolone equivalents for <7 days, and was increased at treatment durations of 7–28 days and >28 days (aHR 2.02–3.85; p <0.001–0.012). In HBsAg-negative patients who were only anti-HBc positive, high peak daily doses of corticosteroids increased the risk of hepatitis flare, but not seroreversion. The rate of liver failure was low and similar in HBV exposed and unexposed individuals; there were no deaths, nor any requirement for liver transplantation. Chronic HBV infection remains a considerable global health problem despite vaccination.1 Approximately 2 billion people of the world population have been infected; an estimated 257 million people are living with HBV infection (defined as HBsAg positive).2 HBsAg seroclearance, which may occur either spontaneously or after antiviral treatment,3–5 is currently regarded as the functional cure of chronic hepatitis B (CHB).6–8 Neither disappearance of HBsAg in patients with CHB nor recovery from a self-limiting acute hepatitis B guarantees lifelong protection. Some patients may suffer from occult hepatitis B infection (OBI), a status of undetectable serum HBsAg yet detectable serum and/or intrahepatic HBV DNA.9 Past or resolved HBV infection may still lead to HCC, cirrhotic complications and liver-related death.3–5 
Fatty acid translocase CD36 (CD36) is a membrane protein with multiple immuno-metabolic functions. Palmitoylation has been suggested to regulate the distribution and functions of CD36, but little is known about its significance in non-alcoholic steatohepatitis (NASH). Human liver tissue samples were obtained from patients undergoing liver biopsy for diagnostic purposes. CD36 knockout mice were injected with lentiviral vectors expressing wild-type CD36 or CD36 with mutated palmitoylation sites. Liver histology, immunofluorescence, mRNA expression profile, subcellular distributions and functions of CD36 protein were assessed. The localization of CD36 on the plasma membrane of hepatocytes was markedly increased in patients with NASH compared to patients with normal liver and those with simple steatosis. Increased CD36 palmitoylation and increased localization of CD36 on the plasma membrane of hepatocytes were also observed in livers of mice with NASH. Furthermore, inhibition of CD36 palmitoylation protected mice from developing NASH. The absence of palmitoylation decreased CD36 protein hydrophobicity reducing its localization on the plasma membrane as well as in lipid raft of hepatocytes. Consequently, a lack of palmitoylation decreased fatty acid uptake and CD36/Fyn/Lyn complex in HepG2 cells. Inhibition of CD36 palmitoylation not only ameliorated intracellular lipid accumulation via activation of the AMPK pathway, but also inhibited the inflammatory response through the inhibition of the JNK signaling pathway. Our findings demonstrate the key role of palmitoylation in regulating CD36 distributions and its functions in NASH. Inhibition of CD36 palmitoylation may represent an effective therapeutic strategy in patients with NASH. Non-alcoholic fatty liver disease (NAFLD) describes a range of conditions caused by the accumulation of fat in hepatocytes. Fifteen percent to 30% of the general population in both the Western world and Asia suffer from NAFLD.1,2 The prevalence is increased in type 2 diabetes mellitus (T2DM) (70%) and morbid obesity (90%).3 Non-alcoholic steatohepatitis (NASH) is a subset of NAFLD characterized by excessive fat accumulation in hepatocytes (steatosis) associated with liver tissue inflammation. Unlike steatosis alone (simple steatosis [SS]), which is generally considered benign and reversible, NASH may progress to fibrosis, cirrhosis and hepatocellular carcinoma.4,5 Approximately 2–3% of the global population is thought to have NASH, tending to rise rapidly along with the incidence of obesity, T2DM and the metabolic syndrome.6,7 The pathogenesis of NASH appears complex and it is still poorly understood, hence there are no specific therapeutic strategies for NASH. 
Non-alcoholic fatty liver disease (NAFLD) is a growing public health problem worldwide and has become an important field of biomedical inquiry. We aimed to determine whether European countries have mounted an adequate public health response to NAFLD and non-alcoholic steatohepatitis (NASH). In 2018 and 2019, NAFLD experts in 29 European countries completed an English-language survey on policies, guidelines, awareness, monitoring, diagnosis and clinical assessment in their country. The data were compiled, quality checked against existing official documents and reported descriptively. None of the 29 participating countries had written strategies or action plans for NAFLD. Two countries (7%) had mentions of NAFLD or NASH in related existing strategies (obesity and alcohol). Ten (34%) reported having national clinical guidelines specifically addressing NAFLD and, upon diagnosis, all included recommendations for the assessment of diabetes and liver cirrhosis. Eleven countries (38%) recommended screening for NAFLD in all patients with either diabetes, obesity and/or metabolic syndrome. Five countries (17%) had referral algorithms for follow-up and specialist referral in primary care, and 7 (24%) reported structured lifestyle programmes aimed at NAFLD. Seven (24%) had funded awareness campaigns that specifically included prevention of liver disease. Four countries (14%) reported having civil society groups which address NAFLD and 3 countries (10%) had national registries that include NAFLD. We found that a comprehensive public health response to NAFLD is lacking in the surveyed European countries. This includes policy in the form of a strategy, clinical guidelines, awareness campaigns, civil society involvement, and health systems organisation, including registries. Non-alcoholic fatty liver disease (NAFLD) is a growing challenge to global public health. It is defined as the increased accumulation of hepatic triglyceride (>5%) in the absence of excessive alcohol consumption or other causes of liver disease. The NAFLD spectrum encompasses steatosis (non-alcoholic fatty liver, NAFL) and non-alcoholic steatohepatitis (NASH), an inflammatory form of the condition marked by the presence of hepatocyte damage and progressive fibrosis that may lead to cirrhosis.1,2 Although NAFLD may occur in patients with normal weight, it is closely associated with the presence of the metabolic syndrome, and therefore with obesity, type 2 diabetes mellitus, hypertension and dyslipidaemia.3 The prevalence estimates of NAFLD vary widely according to the modality used to detect NAFLD and the geographical area.3,4 Most of the larger studies on NAFLD prevalence are based on ultrasonography,5 which is insensitive to modest increases in hepatic lipid accumulation at levels <30%, and do not employ diagnostic tools recommended by current guidance (e.g. transient elastography, NAFLD fibrosis score, magnetic resonance imaging, or the gold standard, liver biopsy).1 Nevertheless, a recent meta-analysis estimated the global prevalence of NAFLD to be 25%, with the highest estimates in the Middle East and South America (32% and 31%, respectively) and the lowest estimates in the African continent (14%); the estimates for Asia, the USA, and Europe were 27%, 24% and 23%, respectively.4 
Living-donor liver transplantation (LDLT) can simultaneously cure hepatocellular carcinoma (HCC) and underlying liver cirrhosis, improving long-term results in patients with HCC. ABO-incompatible LDLT could expand the living-donor pool, reduce waiting times for deceased-donor liver transplantation, and improve long-term survival for some patients with HCC. We retrospectively reviewed the medical records of patients undergoing LDLT for HCC from November 2008 to December 2015 at a single institution in Korea. In total, 165 patients underwent ABO-incompatible and 753 patients underwent ABO-compatible LDLT for HCC. ABO-incompatible recipients underwent desensitization to overcome the ABO blood group barrier, including pretransplant plasma exchange and rituximab administration (300–375 mg/m2 /body surface area). We performed 1:1 propensity score matching and included 165 patients in each group. 82.4% of ABO-incompatible and 83.0% of -compatible LDLT groups had HCC within conventional Milan criteria, respectively, and 92.1% and 92.7% of patients in each group had a Child-Pugh score of A or B. ABO-incompatible and -compatible LDLT groups were followed up for 48.0 and 48.7 months, respectively, with both groups showing comparable recurrence-free survival rates (hazard ratio [HR] 1.14; 95% CI 0.68–1.90; p = 0.630) and overall patient-survival outcomes (HR 1.10; 95% CI 0.60–2.00; p = 0.763). These findings suggested that ABO-incompatible liver transplantation is a feasible option for patients with HCC, especially for those with compensated cirrhosis with HCC within conventional Milan criteria. Although several potentially curative treatments for hepatocellular carcinoma (HCC) are known, such as liver resection and local ablation, HCC is difficult to manage, because the tumor mostly develops on a background of cirrhosis, and the limited functional reserves of the liver making their use difficult.1 In recent years, liver transplantation (LT) has generally been considered a feasible treatment capable of simultaneously curing HCC and the underlying liver cirrhosis, with living-donor liver transplantation (LDLT) performed on select patients with HCC as a practical alternative to deceased-donor liver transplantation (DDLT).2–4 However, many patients with HCC who may benefit from long-term survival through LT still do not have a chance of transplantation due to a lack of organs, especially in Asia where HCC incidence combined with chronic HBV- and HCV-related liver diseases is high.5 
MSDC-0602K is a novel insulin sensitizer designed to preferentially target the mitochondrial pyruvate carrier while minimizing direct binding to the transcriptional factor PPARγ. Herein, we aimed to assess the efficacy and safety of MSDC-0602K in patients with non-alcoholic steatohepatitis. Patients with biopsy-confirmed NASH and fibrosis (F1-F3) were randomized to daily oral placebo, or 1 of 3 MSDC-0602K doses in a 52-week double-blind study. The primary efficacy endpoint was hepatic histological improvement of ≥2 points in non-alcoholic fatty liver disease activity score (NAS) with a ≥1-point reduction in either ballooning or lobular inflammation and no increase in fibrosis stage at 12 months. Secondary endpoints included NAS improvement without worsening fibrosis, NASH resolution, and fibrosis reduction. Exploratory endpoints included changes in insulin sensitivity, liver injury and liver fibrosis markers. Patients were randomly assigned to placebo (n = 94), or 62.5 mg (n = 99), 125 mg (n = 98), or 250 mg (n = 101) of MSDC-0602K. At baseline, glycated hemoglobin was 6.4 ± 1.0%, 61.5% of patients had fibrosis F2/F3 and the average NAS was 5.3. The primary endpoint was reached in 29.7%, 29.8%, 32.9% and 39.5% of patients in the placebo, 62.5 mg, 125 mg and 250 mg dose arms, respectively, with adjusted odds ratios relative to placebo of 0.89 (95% CI 0.44–1.81), 1.22 (95% CI 0.60–2.48), and 1.64 (95% CI 0.83–3.27). The 2 highest doses of MSDC-0602K led to significant reductions in glucose, glycated hemoglobin, insulin, liver enzymes and NAS compared to placebo. The incidence of hypoglycemia and PPARγ-agonist-associated events such as edema and fractures were similar in the placebo and MSDC-0602K groups. MSDC-0602K did not demonstrate statistically significant effects on primary and secondary liver histology endpoints. However, effects on non-invasive measures of liver cell injury and glucose metabolism support further exploration of MSDC-0602K’s safety and potential efficacy in patients with type 2 diabetes and liver injury. [ ClinicalTrials.gov Identifier: NCT02784444]. Non-alcoholic fatty liver disease (NAFLD) and non-alcoholic steatohepatitis (NASH) are hepatic manifestations of metabolic syndrome and are increasing globally.1,2 Patients with NAFLD/NASH often present with either type 2 diabetes (T2D) or have insulin resistance and elevated fasting insulin levels, which are all established precursors for the development of diabetes and for adverse macrovascular sequalae.3–5 NASH, T2D, and insulin resistance are thought to be growing in incidence and severity because of the increased availability of nutrients and consequent overnutrition.2 
In recent years, circular RNAs (circRNAs) have been shown to have critical regulatory roles in cancer biology. However, the contributions of circRNAs to hepatocellular carcinoma (HCC) remain largely unknown. cSMARCA5 (a circRNA derived from exons 15 and 16 of the SMARCA5 gene, hsa_circ_0001445) was identified by RNA-sequencing and validated by quantitative reverse transcription PCR. The role of cSMARCA5 in HCC progression was assessed both in vitro and in vivo. circRNAs in vivo precipitation, luciferase reporter assay, biotin-coupled microRNA capture and fluorescence in situ hybridization were conducted to evaluate the interaction between cSMARCA5 and miR-17-3p/miR-181b-5p. The expression of cSMARCA5 was lower in HCC tissues, because of the regulation of DExH-Box Helicase 9, an abundant nuclear RNA helicase. The downregulation of cSMARCA5 in HCC was significantly correlated with aggressive characteristics and served as an independent risk factor for overall survival and recurrence-free survival in patients with HCC after hepatectomy. Our in vivo and in vitro data indicated that cSMARCA5 inhibits the proliferation and migration of HCC cells. Mechanistically, we found that cSMARCA5 could promote the expression of TIMP3, a well-known tumor suppressor, by sponging miR-17-3p and miR-181b-5p. These results reveal an important role of cSMARCA5 in the growth and metastasis of HCC and provide a fresh perspective on circRNAs in HCC progression. Hepatocellular carcinoma (HCC) is one of the most common malignancies worldwide.1 Although it has long been highly prevalent in Asia and Africa, it was relatively less common in the Western world. However, over the past three decades HCC incidence has doubled in the United Kingdom and tripled in the United States.2,3 Largely because of the propensity for metastasis, the five-year survival rate of patients with HCC remains poor, and approximately 600,000 patients die each year.2,4 Identifying prognostic biomarkers and treatment targets for metastatic HCC is of paramount importance. 
Non-alcoholic fatty liver disease (NAFLD) is associated with increased cardiovascular risk. Among categories of NAFLD, hepatic fibrosis is most likely to affect mortality. Myocardial function and its energy metabolism are tightly linked, which might be altered by an insulin resistant condition such as NAFLD. We investigated whether hepatic steatosis and fibrosis were associated with myocardial dysfunction relative to myocardial glucose uptake. A total of 308 patients (190 without NAFLD, 118 with NAFLD) were studied in a tertiary care hospital. Myocardial glucose uptake was evaluated at fasted state using [18F]-fluorodeoxyglucose-positron emission tomography (18FDG-PET). Hepatic steatosis and fibrosis were assessed by transient liver elastography (Fibroscan®) with controlled attenuation parameter, which quantifies hepatic fat and by surrogate indices (fatty liver index and NAFLD fibrosis score). Cardiac structure and function were examined by echocardiogram. Compared to those without NAFLD, patients with NAFLD had alterations in cardiac remodeling, manifested by increased left ventricular mass index, left ventricular end-diastolic diameter, and left atrial volume index (all p <0.05). Hepatic steatosis was significantly associated with left ventricular filling pressure (E/e’ ratio), which reflects diastolic dysfunction (p for trend <0.05). Those without NAFLD were more likely to have higher myocardial glucose uptake compared to those with NAFLD. Significant hepatic fibrosis was also correlated with diastolic dysfunction and impaired myocardial glucose uptake. Using multivariable linear regression, E/e’ ratio was independently associated with hepatic fibrosis (standardized β = 0.12 to 0.27; all p <0.05). Association between hepatic steatosis and E/e’ ratio was also significant (standardized β = 0.10 to 0.15; all p <0.05 excluding the model adjusted for adiposity). Hepatic steatosis and fibrosis are significantly associated with diastolic heart dysfunction. This association is linked with myocardial glucose uptake evaluated by 18FDG-PET. Non-alcoholic fatty liver disease (NAFLD) has become a common metabolic liver disease worldwide, with an estimated prevalence ranging from 25% to 45% in Asian as well as Western countries.1,2 NAFLD is defined as accumulation of lipids, mainly triglycerides, in ≥5% of hepatocytes with no evidence of excessive alcohol consumption or other secondary causes.3 The NAFLD spectrum ranges from simple steatosis, a benign disease with absence of hepatic inflammation and fibrosis, to non-alcoholic steatohepatitis (NASH), an aggressive condition that can develop into cirrhosis, hepatocellular carcinoma, and liver-related mortality.4,5 A recent meta-analysis reported that in patients with NASH, 35% progressed to cirrhosis in an average of seven years, often followed by liver-related complications.6 
Direct-acting antiviral therapies (DAA) are an important tool for hepatitis C virus (HCV) elimination. However, reinfection among people who inject drugs (PWID) may hamper elimination targets. Therefore, we estimated HCV reinfection rates among DAA-treated individuals, including PWID. We analyzed data from the British Columbia Hepatitis Testers Cohort which included ∼1.7 million individuals screened for HCV in British Columbia, Canada. We followed HCV-infected individuals treated with DAAs who achieved a sustained virologic response (SVR) and had ≥1 subsequent HCV RNA measurement to April 22nd, 2018. Reinfection was defined as a positive RNA measurement after SVR. PWID were identified using a validated algorithm and classified based on recent (<3 years) or former (≥3 years before SVR) use. Crude reinfection rates per 100 person-years (PYs) were calculated. Poisson regression was used to model adjusted incidence rate ratios (IRRs) and 95% CIs. Of 4,114 individuals who met the inclusion criteria, most were male (n = 2,692, 65%), born before 1965 (n = 3,411, 83%) and were either recent (n = 875, 21%) or former PWID (n = 1,793, 44%). Opioid-agonist therapy (OAT) was received by 19% of PWID. We identified 40 reinfections during 2,767 PYs. Reinfection rates were higher among recent (3.1/100 PYs; IRR 6.7; 95% CI 1.9–23.5) and former PWID (1.4/100 PYs; IRR 3.7; 95% CI 1.1–12.9) than non-PWID (0.3/100 PYs). Among recent PWID, reinfection rates were higher among individuals born after 1975 (10.2/100 PYs) and those co-infected with HIV (5.7/100 PYs). Only one PWID receiving daily OAT developed reinfection. Population-level reinfection rates remain elevated after DAA therapy among PWID because of ongoing exposure risk. Engagement of PWID in harm-reduction and support services is needed to prevent reinfections. Chronic hepatitis C virus (HCV) is associated with substantial morbidity and mortality, with >71 million people infected worldwide in 2015.1 In 2013, an estimated 700,000 deaths globally were attributed to HCV-related liver disease sequelae, namely cirrhosis and hepatocellular carcinoma.2 Mortality is expected to increase as many individuals infected with HCV decades ago are aging and at high risk of chronic liver disease.3,4 With the introduction of highly effective and well-tolerated, all-oral direct-acting antiviral (DAA) therapies to treat chronic HCV infection,5,6 there has been renewed optimism regarding the ability to reduce the HCV disease burden and improve treatment outcomes in traditionally difficult to treat or cure populations. This includes those co-infected with HIV,7 as well as people who inject drugs (PWID).8 
Sorafenib is the standard of care for advanced hepatocellular carcinoma (HCC). Combining sorafenib with another treatment, to improve overall survival (OS) within an acceptable safety profile, might be the next step forward in the management of patients with advanced HCC. We aimed to assess whether a combination of sorafenib and a statin improved survival in patients with HCC. The objective of the PRODIGE-11 trial was to compare the respective clinical outcomes of the sorafenib-pravastatin combination (arm A) versus sorafenib alone (arm B) in patients with advanced HCC. Child-Pugh A patients with advanced HCC who were naive to systemic treatment (n = 323) were randomly assigned to sorafenib-pravastatin combination (n = 162) or sorafenib alone (n = 161). The primary endpoint was OS; secondary endpoints were progression-free survival, time to tumor progression, time to treatment failure, safety, and quality of life. After randomization, 312 patients received at least 1 dose of study treatment. After a median follow-up of 35 months, 269 patients died (arm A: 135; arm B: 134) with no difference in median OS between treatments arms (10.7 months vs. 10.5 months; hazard ratio = 1.00; p = 0.975); no difference was observed in secondary survival endpoints either. In the univariate analysis, the significant prognostic factors for OS were CLIP score, performance status, and quality of life scores. The multivariate analysis showed that the only prognostic factor for OS was the CLIP score. The main toxicity was diarrhea (which was severe in 11% of patients in arm A, and 8.9% in arm B), while severe nausea-vomiting was rare, and no toxicity-related deaths were reported. Adding pravastatin to sorafenib did not improve survival in patients with advanced HCC. Hepatocellular carcinoma (HCC) incidence is still growing in Western countries.1,2 Tumor development is frequently related to liver damage, typically caused by cirrhosis. While curative treatments (transplantation, resection, percutaneous destruction) can be proposed for small tumors, about two-thirds of patients are not eligible for such treatments.3 
Two major body compartments, skeletal muscle and adipose tissue, exhibit independent functions. We aimed to explore the prognostic significance of skeletal muscle, visceral and subcutaneous adipose tissue, according to sex, in patients with cirrhosis assessed for liver transplantation (LT). CT images taken at the 3rd lumbar vertebra from 677 patients were quantified for three body composition indexes (cm2/m2), visceral adipose tissue index, subcutaneous adipose tissue index (SATI), and skeletal muscle index (SMI). Cox proportional and competing-risk analysis hazard models were conducted to assess associations between mortality and body composition. The majority of patients were male (67%) with a mean age of 57 ± 7 years, model for end-stage liver disease (MELD) score of 14 ± 8 and mean body mass index of 27 ± 6 kg/m2. Despite similar body mass index between the sexes, male patients had greater SMI (53 ± 12 vs. 45 ± 9 cm2/m2), whereas SATI (67 ± 52 vs. 48 ± 37 cm2/m2) was higher in females (p <0.001 for each). In sex stratified multivariate analyses after adjustment for MELD score and other confounding variables, SATI in females (hazard ratio [HR] 0.99; 95% CI 0.98–1.00; p = 0.01) and SMI in males (HR 0.98; 95% CI 0.96–1.00; p = 0.02) were significant predictors of mortality. Female patients with low SATI (<60 cm2/m2) had a higher risk of mortality (HR 2.06; 95% CI 1.08–3.91; p = 0.03). Using competitive risk analysis in female patients listed for LT, low SATI was also an independent predictor of mortality (subdistribution HR 2.80; 95% CI 1.28–6.12; p = 0.01) after adjusting for MELD, and other confounding factors. A lower SATI is associated with higher mortality in female patients with cirrhosis. Subcutaneous adipose tissue has a favorable metabolic profile – low SATI may reflect depletion of this major energy reservoir, leading to poor clinical outcomes. Differences in body composition between the sexes exist in healthy subjects,1,2 and populations with cancer3 and cirrhosis,4–6 with females having higher adipose tissue mass (adiposity) and males having greater muscularity. Body composition of people with cirrhosis has been assessed using approaches such as air displacement plethysmography, bioelectrical impedance analysis, and dual-energy X-ray absorptiometry.4,7 However, the ability of these techniques to differentiate between two major body compartments, muscle and adipose tissue, as well as their capability to provide a specific measure of adipose tissue depots (i.e., visceral, subcutaneous) is limited. Moreover, their accuracy might also be affected by changes in fluid homeostasis, which is a frequent complication in cirrhosis.7 Computed tomography (CT) image analysis has emerged as a specific and precise method for body composition assessment, that enables quantification of muscle and different adipose tissue depots.8 In the clinical setting, CT is frequently requested for patients with cirrhosis as part of liver transplantation (LT) assessment, in order to pre-operatively map the vascular and biliary anatomy and screen for hepatocellular carcinoma (HCC). 
Cure rates in response to retreatment with sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) are high, but this regimen has not been studied in patients with a history of poor adherence or treatment interruption, nor in patients with HIV/HCV coinfection. Herein, we aimed to assess the safety and efficacy of this combination in patients with genotype 1 HCV infection who had relapsed following combination direct-acting antiviral (DAA) therapy, regardless of HIV infection or previous treatment course. The RESOLVE study was a multicenter, open-label, phase IIb study investigating the safety, tolerability and efficacy of SOF/VEL/VOX in 77 patients with virologic rebound following combination DAA therapy. Efficacy was defined as HCV RNA below the lower limit of detection 12 weeks after the end of treatment (SVR12), while safety endpoints included the incidence of grade 3 and 4 adverse events (AEs) following treatment, and the proportion of patients who stopped treatment prematurely due to AEs. In an intent-to-treat analysis, 70/77 (90.9%, 95% CI 82.1–95.8%) patients achieved SVR12, including 14/17 (82.4%) HIV coinfected participants and 18/22 (81.8%) of those with previous non-completion of DAA therapy. In an analysis of all patients who completed 12 weeks of study medication, 70/71 patients (99%) achieved SVR12. One patient experienced a grade 3 AE, and 4 experienced a grade 4 AE, all unrelated to study participation. Reported AEs were similar in HIV-coinfected patients, and patients receiving dolutegravir-based antiretroviral treatment experienced no clinically significant increases in aminotransferases. Retreatment with 12 weeks of SOF/VEL/VOX was safe and effective in patients with relapsed HCV following initial combination DAA-based treatment. Treatment response was not affected by HIV coinfection or previous treatment course. Treatment of chronic HCV infection has changed dramatically in the past 5 years. Combinations of direct-acting antivirals (DAAs) can achieve high rates of sustained virologic response (SVR), synonymous with eradication or cure of HCV, of above 95% in clinical trials, regardless of stage of hepatic fibrosis, treatment history, or HCV genotype.1 Recently, real world efficacy studies have confirmed high cure rates, with SVR rates of 90–95% outside of clinical trials.2–4 The proportion of patients in these reports who do not achieve SVR is low, at less than 10%, when combining those who have virologic relapse, are lost to follow-up, or discontinue treatment due to adverse events (AEs). Nevertheless, given that there are an estimated 71 million people with chronic HCV worldwide,5 the number of patients who will require HCV retreatment is substantial and will increase as HCV treatment becomes more accessible. 
Tenofovir disoproxil fumarate (TDF) is one the most potent nucleot(s)ide analogues for treating chronic hepatitis B virus (HBV) infection. Phenotypic resistance caused by genotypic resistance to TDF has not been reported. This study aimed to characterize HBV mutations that confer tenofovir resistance. Two patients with viral breakthrough during treatment with TDF-containing regimens were prospectively enrolled. The gene encoding HBV reverse transcriptase was sequenced. Eleven HBV clones harboring a series of mutations in the reverse transcriptase gene were constructed by site-directed mutagenesis. Drug susceptibility of each clone was determined by Southern blot analysis and real-time PCR. The relative frequency of mutants was evaluated by ultra-deep sequencing and clonal analysis. Five mutations (rtS106C [C], rtH126Y [Y], rtD134E [E], rtM204I/V, and rtL269I [I]) were commonly found in viral isolates from 2 patients. The novel mutations C, Y, and E were associated with drug resistance. In assays for drug susceptibility, the IC50 value for wild-type HBV was 3.8 ± 0.6 µM, whereas the IC50 values for CYE and CYEI mutants were 14.1 ± 1.8 and 58.1 ± 0.9 µM, respectively. The IC90 value for wild-type HBV was 30 ± 0.5 µM, whereas the IC90 values for CYE and CYEI mutants were 185 ± 0.5 and 790 ± 0.2 µM, respectively. Both tenofovir-resistant mutants and wild-type HBV had similar susceptibility to the capsid assembly modulator NVR 3–778 (IC50 <0.4 µM vs. IC50 = 0.4 µM, respectively). Our study reveals that the quadruple (CYEI) mutation increases the amount of tenofovir required to inhibit HBV by 15.3-fold in IC50 and 26.3-fold in IC90. These results demonstrate that tenofovir-resistant HBV mutants can emerge, although the genetic barrier is high. Worldwide, an estimated 2 billion people are infected with hepatitis B virus (HBV), and 686,000 people die from complications due to HBV each year.1 Despite the development of nucleos(t)ide analogue (NA) drugs, antiviral therapy of HBV infection remains a major clinical issue. Due to both the viral persistence and heterogeneity of the HBV genome, the emergence of drug-resistant mutants is inevitable. The development of drug resistance is associated with poor prognosis. Problems arising from drug resistance include hepatitis flares, reversion of histologic improvement, and sometimes severe exacerbation of illness, hepatic decompensation, or death.2 
Recently, the PAGE-B score and Toronto HCC risk index (THRI) have been developed to predict the risk of hepatocellular carcinoma (HCC) in Caucasian patients with chronic hepatitis B (CHB). We aimed to validate PAGE-B scores and THRI in Asian patients with CHB and suggested modified PAGE-B scores to improve the predictive performance. From 2007 to 2017, we examined 3,001 Asian patients with CHB receiving entecavir/tenofovir therapy. We assessed the performances of PAGE-B, THRI, CU-HCC, GAG-HCC, and REACH-B for HCC development. A modified PAGE-B score (mPAGE-B) was developed (derivation set, n = 2,001) based on multivariable Cox models. Bootstrap for internal validation and external validation (validation set, n = 1,000) were performed. The five-year cumulative HCC incidence rates were 6.6% and 7.2% in the derivation and validation datasets after entecavir/tenofovir onset. In the derivation dataset, age, gender, serum albumin levels, and platelet counts were independently associated with HCC. The mPAGE-B score was developed based on age, gender, platelet counts, and serum albumin levels (time-dependent area under receiver operating characteristic curves [AUROC] = 0.82). In the validation set, the PAGE-B and THRI showed similar AUROCs to CU-HCC, GAG-HCC, and REACH-B at five years (0.72 and 0.73 vs. 0.70, 0.71, and 0.61 respectively; all p >0.05 except REACH-B), whereas the AUROC of mPAGE-B at five years was 0.82, significantly higher than the five other models (all p <0.01). HCC incidence rates after initiation of entecavir/tenofovir therapy in patients with CHB were significantly decreased in all risk groups in long-term follow-up periods. Although PAGE-B and THRI are applicable in Asian patients with CHB receiving entecavir/tenofovir therapy, mPAGE-B scores including additional serum albumin levels showed better predictive performance than the PAGE-B score. Approximately 240 million people are diagnosed with chronic hepatitis B (CHB) worldwide, and the highest prevalence is found in Africa and Asia.1 Early diagnosis of hepatocellular carcinoma (HCC) improves patients’ prognosis through timely intervention,2 and it is therefore important to stratify the risk of HCC in patients with CHB. 
The fixed-dose combination of sofosbuvir/velpatasvir was highly efficacious in patients infected with genotype (GT)1–6 hepatitis C virus (HCV) in the ASTRAL studies. This analysis evaluated the impact of baseline resistance-associated substitutions (RASs) on treatment outcome and emergence of RASs in patients infected with HCV GT1-6 who were treated with sofosbuvir/velpatasvir. Non-structural protein 5A and 5B (NS5A and NS5B) deep sequencing was performed at baseline and at the time of relapse for all patients treated with sofosbuvir/velpatasvir for 12 weeks (n = 1,778) in the ASTRAL-1–3, ASTRAL-5 and POLARIS-2–3 studies. Patients with 37 known and 19 novel HCV subtypes were included in these analyses. Overall, 28% (range 9% to 61% depending on genotype) had detectable NS5A class RASs at baseline, using a 15% sequencing assay cut-off. There was no significant effect of baseline NS5A class RASs on sustained virologic response at week 12 (SVR12) with sofosbuvir/velpatasvir; the SVR12 rate in the presence of NS5A class RASs was 100% and 97%, in patients with GT1a and GT1b infection, respectively, and 100% in patients with GT2 and GT4–6 infections. In GT3 infection, the SVR rate was 93% and 98% in patients with and without baseline NS5A class RASs, respectively. The overall virologic failure rate was low (20/1,778 = 1.1%) in patients treated with sofosbuvir/velpatasvir. Single NS5A class resistance was observed at virologic failure in 17 of the 20 patients. Sofosbuvir/velpatasvir taken for 12 weeks once daily resulted in high SVR rates in patients infected with GT1–6 HCV, irrespective of baseline NS5A RASs. NS5A inhibitor resistance, but not sofosbuvir resistance, was detected in the few patients with virologic failure. These data highlight the high barrier to resistance of this regimen for the treatment of chronic HCV across all genotypes in the vast majority of patients. Hepatitis C virus (HCV) infection is a global health burden with an estimated 71–150 million individuals infected worldwide.1–3 Development of direct-acting antivirals (DAAs) in recent years has dramatically enhanced sustained virologic response (SVR) rates in HCV genotype (GT)1 chronically infected patients.4 
Imaging characteristics for discriminating the malignant potential of intraductal papillary neoplasm of the bile duct (IPNB) still remain unclear. This study aimed to define the magnetic resonance (MR) imaging findings that help to differentiate IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia and to investigate their significance with respect to long-term outcomes in patients with surgically resected IPNB. This retrospective study included 120 patients with surgically resected IPNB who underwent preoperative MR imaging with MR cholangiography before surgery from January 2008 and December 2017 in two tertiary referral centers. Clinical and MR imaging features of IPNB with intraepithelial neoplasia (n = 34) and IPNB with an associated invasive carcinoma (n = 86) were compared. Regarding significant features for discriminating IPNB with or without an associated invasive carcinoma, recurrence-free survival (RFS) rates were evaluated. Significant MR imaging findings for differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia were intraductal visible mass, tumor size ≥2.5 cm, multiplicity of the tumor, bile duct wall thickening, and adjacent organ invasion (all p ≤0.002). The 1-, 3-, and 5-year RFS rates for surgically resected IPNB were 93.8%, 79.1%, and 70.0%, respectively. RFS rates were significantly lower in patients with each significant MR imaging finding of IPNB with an associated invasive carcinoma than in those without significant MR imaging findings (all p ≤0.039). MR imaging with MR cholangiography may be helpful in differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia. Significant MR imaging findings of IPNB with an associated invasive carcinoma have a negative impact on RFS. Intraductal papillary neoplasm of the bile duct (IPNB) is characterized by a papillary or villous biliary neoplasm covering delicate fibrovascular stalks and a histological spectrum ranging from benign disease to invasive malignancy.1,2 According to the 2010 World Health Organization classification, IPNB can be classified into the following 3 histologic grades: IPNB with low- or intermediate-grade intraepithelial neoplasia, IPNB with high-grade intraepithelial neoplasia, and IPNB with an associated invasive carcinoma. IPNB with low- or intermediate- and high-grade intraepithelial neoplasia are regarded as premalignant and non-invasive IPNB, whereas IPNB with an associated invasive carcinoma is considered malignant and invasive IPNB. 
Proof-of-concept studies frequently assess changes in intrahepatic triglyceride (IHTG) content by magnetic resonance-based techniques as a surrogate marker of histology. The aim of this study was to establish how reliable this strategy is to predict changes in liver histology in patients with non-alcoholic steatohepatitis (NASH). Patients with NASH who had participated in our prior randomized controlled trials of pioglitazone with complete paired data for IHTG content by magnetic resonance spectroscopy and liver histology were included in the study. A total of 121 patients were included. Changes in IHTG were assessed in several ways: as a continuous variable (correlations), as categorical groups (IHTG change ≥0%; or IHTG reduction of 1–30%; 31–50%; 51–70%; or >70%), and in a binomial way as steatosis resolution or not (defined as achieving IHTG <5.56%). Changes in IHTG correlated with steatosis on histology (r = 0.54; p <0.01). However, the magnitude of IHTG reduction was not associated with the rate of response of the primary histological outcome (2-point improvement in the NAFLD activity score from 2 different parameters, without worsening of fibrosis) or resolution of NASH without worsening of fibrosis, neither in patients receiving pioglitazone nor placebo. Changes in lobular inflammation, hepatocyte ballooning, or liver fibrosis were also independent of changes in IHTG, irrespective of treatment arm. Steatosis resolution was not associated with better histological outcomes either. Changes in IHTG predict changes in steatosis but not of other liver histological parameters. This implies that IHTG response to treatment should be interpreted with caution, as it may not be as reliable as previously believed to predict a treatment’s overall clinical efficacy in patients with NASH. In recent years, there has been a growing interest in the discovery of new pharmacological agents for the treatment of non-alcoholic fatty liver disease (NAFLD). This recent interest responds to an alarming rise in its prevalence and, more importantly, to the associated risk of progression to end-stage liver disease.1 Several randomized, controlled trials have recently been completed,2–8 and many more are still ongoing.9 
The aim of this study was to compare the performance of MRIs with extracellular contrast agents (ECA-MRI) to HB contrast agents (HBA-MRI) for the non-invasive diagnosis of small HCCs in a head-to-head comparison. Between August 2014 and October 2017; 171 cirrhotic patients, each with 1 to 3 nodules measuring 1 to 3 cm, were included across eight centers. All patients had both an ECA-MRI and an HBA-MRI within a month. The non-invasive diagnosis of HCC was made when the nodule was hyper-enhanced at the arterial phase (HA) with wash-out at the portal phase (PP) and/or delayed phase (DP) for the ECA-MRI, or PP and/or HB phase (HBP) for the HBA-MRI. The gold standard was defined by a composite algorithm previously published. 225 nodules, of which 153 were HCCs and 72 were not HCCs, were included. Both MRI sensitivities were similar (71.2% [63.4-78.3]). Specificity was 83.3% [72.7-91.1] for the ECA-MRI and 68.1% [56.0-78.6] for the HBA-MRI. With regard to HCCs, on ECA-MRI, 138 were HA, 84 had wash-out at PP and 104 at DP; on HBA-MRI, 128 were HA, 71 had wash-out at PP and 99 at HBP. For nodules from 2 to 3 cm, sensitivity and specificity were similar with 70.9% [57.1-82.4] and 75.0% [47.6-92.7] respectively. For nodules from 1 to 2 cm, specificity dropped to 66.1% [52.2-78.2] for the HBA-MRI vs. 85.7% [73.8-93.6] for the ECA-MRI. HBA-MRI specificity is lower than ECA-MRI for diagnosing small HCCs on cirrhotic patients. These results raise the question of the proper use of HBA-MRI in algorithms for the non-invasive diagnosis of small HCCs. Liver cancer is the second most frequent cause of cancer-related deaths worldwide and hepatocellular carcinoma (HCC) represents about 90% of primary liver cancers [1]. The concept of imaging-based non-invasive diagnosis of HCC is part of both Western and Asian guidelines [1–5]. In Western guidelines, the diagnosis can be made in cirrhotic patients for nodules ≥ 1cm showing typical hallmarks on contrast-enhanced multiphasic imaging (computer tomography (CT) or magnetic resonance imaging (MRI)) [1]. 
Programmed cell death 1 ligand 1 (PD-L1) expression on antigen-presenting cells is essential for T cell impairment, and PD-L1-expressing macrophages may mechanistically shape and therapeutically predict the clinical efficacy of PD-L1 or programmed cell death 1 blockade. We aimed to elucidate the mechanisms underlying PD-L1 upregulation in human tumor microenvironments, which remain poorly understood despite the clinical success of immune checkpoint inhibitors. Monocytes/macrophages were purified from peripheral blood, non-tumor, or paired tumor tissues of patients with hepatocellular carcinoma (HCC), and their possible glycolytic switch was evaluated. The underlying regulatory mechanisms and clinical significance of metabolic switching were studied with both ex vivo analyses and in vitro experiments. We found that monocytes significantly enhanced the levels of glycolysis at the peritumoral region of human HCC. The activation of glycolysis induced PD-L1 expression on these cells and subsequently attenuated cytotoxic T lymphocyte responses in tumor tissues. Mechanistically, tumor-derived soluble factors, including hyaluronan fragments, induced the upregulation of a key glycolytic enzyme, PFKFB3, in tumor-associated monocytes. This enzyme not only modulated the cellular metabolic switch but also mediated the increased expression of PD-L1 by activating the nuclear factor kappa B signaling pathway in these cells. Consistently, the levels of PFKFB3+CD68+ cell infiltration in peritumoral tissues were negatively correlated with overall survival and could serve as an independent prognostic factor for survival in patients with HCC. Our results reveal a mechanism by which the cellular metabolic switch regulates the pro-tumor functions of monocytes in a specific human tumor microenvironment. PFKFB3 in both cancer cells and tumor-associated monocytes is a potential therapeutic target in human HCC. Monocytes/macrophages constitute a major component of most solid tumors and exhibit great plasticity and diversity according to different environmental cues.1–3 Instead of inducing anti-tumor immune responses, monocytes/macrophages can be educated by the tumor microenvironment and facilitate disease progression via diverse mechanisms.4,5 For example, we and others have found that the peritumoral stroma of human hepatocellular carcinoma (HCC) is highly infiltrated by monocytes with activated phenotypes.6,7 These activated monocytes attenuate the T cell response by expressing programmed cell death 1 ligand 1 (PD-L1) but retain their proinflammatory properties to induce angiogenesis and tissue remodeling by inducing interleukin (IL)-17A-producing cell expansion and neutrophil recruitment, thus rerouting the inflammatory response in a tumor-promoting direction.8,9 The specific phenotype and functions of these tumor-infiltrating monocytes/macrophages are generally thought to be induced and maintained by local environmental factors, but the underlying mechanisms are still not well understood. 
In Europe, hepatitis C virus (HCV) screening still targets people at high risk of infection. We aim to determine the cost-effectiveness of expanded HCV screening in France. A Markov model simulated chronic hepatitis C (CHC) prevalence, incidence of events, quality-adjusted life years (QALYs), costs and incremental cost-effectiveness ratio (ICER) in the French general population, aged 18 to 80 years, undiagnosed for CHC for different strategies: S1 = current strategy targeting the at risk population; S2 = S1 and all men between 18 and 59 years; S3 = S1 and all individuals between 40 and 59 years; S4 = S1 and all individuals between 40 and 80 years; S5 = all individuals between 18 and 80 years (universal screening). Once CHC was diagnosed, treatment was initiated either to patients with fibrosis stage ≥F2 or regardless of fibrosis. Data were extracted from published literature, a national prevalence survey, and a previously published mathematical model. ICER were interpreted based on one or three times French GDP per capita (€32,800). Universal screening led to the lowest prevalence of CHC and incidence of events, regardless of treatment initiation. When considering treatment initiation to patients with fibrosis ≥F2, targeting all people aged 40–80 was the only cost-effective strategy at both thresholds (€26,100/QALY). When we considered treatment for all, although universal screening of all individuals aged 18–80 is associated with the highest costs, it is more effective than targeting all people aged 40–80, and cost-effective at both thresholds (€31,100/QALY). In France, universal screening is the most effective screening strategy for HCV. Universal screening is cost-effective when treatment is initiated regardless of fibrosis stage. From an individual and especially from a societal perspective of HCV eradication, this strategy should be implemented. In Europe, recommendations for hepatitis C virus (HCV) screening still target only people at high risk of infection.1 With this screening strategy, around 40% of infections are still not detected in France,2 equating to 75,000 individuals in 2014.3 
The efficacy of direct-acting antivirals (DAAs) has dramatically changed the prognosis of patients with chronic hepatitis C. We aimed to evaluate the impact of DAA therapy on the composition of the liver transplant (LT) waiting list and the early post-transplant survival. We evaluated all patients admitted to the waiting list for a primary LT between 1st January 2008 and 31st of December 2016 in Catalonia, Spain. Time span was divided into two periods according to the availability of different antiviral therapies: 2008–2013 (interferon-based therapies) and 2014–2016 (DAA). Changes in the indications of LT and the aetiology of liver disease, as well as post-LT patient survival, were evaluated according to the year of inclusion and transplantation, respectively. We included 1,483 patients. Admissions in the waiting list for hepatitis C virus (HCV)-related liver disease decreased significantly, from 47% in 2008–2013 to 35% in 2014–2016 (p <0.001), particularly because of a reduction in patients with decompensated cirrhosis. In contrast, NASH-related inclusions increased from 4% to 7% (p = 0.003). Three-year post-LT patient survival increased significantly in the second period in the whole cohort (82% vs. 91%, p = 0.002), because of better survival in anti-HCV positive patients (76% vs. 91%, p = 0.001), but not in anti-HCV negative patients (88% vs. 91% p = 0.359). Anti-HCV positive serology, the time period of 2008–2013 and higher donor age were independently associated with post-LT mortality in the whole cohort; while time period and donor age were independently associated with post-LT mortality in anti-HCV positive recipients. The high efficacy of DAAs is associated with significant changes in the composition of the LT waiting list and, more importantly, results in improved post-transplant survival. Hepatitis C virus (HCV) infection is the main cause of end-stage liver disease leading to liver transplantation (LT) in the Western world, and until the widespread use of the direct-acting antiviral (DAA) regimens it had a significant detrimental impact on post-transplant patient and graft survival.1–4 However, the availability of DAAs, harbouring excellent efficacy and tolerability, has represented a dramatic change in the prognosis of patients with HCV infection, as sustained virological response (SVR) rates are around 95% even in LT patients and in the range of 85–90% in patients with decompensated cirrhosis.5–11 The impact of viral eradication on patient survival and the risk of hepatocellular carcinoma (HCC) has been demonstrated in studies both in the interferon (IFN) and DAA eras.12–14 However, the possibility of using DAAs in patients with more advanced liver disease may lead to significant changes in the LT waiting list and in early post-LT survival.15 In this regard, two registry and population-based studies from the US have shown a significant decrease in the proportion of patients with HCV-related liver disease that are included on the waiting list, particularly with the indication of decompensated cirrhosis, accompanied by a progressive increase in patients being put on the waiting list because of non-alcoholic steatohepatitis (NASH) and alcoholic liver disease.16,17 In addition, it has been demonstrated in other studies that approximately 20% of HCV-infected patients with decompensated cirrhosis who achieved SVR on the waiting list could be delisted because of clinical improvement.18,19 In a recent study, a significant improvement in post-LT survival in the period 2011–2014 was seen compared to 2004–2010.20 In this study, such improvement was also seen in patients who underwent LT for other aetiologies of liver disease, although the magnitude of improvement seemed to be higher in patients with HCV-related liver disease. In this manuscript, we aimed to evaluate the efficacy of DAA on the composition of the LT waiting list and particularly on early post-LT survival. SECTION Patients and methods 
There have been calls to integrate HCV testing into existing services, including harm reduction and HIV prevention and treatment, but there are few empirical trials to date. We evaluated the impact of integrating HCV testing/education into integrated care centers (ICCs) delivering HIV services to people who inject drugs (PWID) across India, using a cluster-randomized trial. We compared ICCs with usual care in the PWID stratum (12 sites) of a 22-site cluster-randomized trial. In 6 sites, ICCs delivering HIV testing, harm reduction, other preventive services and linkage to HIV treatment were scaled from opioid agonist therapy centers and operated for 2 years. On-site rapid HCV antibody testing was integrated after 1 year. To assess impact, we conducted baseline and evaluation surveys using respondent-driven sampling (RDS) across the 12 sites (n = 11,993 recruited at baseline; n = 11,721 recruited at evaluation). The primary outcome was population-level self-reported HCV testing history. At evaluation, HCV antibody prevalence ranged from 7.2–76.6%. Across 6 ICCs, 5,263 ICC clients underwent HCV testing, of whom 2,278 were newly diagnosed. At evaluation, PWID in ICC clusters were 4-fold more likely to report being tested for HCV than in usual care clusters, adjusting for baseline testing (adjusted prevalence ratio [aPR] 3.69; 95% CI 1.34–10.2). PWID in ICC clusters were also 7-fold more likely to be aware of their HCV status (aPR 7.11; 95% CI 1.14–44.3) and significantly more likely to initiate treatment (aPR 9.86; 95% CI 1.52–63.8). We provide among the first empirical data supporting the integration of HCV testing into HIV/harm reduction services. To achieve elimination targets, programs will need to scale-up such venues to deliver comprehensive HCV services. An estimated 71 million people are chronically infected with HCV.1 The availability of safe, short duration, curative therapies2–4 prompted the World Health Organization (WHO) to release targets for HCV elimination – 80% reduction in incidence and 65% reduction in mortality by 2030.5 Achieving these targets requires 80% of all people with active infection to be treated. Thus, it is essential that major inroads are made in people who inject drugs (PWID) in low- and middle-income countries (LMICs). In these settings, awareness of HCV status is well below 10% and most have not even received the most basic HCV education.6 
High red and processed meat consumption is related to type 2 diabetes. In addition, cooking meat at high temperatures for a long duration forms heterocyclic amines (HCAs), which are related to oxidative stress. However, the association between meat consumption and non-alcoholic fatty liver disease (NAFLD) is yet to be thoroughly tested. Therefore, we aimed to test the association of meat type and cooking method with NAFLD and insulin resistance (IR). This was a cross-sectional study in individuals who were 40–70 years old and underwent screening colonoscopy between 2013 and 2015 in a single center in Israel. NAFLD and IR were evaluated by ultrasonography and homeostasis model assessment. Meat type and cooking method were measured by a food frequency questionnaire (FFQ) and a detailed meat questionnaire. Unhealthy cooking methods were considered as frying and grilling to a level of well done and very well done. Dietary HCA intake was calculated. A total of 789 individuals had a valid FFQ and 357 had a valid meat questionnaire. High consumption of total meat (portions/day above the median) (odds ratio [OR] 1.49; 95% CI 1.05–2.13; p = 0.028; OR 1.63; 1.12–2.37; p = 0.011), red and/or processed meat (OR 1.47; 95% CI 1.04–2.09; p = 0.031; OR 1.55; 1.07–2.23; p = 0.020) was independently associated with higher odds of NAFLD and IR, respectively, when adjusted for: body mass index, physical activity, smoking, alcohol, energy, saturated fat and cholesterol intake. High intake of meat cooked using unhealthy methods (OR 1.92; 95% CI 1.12–3.30; p = 0.018) and HCAs (OR 2.22; 95% CI 1.28–3.86; p = 0.005) were independently associated with higher odds of IR. High consumption of red and/or processed meat is associated with both NAFLD and IR. High HCA intake is associated with IR. If confirmed in prospective studies, limiting the consumption of unhealthy meat types and improving preparation methods may be considered as part of NAFLD lifestyle treatment. Non-alcoholic Fatty liver disease (NAFLD) is becoming a major global health burden in both developed and developing countries.1 NAFLD is considered as the hepatic component of the metabolic syndrome, with insulin resistance (IR) as the key factor in its pathophysiology.2 Unhealthy Western lifestyle plays a major role in the development and progression of NAFLD,3 namely, lack of physical activity and high consumption of fructose and saturated fat.4,5 There are other common foods in the Western diet, namely red and processed meats, which may also increase the risk for NAFLD.6 Meat in general contains valuable nutrients for human health including protein, iron, zinc and vitamin B12.7 However, meat also contains saturated fatty acids (SFA) and cholesterol, both harmful for patients with NAFLD,8–11 as well as other potentially harmful compounds such as heme-iron,12 sodium,13 other preservatives12 and advanced glycation end products (AGEs).12,14 Indeed, high meat consumption has been demonstrated to be associated with IR and type 2 diabetes,15–17 the metabolic syndrome17 and oxidative stress.18 More specifically, red meat has been shown to be associated with a higher risk of mortality, owing to chronic liver disease and hepatocellular carcinoma.19 The association between meat consumption and NAFLD was demonstrated in a few studies,5,6,20,21 in which meat type and cooking method were not fully addressed. We have previously demonstrated an independent association between high meat consumption and NAFLD,20 with no distinction between meat types or cooking methods, because of a small sample size and lack of information on the cooking methods in the standard food frequency questionnaire (FFQ). 
Intrahepatic (ICC) and extrahepatic cholangiocarcinoma (ECC) have rarely been studied individually, probably due to difficulties in their diagnosis and certification. Mortality trends from these 2 neoplasms have been inconsistent over the last decades. The aim of this study was to analyze worldwide trends in mortality from ICC and ECC in selected countries. We extracted death certification data for ICC and ECC, and population estimates from the World Health Organization and Pan American Health Organization databases for 32 selected countries from Europe, the Americas, and Australasia from 1995 to 2016. We computed age-standardized (world population) mortality rates from ICC and ECC, and performed joinpoint regression analysis. Mortality rates from ICC increased in all countries considered, with a levelling off over recent years in Germany (women), Italy (men), Argentina (men), the USA (men), Hong Kong (men), and Japan (both sexes). The highest rates in 2010–2014 (1.5–2.5/100,000 in men and 1.2–1.7/100,000 in women) were registered in Hong Kong, France, Austria, Spain, the UK, and Australia. The lowest rates (0.2–0.6/100,000 in both sexes) were registered in Latin American and eastern European countries. Mortality from ECC decreased in most of the countries considered, with rates below 1/100,000 in both sexes between 2010 and 2014, with the only exception being Japan (2.8/100,000 in men and 1.4/100,000 in women). Increasing mortality from ICC was observed globally, due to trends in risk factors and possibly, in part, due to better disease classification. Mortality from ECC levelled off or decreased, most likely following the increased use of laparoscopic cholecystectomy. Cholangiocarcinoma is the second most common primary liver neoplasm after hepatocellular carcinoma.1,2 These neoplasms arise from the epithelial cells of the biliary tree, commonly classified as intrahepatic cholangiocarcinoma (ICC) if they arise above the hilar junction of bile ducts or extrahepatic cholangiocarcinoma (ECC) if they arise within or below the hilum. 
Liver cancer is a common malignant neoplasm worldwide. The etiologies for liver cancer are diverse and the incidence trends of liver cancer caused by specific etiologies are rarely studied. We therefore aimed to determine the pattern of liver cancer incidence, as well as temporal trends. We collected detailed information on liver cancer etiology between 1990–2016, derived from the Global Burden of Disease study in 2016. Estimated annual percentage changes (EAPCs) in liver cancer age standardized incidence rate (ASR), by sex, region, and etiology, were calculated to quantify the temporal trends in liver cancer ASR. Globally, incident cases of liver cancer increased 114.0% from 471,000 in 1990 to 1,007,800 in 2016. The overall ASR increased by an average 0.34% (95% CI 0.22%–0.45%) per year in this period. The ASR of liver cancer due to hepatitis B, hepatitis C, and other causes increased between 1990 and 2016. The corresponding EAPCs were 0.22 (95% CI 0.08–0.36), 0.57 (95% CI 0.48–0.66), and 0.51 (95% CI 0.41–0.62), respectively. The ASR of liver cancer due to reported alcohol use remained stable (EAPC = 0.10, 95% CI −0.06–0.25). This increasing pattern was heterogeneous across regions and countries. The most pronounced increases were generally observed in countries with a high socio-demographic index, including the Netherlands, the UK, and the USA. Liver cancer remains a major public health concern globally, though control of hepatitis B and C virus infections has contributed to the decreasing incidence in some regions. We observed an unfavorable trend in countries with a high socio-demographic index, suggesting that current prevention strategies should be reoriented, and much more targeted and specific strategies should be established in some countries to forestall the increase in liver cancer. Liver cancer is a common lethal malignancy that afflicts in excess of 1 million people and caused 800,000 deaths globally in 2016.1 It has been well documented that the incidence of liver cancer varies considerably across the world, with the highest incidence observed in East Asia. In contrast, the incidence in America is nearly 5- to 10-fold lower than the incidence observed in East Asia.2 Recent decreases in the incidence of liver cancer have been reported in China and Japan.3–5 However, newly diagnosed cases and the age standardized incidence rate of liver cancer have increased on a global level during the last few decades, albeit significant public health efforts have been made to counter this problem.2,6,7 
Non-alcoholic steatohepatitis (NASH) is associated with dysregulation of lipid metabolism and hepatic inflammation. The causal mechanism underlying NASH is not fully elucidated. We aim to investigate the role of β-arrestin1 (ARRB1) in the progression of NASH. Human liver tissues from patients with NASH and control subjects were obtained to evaluate ARRB1 expression. NASH models were established in ARRB1 knockout and wild type mice fed high-fat diet (HFD) for 26 weeks or methionine/choline deficient (MCD) diet for 6 weeks. ARRB1 expression was diminished in NASH patient liver samples. Moreover, diminished ARRB1 levels were detected in mice NASH models. ARRB1 deficiency accelerated steatohepatitis development in HFD-/MCD diet-fed mice accompanied by upregulation of lipogenic genes and downregulation of β-oxidative genes. Intriguingly, ARRB1 was found to interact with GDF15 and facilitated the transportation of GDF15 precursor (pro-GDF15) to Golgi apparatus for cleavage and maturation. Treatment with recombinant GDF15 ablated the lipid accumulation in the presence of ARRB1 deletion in vitro and in vivo. Re-expression of ARRB1 in the NASH models ameliorated the liver disease, and the effect was greater in the presence of pro-GDF15 overexpression. In contrast, the effect of pro-GDF15 overexpression alone was impaired in ARRB1-deficient mice. In addition, the severity of liver disease in patients with NASH was negatively correlated with ARRB1 expression. ARRB1 acts as a vital regulator in the development of NASH via facilitating GDF15’s translocation to the Golgi apparatus and subsequent maturation. ARRB1 thus is a potential therapeutic target for the treatment of NASH. With the dramatic changes in people’s dietary choices and life styles, metabolic disorders including obesity, insulin resistance, and nonalcoholic fatty liver disease (NAFLD) have become a public health issue worldwide [1, 2]. Excessive nutritional intake and decreased energy expenditure appear to be crucial in the pathogenesis of NAFLD. NAFLD comprises a spectrum of liver diseases ranging from simple fatty liver to non-alcoholic steatohepatitis (NASH), which can potentially progress to cirrhosis and liver cancer [3, 4]. NASH is associated with reprogrammed hepatic metabolic profiles that lead to excessive lipid accumulation in the liver and imbalances in lipid metabolism and lipid catabolism [5, 6]. More advanced NASH is associated with impaired lipid metabolism, thus leading to the accumulation of triglycerides and other lipids in hepatocytes [7]. Lipotoxicity in the liver is the primary insult that initiates and propagates damage leading to hepatocyte injury and resultant inflammation [8]. Hepatic lipid homeostasis is fine-tuned by a complex machinery comprising hormones, signaling/transcriptional pathways, and downstream genes associated with lipogenesis and lipolysis [9]. Although many molecular regulatory networks have been described, the underlying mechanisms initiating the metabolic rearrangement and inflammatory response underlying NASH remain incompletely elucidated. 
The mammalian circadian clock controls various aspects of liver metabolism and integrates nutritional signals. Recently, we described Hedgehog (Hh) signaling as a novel regulator of liver lipid metabolism. Herein, we investigated crosstalk between hepatic Hh signaling and circadian rhythm. Diurnal rhythms of Hh signaling were investigated in liver and hepatocytes from mice with ablation of Smoothened (SAC-KO) and crossbreeds with PER2::LUC reporter mice. By using genome-wide screening, qPCR, immunostaining, ELISA and RNAi experiments in vitro we identified relevant transcriptional regulatory steps. Shotgun lipidomics and metabolic cages were used for analysis of metabolic alterations and behavior. Hh signaling showed diurnal oscillations in liver and hepatocytes in vitro. Correspondingly, the level of Indian Hh, oscillated in serum. Depletion of the clock gene Bmal1 in hepatocytes resulted in significant alterations in the expression of Hh genes. Conversely, SAC-KO mice showed altered expression of clock genes, confirmed by RNAi against Gli1 and Gli3. Genome-wide screening revealed that SAC-KO hepatocytes showed time-dependent alterations in various genes, particularly those associated with lipid metabolism. The clock/hedgehog module further plays a role in rhythmicity of steatosis, and in the response of the liver to a high-fat diet or to differently timed starvation. For the first time, Hh signaling in hepatocytes was found to be time-of-day dependent and to feed back on the circadian clock. Our findings suggest an integrative role of Hh signaling, mediated mainly by GLI factors, in maintaining homeostasis of hepatic lipid metabolism by balancing the circadian clock. Circadian rhythm plays an important role in regulating physiology and behavior. In mammals, the natural light–dark cycle synchronizes the central circadian pacemaker in the suprachiasmatic nucleus (SCN), which, in turn, coordinates the rhythms of autonomous clocks in peripheral tissues such as the liver;1 these tissues adapt their functions to the rhythmic cycles of feeding and activity. Indeed, most metabolic and secretory functions of the liver show pronounced circadian rhythms.2,3 In particular, liver carbohydrate and lipid metabolism, which are crucial for energy supply of the entire organism, oscillate throughout the day. The importance of this daily control is emphasized by epidemiological and experimental evidence that interruption or perturbation of circadian rhythms increases the risk for various types of liver disease and may even contribute to diabetes, obesity, metabolic syndrome and cancer.4–6 
Recreational ketamine use has emerged as an important health and social issue worldwide. Although ketamine is associated with biliary tract damage, the clinical and radiological profiles of ketamine-related cholangiopathy have not been well described. Chinese individuals who had used ketamine recreationally at least twice per month for six months in the previous two years via a territory-wide community network of charitable organizations tackling substance abuse were recruited. Magnetic resonance cholangiography (MRC) was performed, and the findings were interpreted independently by two radiologists, with the findings analysed in association with clinical characteristics. Among the 343 ketamine users referred, 257 (74.9%) were recruited. The mean age and ketamine exposure duration were 28.7 (±5.8) and 10.5 (±3.7) years, respectively. A total of 159 (61.9%) had biliary tract anomalies on MRC, categorized as diffuse extrahepatic dilatation (n = 73), fusiform extrahepatic dilatation (n = 64), and intrahepatic ductal changes (n = 22) with no extrahepatic involvement. Serum alkaline phosphatase (ALP) level (odds ratio [OR] 1.007; 95% CI 1.002–1.102), lack of concomitant recreational drug use (OR 1.99; 95% CI 1.11–3.58), and prior emergency attendance for urinary symptoms (OR 1.95; 95% CI 1.03–3.70) had high predictive values for biliary anomalies on MRC. Among sole ketamine users, ALP level had an AUC of 0.800 in predicting biliary anomalies, with an optimal level of ≥113 U/L having a positive predictive value of 85.4%. Cholangiographic anomalies were reversible after ketamine abstinence, whereas decompensated cirrhosis and death were possible after prolonged exposure. We have identified distinctive MRC patterns in a large cohort of ketamine users. ALP level and lack of concomitant drug use predicted biliary anomalies, which were reversible after abstinence. The study findings may aid public health efforts in combating the growing epidemic of ketamine abuse. Recreational inhalation of ketamine is emerging as a major global social and health issue.1,2 Although ketamine, an N-methyl-d-aspartate receptor antagonist, has medical uses in anaesthesia and chronic pain control, its highly addictive nature has led to a massive increase in recreational consumption worldwide. Because of the ease of production and low cost, the non-medical use of ketamine is increasing especially in East and South-East Asia, with its lifetime prevalence in the general population ranging from 0.3% to 2.0%,3 comprising up to 39.7% of total recreational drugs users in these regions.4 The self-reported recreational use of ketamine in Western countries, including the UK, Australia, and Canada, is also increasing.5,6 From 2008 to 2014, law enforcement seizures of ketamine worldwide increased by more than threefold.3 
Sorafenib is the recommended treatment for patients with advanced hepatocellular carcinoma (HCC). We aimed to compare the efficacy and safety of a combination of sorafenib and selective internal radiation therapy (SIRT) – with yttrium-90 (90Y) resin microspheres – to sorafenib alone in patients with advanced HCC. SORAMIC is a randomised controlled trial comprising diagnostic, local ablation and palliative cohorts. Based on diagnostic study results, patients were assigned to local ablation or palliative cohorts. In the palliative cohort, patients not eligible for TACE were randomised 11:10 to SIRT plus sorafenib (SIRT + sorafenib) or sorafenib alone. The primary endpoint was overall survival (OS; Kaplan-Meier analysis) in the intention-to-treat (ITT) population. In the ITT cohort, 216 patients were randomised to SIRT + sorafenib and 208 to sorafenib alone. Median OS was 12.1 months in the SIRT + sorafenib arm, and 11.4 months in the sorafenib arm (hazard ratio [HR] 1.01; 95% CI 0.81–1.25; p = 0.9529). Median OS in the per protocol population was 14.0 months in the SIRT + sorafenib arm (n = 114), and 11.1 months in the sorafenib arm (n = 174; HR 0.86; p = 0.2515). Subgroup analyses of the per protocol population indicated a survival benefit of SIRT + sorafenib for patients without cirrhosis (HR 0.46; 0.25–0.86; p = 0.02); cirrhosis of non-alcoholic aetiology (HR 0.63; p = 0.012); or patients ≤65 years old (HR 0.65; p = 0.05). Adverse events (AEs) of Common Terminology Criteria for AE Grades 3–4 were reported in 103/159 (64.8%) patients who received SIRT + sorafenib, 106/197 (53.8%) patients who received sorafenib alone (p = 0.04), and 8/24 (33.3%) patients who only received SIRT. Addition of SIRT to sorafenib did not result in a significant improvement in OS compared with sorafenib alone. Subgroup analyses led to hypothesis-generating results that will support the design of future studies. Hepatocellular carcinoma (HCC) is the most common type of malignant primary liver tumour, accounting for 80–90% of all liver cancers.1 In the USA, for example, 30,640 new liver and intrahepatic bile duct cancers are estimated to have occurred in 2013, with 21,670 associated deaths.2,3 
Most patients with hepatitis C virus (HCV) infection will undergo antiviral treatment with direct-acting antivirals (DAAs) and achieve sustained virologic response (SVR). We aimed to develop models estimating hepatocellular carcinoma (HCC) risk after antiviral treatment. We identified 45,810 patients who initiated antiviral treatment in the Veterans Affairs (VA) national healthcare system from 1/1/2009 to 12/31/2015, including 29,309 (64%) DAA-only regimens and 16,501 (36%) interferon ± DAA regimens. We retrospectively followed patients until 6/15/2017 to identify incident cases of HCC. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at the time of antiviral treatment. We identified 1,412 incident cases of HCC diagnosed at least 180 days after initiation of antiviral treatment during a mean follow-up of 2.5 years (range 1.0–7.5 years). Models predicting HCC risk after antiviral treatment were developed and validated separately for four subgroups of patients: cirrhosis/SVR, cirrhosis/no SVR, no cirrhosis/SVR, no cirrhosis/no SVR. Four predictors (age, platelet count, serum aspartate aminotransferase/√alanine aminotransferase ratio and albumin) accounted for most of the models’ predictive value, with smaller contributions from sex, race-ethnicity, HCV genotype, body mass index, hemoglobin and serum alpha-fetoprotein. Fitted models were well-calibrated with very good measures of discrimination. Decision curves demonstrated higher net benefit of using model-based HCC risk estimates to determine whether to recommend screening or not compared to the screen-all or screen-none strategies. We developed and internally validated models that estimate HCC risk following antiviral treatment. These models are available as web-based tools that can be used to inform risk-based HCC surveillance strategies in individual patients. Most patients with chronic hepatitis C virus (HCV) infection have either already received antiviral treatment or are expected to receive treatment with direct-acting antivirals (DAAs) in the next 3–5 years in the United States. With sustained virologic response (SVR) rates well in excess of 90%, the vast majority of treated patients will achieve HCV eradication. SVR reduces hepatocellular carcinoma (HCC) risk substantially, irrespective of whether it is achieved by interferon (IFN) or DAA-based regimens.1 It follows that HCC risk needs to be estimated specifically for the period following antiviral treatment, incorporating whether SVR was achieved or not, and that previous models predicting HCC risk in untreated HCV-infected patients do not apply to patients who have undergone antiviral treatment. 
In treated patients with chronic hepatitis B (CHB) who have achieved complete viral suppression, it is unclear if functional cure as indicated by hepatitis B surface antigen (HBsAg) seroclearance confers additional clinical benefit. We compared the risk of hepatocellular carcinoma (HCC) and hepatic events in nucleos(t)ide analogue (NA)-treated patients with and without HBsAg seroclearance. We performed a territory-wide retrospective cohort study on all patients with CHB who had received entecavir and/or tenofovir disoproxil fumarate (TDF) for at least 6 months between 2005 and 2016 from Hospital Authority, Hong Kong. Patients’ demographics, comorbidities, and laboratory parameters were analyzed. The primary outcome was HCC. The secondary outcomes were hepatic events including cirrhotic complications, liver transplantation, and liver-related mortality. A total of 20,263 entecavir/TDF-treated patients with CHB were identified; 17,499 (86.4%) patients had complete viral suppression; 376 (2.1%) achieved HBsAg seroclearance. At a median (interquartile range) follow-up of 4.8 (2.8–7.0) years, 603 (3.5%) and 121 (4.4%) patients with and without complete viral suppression developed HCC; 2 (0.5%) patients with HBsAg seroclearance developed HCC. Compared to complete viral suppression, lack of complete viral suppression was associated with a higher risk of HCC (7.8% vs. 5.6% at 8 years, Gray’s test, p <0.001) (adjusted hazard ratio [aHR] 1.69; 95% CI 1.36–2.09; p <0.001); patients who achieved functional cure had a lower risk of HCC (0.6% vs. 5.6% at 8 years, Gray’s test, p <0.001) (aHR 0.24; 95% CI 0.06–0.97; p = 0.045) but not hepatic events (aHR 0.99; 95% CI 0.30–3.26; p = 0.991). Patients who achieved HBsAg seroclearance on top of complete viral suppression with entecavir/TDF treatment may have a lower risk of HCC but not hepatic events. Chronic hepatitis B (CHB) is one of the leading causes of hepatocellular carcinoma (HCC), cirrhotic complications and liver-related death worldwide.1 Antiviral therapy with potent nucleos(t)ide analogues (NA), namely entecavir and tenofovir disoproxil fumarate (TDF), reduces the risk of HCC and hepatic complications.2,3 Recent data suggest that complete viral suppression is an important treatment goal because patients with low detectable serum hepatitis B virus (HBV) DNA levels still have a higher risk of developing HCC.4,5 Complete viral suppression also leads to histological improvement and regression of liver fibrosis and cirrhosis.6,7 
Cancer is a major cause of death in patients with non-alcoholic fatty liver disease (NAFLD). Obesity is a risk factor for cancers; however, the role of NAFLD in this association is unknown. We investigated the effect of NAFLD versus obesity on incident cancers. We identified all incident cases of NAFLD in a US population between 1997-2016. Individuals with NAFLD were matched by age and sex to referent individuals from the same population (1:3) on the index diagnosis date. We ascertained the incidence of cancer after index date until death, loss to follow-up or study end. NAFLD and cancer were defined using a code-based algorithm with high validity and tested by medical record review. The association between NAFLD or obesity and cancer risk was examined using Poisson regression. A total of 4,722 individuals with NAFLD (median age 54, 46% male) and 14,441 age- and sex-matched referent individuals were followed for a median of 8 (range 1–21) years, during which 2,224 incident cancers occurred. NAFLD was associated with 90% higher risk of malignancy: incidence rate ratio (IRR) = 1.9 (95% CI 1.3–2.7). The highest risk increase was noted in liver cancer, IRR = 2.8 (95% CI 1.6–5.1), followed by uterine IRR = 2.3 (95% CI 1.4–4.1), stomach IRR = 2.3 (95% CI 1.3–4.1), pancreas IRR = 2.0 (95% CI 1.2–3.3) and colon cancer IRR = 1.8 (95% CI 1.1–2.8). In reference to non-obese controls, NAFLD was associated with a higher risk of incident cancers (IRR = 2.0, 95% CI 1.5–2.9), while obesity alone was not (IRR = 1.0, 95% CI 0.8–1.4). NAFLD was associated with increased cancer risk, particularity of gastrointestinal types. In the absence of NAFLD, the association between obesity and cancer risk is small, suggesting that NAFLD may be a mediator of the obesity-cancer association. Cancer is a major cause of death in the United States and worldwide.1,2 Numerous meta-analyses support the link between the risk of malignancy and excess body weight.3–5 Some associations are flawed because of bias that exaggerates the effect of obesity on cancer incidence, but strong evidence supports this association with 11 cancers, predominantly among digestive organs and hormone-related malignancies in women.4 
Liver fibrosis regression but also progression may occur in patients with autoimmune hepatitis (AIH) under treatment. There is a need for non-invasive surrogate markers for fibrosis development in AIH to better guide immunosuppressive treatment. The aims of the study were to assess the impact of complete biochemical remission defined as normalisation of aminotransferases and IgG on histological activity and fibrosis development, and the value of repeat transient elastography (TE) measurement for monitoring disease progression in AIH. A total of 131 liver biopsies from 60 patients with AIH and more than 900 TE from 125 patients with AIH, 130 with primary biliary cholangitis (PBC) and 100 with primary sclerosing cholangitis (PSC), were evaluated. Time intervals between TE were at least 12 months. Patients with AIH were treated for at least six months at first TE. In contrast to PBC and PSC, a decrease of liver stiffness (LS) was observed in the whole group of patients with AIH (−6.2%/year; 95% CI −12.6% to −0.2%; p = 0.04). The largest decrease of LS was observed in patients with severe fibrosis at baseline (F4: −11.7%/year; 95% CI −19% to −3.5%; p = 0.006). Complete biochemical remission was strongly linked to regression of LS (“remission”: −7.5%/year vs. “no remission”: +1.7%/year, p <0.001). Similarly, complete biochemical remission predicted low histological disease activity and was the only independent predictor for histological fibrosis regression (relative risk 3.66; 95% CI 1.54–10.2; p = 0.001). Patients with F3/F4-fibrosis, who remained in biochemical remission showed a considerable decrease of fibrosis stage (3.7 ± 0.5 to 1.8 ± 1.7; p = 0.007) on histological follow-up. This study demonstrates that complete biochemical remission is a reliable predictor of a good prognosis in AIH and leads to fibrosis regression that can be monitored by TE. Autoimmune hepatitis (AIH) is a chronic disease, which harbours a significant risk of fibrosis progression when treatment is insufficient.1,2 Some patients may progress despite immunosuppressive treatment.3,4 Therefore, AIH patients require close, life-long follow-up. In daily practice, treatment monitoring is mainly guided by biochemical surrogate markers for hepatic inflammation. Besides serum aminotransferases, it has been demonstrated that selectively elevated IgG levels also indicate ongoing inflammatory activity in AIH.5 Therefore, complete biochemical remission has been defined as repeatedly normal serum aminotransferase and IgG levels. Although the prognostic relevance of this definition is uncertain, current treatment guidelines have incorporated this definition as the goal of immunosuppressive treatment in AIH.6–8 Since complete biochemical remission is hard to achieve in some patients and requires high doses of immunosuppressants in others, it is important to validate the prognostic significance of this definition. 
To affect immune response and inflammation, the hepatitis C virus (HCV) substantially influences intercellular communication pathways that are decisive for immune cell recruitment. The present study investigates mechanisms by which HCV modulates chemokine-mediated intercellular communication from infected cells. Chemokine expression was studied in HCVcc-infected cell lines or cell lines harbouring a subgenomic replicon, as well as in serum samples from patients. Expression or activity of mediators and signalling intermediates was manipulated using knockdown approaches or specific inhibitors. HCV enhances expression of CXCR2 ligands in its host cell via the induction of epidermal growth factor (EGF) production. Knockdown of EGF or of the p65 subunit of the NF-κB complex results in a substantial downregulation of HCV-induced CXCR2 ligand expression, supporting the involvement of an EGF-dependent mechanism as well as activation of NF-κB. Furthermore, HCV upregulates expression of CXCR2 ligands in response to EGF stimulation via downregulation of the T-cell protein tyrosine phosphatase (TC-PTP [PTPN2]), activation of NF-κB, and enhancement of EGF-inducible signal transduction via MEK1 (MAP2K1). This results in the production of a cytokine/chemokine pattern by the HCV-infected cell that can recruit neutrophils but not monocytes. These data reveal a novel EGF-dependent mechanism by which HCV influences chemokine-mediated intercellular communication. We propose that this mechanism contributes to modulation of the HCV-induced inflammation and the antiviral immune response. As a leading cause of chronic liver disease worldwide, in more than 70% of infected individuals HCV establishes a persistent infection characterised by continuous replication and high serum titres. The high propensity for persistence and the insidious course of disease is suggestive of powerful mechanisms allowing HCV to subvert host antiviral immunity, to modify the inflammatory response, and to utilise host cell infrastructure without affecting cell viability. This enables the virus to avert development of overt disease over decades, despite ongoing viral replication. At least in part, this is because HCV is tightly interconnected to host intercellular and intracellular signalling pathways.1,2 Hence, investigating the interaction of HCV with its host also provides a possibility to illuminate pathways that are relevant for the regulation of different processes of the host, such as growth factor signalling, control of cell growth, differentiation or cell death, inflammatory processes, and antiviral immunity. Amazingly, HCV achieves this molecular piracy with only 10 viral proteins, proteolytically released from one single precursor protein, which is encoded by a positive stranded viral RNA genome. These proteins are multifunctional and required for virus particle formation as well as RNA replication, but also for a broad interaction with different signal transduction pathways of the host.1 For example, the viral protease NS3/4A on the one hand is central to the processing of the viral precursor protein to generate mature viral proteins. On the other hand, it mediates proteolytic cleavage of different signalling molecules of the host cell such as mitochondrial antiviral-signalling protein (MAVS)3 and TRIF (TICAM1)4 involved in antiviral signalling, or the ubiquitously expressed T-cell protein tyrosine phosphatase (TC-PTP [PTPN2]).5,6 Being an endogenous negative regulator of the EGF receptor (EGFR),7 cleavage of the latter results in a sensitisation of the EGFR and subsequent signal transduction, as well as in a constitutive and ligand independent activation of AKT kinase.5 Consistently, TC-PTP expression is reduced in individuals with chronic HCV infection5,8 and in mice with liver-specific expression of the viral NS3/4A protease.5,6 These animals not only show an enhanced ligand-induced activation of EGFR and its downstream signalling intermediates but also an altered composition of immune cells in their liver tissue.6,9 This indicates that HCV or HCV-encoded proteins can markedly influence intercellular communication signals crucial for immune cell recruitment. Among these, chemokines are the most important factors that control recruitment of immune cells and determine the local immunological and inflammatory environment. Indeed, the production of several chemokines, including CCL2 or CXCL8, is influenced by HCV replication as well as by isolated expression of HCV-encoded proteins.10 Thereby CXCL8 belongs to a group of chemokines that are recognised by the chemokine receptor CXCR2 and play an important role in the recruitment of neutrophil granulocytes,11 which most likely contribute to the increased serum concentrations of inflammatory cytokines such as TNFα, which can be observed in individuals with chronic HCV infection.12 The upregulation of TNFα appears to also be responsible for protection against the deleterious effects of lipopolysaccharide observed in transgenic mice with liver-specific expression of NS3/4A.13 However, the mechanisms involved in the regulation of chemokine production by HCV are incompletely understood and are the subject of the research described in the present manuscript. SECTION Materials and methods SECTION Materials 
